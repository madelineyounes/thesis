
@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	shorttitle = {wav2vec 2.0},
	url = {http://arxiv.org/abs/2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2022-02-17},
	journal = {arXiv:2006.11477 [cs, eess]},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.11477},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/B96C49V9/Baevski et al. - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/MINTX78M/2006.html:text/html},
}

@article{korzekwa_mispronunciation_2021,
	title = {Mispronunciation {Detection} in {Non}-native ({L2}) {English} with {Uncertainty} {Modeling}},
	url = {http://arxiv.org/abs/2101.06396},
	abstract = {A common approach to the automatic detection of mispronunciation in language learning is to recognize the phonemes produced by a student and compare it to the expected pronunciation of a native speaker. This approach makes two simplifying assumptions: a) phonemes can be recognized from speech with high accuracy, b) there is a single correct way for a sentence to be pronounced. These assumptions do not always hold, which can result in a significant amount of false mispronunciation alarms. We propose a novel approach to overcome this problem based on two principles: a) taking into account uncertainty in the automatic phoneme recognition step, b) accounting for the fact that there may be multiple valid pronunciations. We evaluate the model on non-native (L2) English speech of German, Italian and Polish speakers, where it is shown to increase the precision of detecting mispronunciations by up to 18\% (relative) compared to the common approach.},
	urldate = {2022-02-22},
	journal = {arXiv:2101.06396 [cs, eess]},
	author = {Korzekwa, Daniel and Lorenzo-Trueba, Jaime and Zaporowski, Szymon and Calamaro, Shira and Drugman, Thomas and Kostek, Bozena},
	month = feb,
	year = {2021},
	note = {arXiv: 2101.06396},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: Accepted to ICASSP 2021},
	annote = {Comment: Accepted to ICASSP 2021},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/K2NWE69G/Korzekwa et al. - 2021 - Mispronunciation Detection in Non-native (L2) Engl.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/EXZFFV2Z/2101.html:text/html},
}

@article{jauhiainen_automatic_2018,
	title = {Automatic {Language} {Identification} in {Texts}: {A} {Survey}},
	shorttitle = {Automatic {Language} {Identification} in {Texts}},
	url = {http://arxiv.org/abs/1804.08186},
	abstract = {Language identification (LI) is the problem of determining the natural language that a document or part thereof is written in. Automatic LI has been extensively researched for over fifty years. Today, LI is a key part of many text processing pipelines, as text processing techniques generally assume that the language of the input text is known. Research in this area has recently been especially active. This article provides a brief history of LI research, and an extensive survey of the features and methods used so far in the LI literature. For describing the features and methods we introduce a unified notation. We discuss evaluation methods, applications of LI, as well as off-the-shelf LI systems that do not require training by the end user. Finally, we identify open issues, survey the work to date on each issue, and propose future directions for research in LI.},
	urldate = {2022-02-26},
	journal = {arXiv:1804.08186 [cs]},
	author = {Jauhiainen, Tommi and Lui, Marco and Zampieri, Marcos and Baldwin, Timothy and Lindén, Krister},
	month = nov,
	year = {2018},
	note = {arXiv: 1804.08186},
	keywords = {Computer Science - Computation and Language, notion, Automatic Language Identification},
	annote = {Comment: Under review at JAIR - Journal of Artificial Intelligence Research},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/Q83DWIHY/Jauhiainen et al. - 2018 - Automatic Language Identification in Texts A Surv.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/N9M9TYAH/1804.html:text/html},
}

@article{valk_voxlingua107_2020,
	title = {{VoxLingua107}: a {Dataset} for {Spoken} {Language} {Recognition}},
	shorttitle = {{VoxLingua107}},
	url = {http://arxiv.org/abs/2011.12998},
	abstract = {This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semi-random search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98\%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available.},
	urldate = {2022-02-26},
	journal = {arXiv:2011.12998 [eess]},
	author = {Valk, Jörgen and Alumäe, Tanel},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.12998},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, notion, language recognition, unlabeled dataset, webscrapped dataset},
	annote = {Comment: Accepted at IEEE Spoken Language Technology Workshop (SLT) 2021},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/FPM3XQSN/Valk and Alumäe - 2020 - VoxLingua107 a Dataset for Spoken Language Recogn.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/Y6MWMQS2/2011.html:text/html},
}

@article{revay_multiclass_2019,
	title = {Multiclass {Language} {Identification} using {Deep} {Learning} on {Spectral} {Images} of {Audio} {Signals}},
	url = {http://arxiv.org/abs/1905.04348},
	abstract = {The first step in any voice recognition software is to determine what language a speaker is using, and ideally this process would be automated. The technique described in this paper, language identification for audio spectrograms (LIFAS), uses spectrograms generated from audio signals as inputs to a convolutional neural network (CNN) to be used for language identification. LIFAS requires minimal pre-processing on the audio signals as the spectrograms are generated during each batch as they are input to the network during training. LIFAS utilizes deep learning tools that are shown to be successful on image processing tasks and applies it to audio signal classification. LIFAS performs binary language classification with an accuracy of 97{\textbackslash}\%, and multi-class classification with six languages at an accuracy of 89{\textbackslash}\% on 3.75 second audio clips.},
	urldate = {2022-02-26},
	journal = {arXiv:1905.04348 [cs, eess]},
	author = {Revay, Shauna and Teschke, Matthew},
	month = may,
	year = {2019},
	note = {arXiv: 1905.04348},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/7LXM4M8B/Revay and Teschke - 2019 - Multiclass Language Identification using Deep Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/WHYMFM7S/1905.html:text/html},
}

@article{hsu_hubert_2021,
	title = {{HuBERT}: {Self}-{Supervised} {Speech} {Representation} {Learning} by {Masked} {Prediction} of {Hidden} {Units}},
	shorttitle = {{HuBERT}},
	url = {http://arxiv.org/abs/2106.07447},
	abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
	urldate = {2022-02-26},
	journal = {arXiv:2106.07447 [cs, eess]},
	author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07447},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/RHVN99BA/Hsu et al. - 2021 - HuBERT Self-Supervised Speech Representation Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/UC3E9IAR/2106.html:text/html},
}

@article{hussein_arabic_2021,
	title = {Arabic {Speech} {Recognition} by {End}-to-{End}, {Modular} {Systems} and {Human}},
	url = {http://arxiv.org/abs/2101.08454},
	abstract = {Recent advances in automatic speech recognition (ASR) have achieved accuracy levels comparable to human transcribers, which led researchers to debate if the machine has reached human performance. Previous work focused on the English language and modular hidden Markov model-deep neural network (HMM-DNN) systems. In this paper, we perform a comprehensive benchmarking for end-to-end transformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the Arabic language and its dialects. For the HSR, we evaluate linguist performance and lay-native speaker performance on a new dataset collected as a part of this study. For ASR the end-to-end work led to 12.5\%, 27.5\%, 33.8\% WER; a new performance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our results suggest that human performance in the Arabic language is still considerably better than the machine with an absolute WER gap of 3.5\% on average.},
	urldate = {2022-03-04},
	journal = {arXiv:2101.08454 [cs, eess]},
	author = {Hussein, Amir and Watanabe, Shinji and Ali, Ahmed},
	month = jun,
	year = {2021},
	note = {arXiv: 2101.08454},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion, Arabic, ASR},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/8X7SAFPI/Hussein et al. - 2021 - Arabic Speech Recognition by End-to-End, Modular S.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/XLMQVGWY/2101.html:text/html},
}

@inproceedings{audhkhasi_end--end_2017,
	address = {New Orleans, LA},
	title = {End-to-end {ASR}-free keyword search from speech},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953076/},
	doi = {10.1109/ICASSP.2017.7953076},
	urldate = {2022-03-07},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Audhkhasi, Kartik and Rosenberg, Andrew and Sethy, Abhinav and Ramabhadran, Bhuvana and Kingsbury, Brian},
	month = mar,
	year = {2017},
	keywords = {notion},
	pages = {4840--4844},
	file = {Submitted Version:/Users/myounes/Zotero/storage/K53ZLW8W/Audhkhasi et al. - 2017 - End-to-end ASR-free keyword search from speech.pdf:application/pdf},
}

@article{lang_self-supervised_2020,
	title = {Self-supervised self-supervision by combining deep learning and probabilistic logic},
	url = {http://arxiv.org/abs/2012.12474},
	abstract = {Labeling training examples at scale is a perennial challenge in machine learning. Self-supervision methods compensate for the lack of direct supervision by leveraging prior knowledge to automatically generate noisy labeled examples. Deep probabilistic logic (DPL) is a unifying framework for self-supervised learning that represents unknown labels as latent variables and incorporates diverse self-supervision using probabilistic logic to train a deep neural network end-to-end using variational EM. While DPL is successful at combining pre-specified self-supervision, manually crafting self-supervision to attain high accuracy may still be tedious and challenging. In this paper, we propose Self-Supervised Self-Supervision (S4), which adds to DPL the capability to learn new self-supervision automatically. Starting from an initial "seed," S4 iteratively uses the deep neural network to propose new self supervision. These are either added directly (a form of structured self-training) or verified by a human expert (as in feature-based active learning). Experiments show that S4 is able to automatically propose accurate self-supervision and can often nearly match the accuracy of supervised methods with a tiny fraction of the human effort.},
	urldate = {2022-03-09},
	journal = {arXiv:2012.12474 [cs, stat]},
	author = {Lang, Hunter and Poon, Hoifung},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.12474},
	keywords = {Computer Science - Machine Learning, notion, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 2 figures},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/2A9EEZGT/Lang and Poon - 2020 - Self-supervised self-supervision by combining deep.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/LM7EKUGH/2012.html:text/html},
}

@inproceedings{samih_neural_2017,
	address = {Valencia, Spain},
	title = {A {Neural} {Architecture} for {Dialectal} {Arabic} {Segmentation}},
	url = {http://aclweb.org/anthology/W17-1306},
	doi = {10.18653/v1/W17-1306},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Proceedings of the {Third} {Arabic} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Samih, Younes and Attia, Mohammed and Eldesouki, Mohamed and Abdelali, Ahmed and Mubarak, Hamdy and Kallmeyer, Laura and Darwish, Kareem},
	year = {2017},
	keywords = {notion},
	pages = {46--54},
	file = {Full Text:/Users/myounes/Zotero/storage/UCSQMH4E/Samih et al. - 2017 - A Neural Architecture for Dialectal Arabic Segment.pdf:application/pdf},
}

@article{baevski_data2vec_2022,
	title = {data2vec: {A} {General} {Framework} for {Self}-supervised {Learning} in {Speech}, {Vision} and {Language}},
	shorttitle = {data2vec},
	url = {http://arxiv.org/abs/2202.03555},
	abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.},
	urldate = {2022-03-09},
	journal = {arXiv:2202.03555 [cs]},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.03555},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/JBKIWIZ9/Baevski et al. - 2022 - data2vec A General Framework for Self-supervised .pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/P5AQERUG/2202.html:text/html},
}

@article{perreault_dating_2012,
	title = {Dating the {Origin} of {Language} {Using} {Phonemic} {Diversity}},
	volume = {7},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0035289},
	doi = {10.1371/journal.pone.0035289},
	language = {en},
	number = {4},
	urldate = {2022-03-09},
	journal = {PLoS ONE},
	author = {Perreault, Charles and Mathew, Sarah},
	editor = {Petraglia, Michael D.},
	month = apr,
	year = {2012},
	keywords = {notion, pheontic diversity},
	pages = {e35289},
	file = {Full Text:/Users/myounes/Zotero/storage/G9PQL34Q/Perreault and Mathew - 2012 - Dating the Origin of Language Using Phonemic Diver.pdf:application/pdf},
}

@article{mohamud_fast_2021,
	title = {Fast {Development} of {ASR} in {African} {Languages} using {Self} {Supervised} {Speech} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2103.08993},
	abstract = {This paper describes the results of an informal collaboration launched during the African Master of Machine Intelligence (AMMI) in June 2020. After a series of lectures and labs on speech data collection using mobile applications and on self-supervised representation learning from speech, a small group of students and the lecturer continued working on automatic speech recognition (ASR) project for three languages: Wolof, Ga, and Somali. This paper describes how data was collected and ASR systems developed with a small amount (1h) of transcribed speech as training data. In these low resource conditions, pre-training a model on large amounts of raw speech was fundamental for the efficiency of ASR systems developed.},
	urldate = {2022-03-09},
	journal = {arXiv:2103.08993 [cs, eess]},
	author = {Mohamud, Jama Hussein and Thompson, Lloyd Acquaye and Ndoye, Aissatou and Besacier, Laurent},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.08993},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: Accepted at AfricaNLP2021 workshop at EACL 2021},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/K626XKHD/Mohamud et al. - 2021 - Fast Development of ASR in African Languages using.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/UTQYGQ8G/2103.html:text/html},
}

@article{ardila_common_2020,
	title = {Common {Voice}: {A} {Massively}-{Multilingual} {Speech} {Corpus}},
	shorttitle = {Common {Voice}},
	url = {http://arxiv.org/abs/1912.06670},
	abstract = {The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.},
	urldate = {2022-03-09},
	journal = {arXiv:1912.06670 [cs]},
	author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M. and Weber, Gregor},
	month = mar,
	year = {2020},
	note = {arXiv: 1912.06670},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
	annote = {Comment: Accepted to LREC 2020},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/NUYPV7G3/Ardila et al. - 2020 - Common Voice A Massively-Multilingual Speech Corp.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/5A7JKZA2/1912.html:text/html},
}

@article{wadhawan_dialect_2021,
	title = {Dialect {Identification} in {Nuanced} {Arabic} {Tweets} {Using} {Farasa} {Segmentation} and {AraBERT}},
	url = {http://arxiv.org/abs/2102.09749},
	abstract = {This paper presents our approach to address the EACL WANLP-2021 Shared Task 1: Nuanced Arabic Dialect Identification (NADI). The task is aimed at developing a system that identifies the geographical location(country/province) from where an Arabic tweet in the form of modern standard Arabic or dialect comes from. We solve the task in two parts. The first part involves pre-processing the provided dataset by cleaning, adding and segmenting various parts of the text. This is followed by carrying out experiments with different versions of two Transformer based models, AraBERT and AraELECTRA. Our final approach achieved macro F1-scores of 0.216, 0.235, 0.054, and 0.043 in the four subtasks, and we were ranked second in MSA identification subtasks and fourth in DA identification subtasks.},
	urldate = {2022-03-09},
	journal = {arXiv:2102.09749 [cs]},
	author = {Wadhawan, Anshul},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.09749},
	keywords = {Computer Science - Computation and Language, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/6MMYQJMG/Wadhawan - 2021 - Dialect Identification in Nuanced Arabic Tweets Us.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/2NCZNULC/2102.html:text/html},
}

@article{khalifa_zero-resource_2021,
	title = {Zero-{Resource} {Multi}-{Dialectal} {Arabic} {Natural} {Language} {Understanding}},
	volume = {12},
	issn = {21565570, 2158107X},
	url = {http://arxiv.org/abs/2104.06591},
	doi = {10.14569/IJACSA.2021.0120369},
	abstract = {A reasonable amount of annotated data is required for fine-tuning pre-trained language models (PLM) on downstream tasks. However, obtaining labeled examples for different language varieties can be costly. In this paper, we investigate the zero-shot performance on Dialectal Arabic (DA) when fine-tuning a PLM on modern standard Arabic (MSA) data only -- identifying a significant performance drop when evaluating such models on DA. To remedy such performance drop, we propose self-training with unlabeled DA data and apply it in the context of named entity recognition (NER), part-of-speech (POS) tagging, and sarcasm detection (SRD) on several DA varieties. Our results demonstrate the effectiveness of self-training with unlabeled DA data: improving zero-shot MSA-to-DA transfer by as large as {\textbackslash}texttildelow 10{\textbackslash}\% F\$\_1\$ (NER), 2{\textbackslash}\% accuracy (POS tagging), and 4.5{\textbackslash}\% F\$\_1\$ (SRD). We conduct an ablation experiment and show that the performance boost observed directly results from the unlabeled DA examples used for self-training. Our work opens up opportunities for leveraging the relatively abundant labeled MSA datasets to develop DA models for zero and low-resource dialects. We also report new state-of-the-art performance on all three tasks and open-source our fine-tuned models for the research community.},
	number = {3},
	urldate = {2022-03-09},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Khalifa, Muhammad and Hassan, Hesham and Fahmy, Aly},
	year = {2021},
	note = {arXiv: 2104.06591},
	keywords = {Computer Science - Computation and Language, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/299KYTQV/Khalifa et al. - 2021 - Zero-Resource Multi-Dialectal Arabic Natural Langu.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/V2K3BLGB/2104.html:text/html},
}

@inproceedings{zhang_no_2019,
	address = {Florence, Italy},
	title = {No {Army}, {No} {Navy}: {BERT} {Semi}-{Supervised} {Learning} of {Arabic} {Dialects}},
	shorttitle = {No {Army}, {No} {Navy}},
	url = {https://www.aclweb.org/anthology/W19-4637},
	doi = {10.18653/v1/W19-4637},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Proceedings of the {Fourth} {Arabic} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Chiyu and Abdul-Mageed, Muhammad},
	year = {2019},
	keywords = {notion},
	pages = {279--284},
	file = {Full Text:/Users/myounes/Zotero/storage/NLCFP8QF/Zhang and Abdul-Mageed - 2019 - No Army, No Navy BERT Semi-Supervised Learning of.pdf:application/pdf},
}

@article{dendani_self-supervised_2021,
	title = {Self-{Supervised} {Speech} {Enhancement} for {Arabic} {Speech} {Recognition} in {Real}-{World} {Environments}},
	volume = {38},
	issn = {07650019, 19585608},
	url = {http://www.iieta.org/journals/ts/paper/10.18280/ts.380212},
	doi = {10.18280/ts.380212},
	abstract = {Mobile speech recognition attracts much attention in the ubiquitous context, however, background noises, speech coding, and transmission errors are prone to corrupt the incoming speech. Therein, building a robust speech recognizer requires the availability of a large number of real-world speech samples. Arabic language, like many other languages, lacks such resources; to overcome this limitation, we propose a speech enhancement step, before the recognition begins. For the speech enhancement purpose, we suggest the use of a deep autoencoder (DAE) algorithm. A two-step procedure is suggested: in the first step, an overcomplete DAE is trained in an unsupervised way, and in the second one, a denoising DAE is trained in a supervised way leveraging the clean speech produced in the previous step. Experimental results performed on a real-life mobile database confirmed the potentials of the proposed approach and show a reduction of the WER (Word Error Rate) of a ubiquitous Arabic speech recognizer. Further experiments show an improvement of the perceptual evaluation of speech quality (PESQ), and the short-time objective intelligibility (STOI) as well.},
	number = {2},
	urldate = {2022-03-09},
	journal = {Traitement du Signal},
	author = {Dendani, Bilal and Bahi, Halima and Sari, Toufik},
	month = apr,
	year = {2021},
	keywords = {notion},
	pages = {349--358},
	file = {Full Text:/Users/myounes/Zotero/storage/NFSK9QGI/Dendani et al. - 2021 - Self-Supervised Speech Enhancement for Arabic Spee.pdf:application/pdf},
}

@inproceedings{samih_multilingual_2016,
	address = {Austin, Texas},
	title = {Multilingual {Code}-switching {Identification} via {LSTM} {Recurrent} {Neural} {Networks}},
	url = {http://aclweb.org/anthology/W16-5806},
	doi = {10.18653/v1/W16-5806},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Proceedings of the {Second} {Workshop} on {Computational} {Approaches} to {Code}           {Switching}},
	publisher = {Association for Computational Linguistics},
	author = {Samih, Younes and Maharjan, Suraj and Attia, Mohammed and Kallmeyer, Laura and Solorio, Thamar},
	year = {2016},
	keywords = {notion},
	pages = {50--59},
	file = {Full Text:/Users/myounes/Zotero/storage/SXDF5VBS/Samih et al. - 2016 - Multilingual Code-switching Identification via LST.pdf:application/pdf},
}

@article{chen_wavlm_2022,
	title = {{WavLM}: {Large}-{Scale} {Self}-{Supervised} {Pre}-{Training} for {Full} {Stack} {Speech} {Processing}},
	shorttitle = {{WavLM}},
	url = {http://arxiv.org/abs/2110.13900},
	abstract = {Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture sequence ordering of input speech, and scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.},
	urldate = {2022-03-09},
	journal = {arXiv:2110.13900 [cs, eess]},
	author = {Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},
	month = jan,
	year = {2022},
	note = {arXiv: 2110.13900},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/48CS7AH3/Chen et al. - 2022 - WavLM Large-Scale Self-Supervised Pre-Training fo.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/RUTX6E7A/2110.html:text/html},
}

@article{tyler_cross-language_2009,
	title = {Cross-language differences in cue use for speech segmentation},
	volume = {126},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3129127},
	doi = {10.1121/1.3129127},
	language = {en},
	number = {1},
	urldate = {2022-03-09},
	journal = {The Journal of the Acoustical Society of America},
	author = {Tyler, Michael D. and Cutler, Anne},
	month = jul,
	year = {2009},
	keywords = {notion},
	pages = {367--376},
	file = {Full Text:/Users/myounes/Zotero/storage/TKZDNERK/Tyler and Cutler - 2009 - Cross-language differences in cue use for speech s.pdf:application/pdf},
}

@article{yang_superb_2021,
	title = {{SUPERB}: {Speech} processing {Universal} {PERformance} {Benchmark}},
	shorttitle = {{SUPERB}},
	url = {http://arxiv.org/abs/2105.01051},
	abstract = {Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.},
	urldate = {2022-03-09},
	journal = {arXiv:2105.01051 [cs, eess]},
	author = {Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I. Jeff and Lakhotia, Kushal and Lin, Yist Y. and Liu, Andy T. and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and Huang, Tzu-Hsien and Tseng, Wei-Cheng and Lee, Ko-tik and Liu, Da-Rong and Huang, Zili and Dong, Shuyan and Li, Shang-Wen and Watanabe, Shinji and Mohamed, Abdelrahman and Lee, Hung-yi},
	month = oct,
	year = {2021},
	note = {arXiv: 2105.01051},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: To appear in Interspeech 2021},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/WIAHDZQ6/Yang et al. - 2021 - SUPERB Speech processing Universal PERformance Be.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/79GJ7KQZ/2105.html:text/html},
}

@article{mohamed_arabic_2021,
	title = {Arabic {Speech} {Emotion} {Recognition} {Employing} {Wav2vec2}.0 and {HuBERT} {Based} on {BAVED} {Dataset}},
	url = {http://arxiv.org/abs/2110.04425},
	abstract = {Recently, there have been tremendous research outcomes in the fields of speech recognition and natural language processing. This is due to the well-developed multi-layers deep learning paradigms such as wav2vec2.0, Wav2vecU, WavBERT, and HuBERT that provide better representation learning and high information capturing. Such paradigms run on hundreds of unlabeled data, then fine-tuned on a small dataset for specific tasks. This paper introduces a deep learning constructed emotional recognition model for Arabic speech dialogues. The developed model employs the state of the art audio representations include wav2vec2.0 and HuBERT. The experiment and performance results of our model overcome the previous known outcomes.},
	urldate = {2022-03-09},
	journal = {arXiv:2110.04425 [cs]},
	author = {Mohamed, Omar and Aly, Salah A.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.04425},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 6 pages, 6 figures},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/ZTGG22PD/Mohamed and Aly - 2021 - Arabic Speech Emotion Recognition Employing Wav2ve.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/UBBLYZ7S/2110.html:text/html},
}

@article{bansal_self-supervised_2020,
	title = {Self-{Supervised} {Meta}-{Learning} for {Few}-{Shot} {Natural} {Language} {Classification} {Tasks}},
	url = {http://arxiv.org/abs/2009.08445},
	abstract = {Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient -- when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.},
	urldate = {2022-03-09},
	journal = {arXiv:2009.08445 [cs]},
	author = {Bansal, Trapit and Jha, Rishikesh and Munkhdalai, Tsendsuren and McCallum, Andrew},
	month = nov,
	year = {2020},
	note = {arXiv: 2009.08445},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
	annote = {Comment: To appear in EMNLP 2020, camera-ready, link to code added},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/VILMEJJ2/Bansal et al. - 2020 - Self-Supervised Meta-Learning for Few-Shot Natural.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/Q59CTKIM/2009.html:text/html},
}

@article{albadr_spoken_2018,
	title = {Spoken language identification based on the enhanced self-adjusting extreme learning machine approach},
	volume = {13},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0194770},
	doi = {10.1371/journal.pone.0194770},
	language = {en},
	number = {4},
	urldate = {2022-03-09},
	journal = {PLOS ONE},
	author = {Albadr, Musatafa Abbas Abbood and Tiun, Sabrina and AL-Dhief, Fahad Taha and Sammour, Mahmoud A. M.},
	editor = {Li, Xiangtao},
	month = apr,
	year = {2018},
	keywords = {notion},
	pages = {e0194770},
	file = {Full Text:/Users/myounes/Zotero/storage/8MUNKXMK/Albadr et al. - 2018 - Spoken language identification based on the enhanc.pdf:application/pdf},
}

@misc{jonathan_bgn_hubert_2021,
	type = {Personal {Technical} {Blog}},
	title = {{HuBERT}: {How} to {Apply} {BERT} to {Speech}, {Visually} {Explained}},
	url = {https://jonathanbgn.com/2021/10/30/hubert-visually-explained.html},
	language = {English},
	journal = {Jonathan Bgn},
	author = {{Jonathan Bgn}},
	month = oct,
	year = {2021},
	keywords = {notion},
}

@misc{jonathan_bgn_wav2vec2_2021,
  type     = {Personal {Technical} {Blog}},
  title    = {An {Illustrated} {Tour} of {Wav2vec 2.0}},
  url      = {https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html},
  language = {English},
  journal  = {Jonathan Bgn},
  author   = {{Jonathan Bgn}},
  month    = oct,
  year     = {2021},
  keywords = {notion}
}

@article{babu_xls-r_2021,
	title = {{XLS}-{R}: {Self}-supervised {Cross}-lingual {Speech} {Representation} {Learning} at {Scale}},
	shorttitle = {{XLS}-{R}},
	url = {http://arxiv.org/abs/2111.09296},
	abstract = {This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34\% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.},
	urldate = {2022-03-11},
	journal = {arXiv:2111.09296 [cs, eess]},
	author = {Babu, Arun and Wang, Changhan and Tjandra, Andros and Lakhotia, Kushal and Xu, Qiantong and Goyal, Naman and Singh, Kritika and von Platen, Patrick and Saraf, Yatharth and Pino, Juan and Baevski, Alexei and Conneau, Alexis and Auli, Michael},
	month = dec,
	year = {2021},
	note = {arXiv: 2111.09296},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/3J9MRUJA/Babu et al. - 2021 - XLS-R Self-supervised Cross-lingual Speech Repres.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/DTB5ZVZN/2111.html:text/html},
}

@article{caron_deep_2019,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	url = {http://arxiv.org/abs/1807.05520},
	abstract = {Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.},
	urldate = {2022-03-11},
	journal = {arXiv:1807.05520 [cs]},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	month = mar,
	year = {2019},
	note = {arXiv: 1807.05520},
	keywords = {notion, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at ECCV 2018},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/HNLAGH8S/Caron et al. - 2019 - Deep Clustering for Unsupervised Learning of Visua.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/VK6ATT26/1807.html:text/html},
}

@article{baevski_effectiveness_2020,
	title = {Effectiveness of self-supervised pre-training for speech recognition},
	url = {http://arxiv.org/abs/1911.03912},
	abstract = {We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on testclean, while achieving a 25\% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.},
	urldate = {2022-03-11},
	journal = {arXiv:1911.03912 [cs]},
	author = {Baevski, Alexei and Auli, Michael and Mohamed, Abdelrahman},
	month = may,
	year = {2020},
	note = {arXiv: 1911.03912},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/96VX5NPN/Baevski et al. - 2020 - Effectiveness of self-supervised pre-training for .pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/MBF3PXEV/1911.html:text/html},
}

@article{lan_albert_2020,
	title = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
	shorttitle = {{ALBERT}},
	url = {http://arxiv.org/abs/1909.11942},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and {\textbackslash}squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
	urldate = {2022-03-11},
	journal = {arXiv:1909.11942 [cs]},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.11942},
	keywords = {Computer Science - Computation and Language, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/QR5WVEJU/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/KD3QYL63/1909.html:text/html},
}

@misc{alexei_baevski_first_2022,
	title = {The first high-performance self-supervised algorithm that works for speech, vision, and text},
	url = {https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/},
	language = {English},
	author = {{Alexei Baevski} and Hsu, Wei-Ning and {Qiantong Xu} and {Arun Babu} and Gu, Jiatao and {Michael Auli}},
	month = jan,
	year = {2022},
	keywords = {notion},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2022-03-12},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
	keywords = {Computer Science - Computation and Language, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/CTDNW69V/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/TVPISKXV/1907.html:text/html},
}

@article{chung_w2v-bert_2021,
	title = {W2v-{BERT}: {Combining} {Contrastive} {Learning} and {Masked} {Language} {Modeling} for {Self}-{Supervised} {Speech} {Pre}-{Training}},
	shorttitle = {W2v-{BERT}},
	url = {http://arxiv.org/abs/2108.06209},
	abstract = {Motivated by the success of masked language modeling{\textasciitilde}(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks{\textasciitilde}(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light{\textasciitilde}60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec{\textasciitilde}2.0 and HuBERT, our model shows{\textasciitilde}5{\textbackslash}\% to{\textasciitilde}10{\textbackslash}\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec{\textasciitilde}2.0 by more than{\textasciitilde}30{\textbackslash}\% relatively.},
	urldate = {2022-03-17},
	journal = {arXiv:2108.06209 [cs, eess]},
	author = {Chung, Yu-An and Zhang, Yu and Han, Wei and Chiu, Chung-Cheng and Qin, James and Pang, Ruoming and Wu, Yonghui},
	month = sep,
	year = {2021},
	note = {arXiv: 2108.06209},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/45G6ES7F/Chung et al. - 2021 - W2v-BERT Combining Contrastive Learning and Maske.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/TPMU63H7/2108.html:text/html},
}

@phdthesis{michael_ellis_accent_nodate,
	title = {Accent {Identification} for {English} {Speakers}},
	abstract = {In this thesis, we look at using semi-supervised models to identify the accent of English
speech samples, with the future aim of using such models for classifying the accent of
non-English speech samples. We focus on semi-supervised models as these can take
advantage of the relatively abundant unlabeled data of less-resourced languages. Using
these methods, we achieve an accuracy of 63.2\% on a dataset of English speech contain-
ing ten accents created from Mozilla Common Voice. We perform a series of ablations
where we reduce the number of samples in our training set and  nd limited bene t to
including more than 12 samples from a given speaker, and we also  nd that having a
breadth of speakers is signi cantly more important than having a breadth of samples.},
	school = {University of New South Wales},
	author = {{Michael Ellis}},
	keywords = {notion},
	file = {Michael Ellis - Accent Identification for English Speakers.pdf:/Users/myounes/Zotero/storage/4ATRWGSL/Michael Ellis - Accent Identification for English Speakers.pdf:application/pdf},
}

@inproceedings{sanket_shah_firstworkshop_2020,
	title = {{FirstWorkshop} on {Speech} {Processing} for {Code}-switching in {Multilingual} {Communities}: {Shared} {Task} on {Code}-switched {Spoken} {Language} {Identification}},
	url = {http://festvox.org/cedar/WSTCSMC2020.pdf},
	abstract = {Code-switched speech and language processing is challenging
due to the paucity of publicly-available datasets for research.
We describe a shared task on language identification
from speech organized as part of the First Workshop on Speech
Technologies for Code-switching in Multilingual Communities.
The shared task consisted two sub-tasks 1. Spoken Language
Identification at the utterance level, where the goal was
to classify an utterance as monolingual or code-switched 2.
Spoken Language Identification within a code-switched utterance,
where the goal was to identify languages at a frame-level.
We released a dataset consisting of 60 hours of data in three
language pairs - Tamil-English, Telugu-English and Gujarati-
English. Six teams participated in the utterance level task, while
two teams participated in the frame-level task. Team VocapiaLIMSI
used a combination of i-vector and phonotactic-based
models to achieve the best performance across languages on
both tasks. We hope that this dataset, which is now available
for research purposes will encourage research in code-switched
spoken language identification.},
	language = {English},
	publisher = {Microsoft Research India, Microsoft Corporation},
	author = {{Sanket Shah} and {Sunayana Sitaram} and {Rupeshkumar Mehta}},
	month = oct,
	year = {2020},
	keywords = {notion},
	pages = {24--28},
	file = {Sanket Shah et al. - FirstWorkshop on Speech Processing for Code-switch.pdf:/Users/myounes/Zotero/storage/WXDBDNXB/Sanket Shah et al. - FirstWorkshop on Speech Processing for Code-switch.pdf:application/pdf},
}

@article{chowdhury_towards_2021,
	title = {Towards {One} {Model} to {Rule} {All}: {Multilingual} {Strategy} for {Dialectal} {Code}-{Switching} {Arabic} {ASR}},
	shorttitle = {Towards {One} {Model} to {Rule} {All}},
	url = {http://arxiv.org/abs/2105.14779},
	abstract = {With the advent of globalization, there is an increasing demand for multilingual automatic speech recognition (ASR), handling language and dialectal variation of spoken content. Recent studies show its efficacy over monolingual systems. In this study, we design a large multilingual end-to-end ASR using self-attention based conformer architecture. We trained the system using Arabic (Ar), English (En) and French (Fr) languages. We evaluate the system performance handling: (i) monolingual (Ar, En and Fr); (ii) multi-dialectal (Modern Standard Arabic, along with dialectal variation such as Egyptian and Moroccan); (iii) code-switching -- cross-lingual (Ar-En/Fr) and dialectal (MSA-Egyptian dialect) test cases, and compare with current state-of-the-art systems. Furthermore, we investigate the influence of different embedding/character representations including character vs word-piece; shared vs distinct input symbol per language. Our findings demonstrate the strength of such a model by outperforming state-of-the-art monolingual dialectal Arabic and code-switching Arabic ASR.},
	urldate = {2022-03-18},
	journal = {arXiv:2105.14779 [cs, eess]},
	author = {Chowdhury, Shammur Absar and Hussein, Amir and Abdelali, Ahmed and Ali, Ahmed},
	month = jul,
	year = {2021},
	note = {arXiv: 2105.14779},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion, Computer Science - Human-Computer Interaction},
	annote = {Comment: Accepted in INTERSPEECH 2021, Multilingual ASR, Multi-dialectal ASR, Code-Switching ASR, Arabic ASR, Conformer, Transformer, E2E ASR, Speech Recognition, ASR, Arabic, English, French},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/ATD54583/Chowdhury et al. - 2021 - Towards One Model to Rule All Multilingual Strate.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/3WJ7F7II/2105.html:text/html},
}

@article{mubarak_qasr_2021,
	title = {{QASR}: {QCRI} {Aljazeera} {Speech} {Resource} -- {A} {Large} {Scale} {Annotated} {Arabic} {Speech} {Corpus}},
	shorttitle = {{QASR}},
	url = {http://arxiv.org/abs/2106.13000},
	abstract = {We introduce the largest transcribed Arabic speech corpus, QASR, collected from the broadcast domain. This multi-dialect speech dataset contains 2,000 hours of speech sampled at 16kHz crawled from Aljazeera news channel. The dataset is released with lightly supervised transcriptions, aligned with the audio segments. Unlike previous datasets, QASR contains linguistically motivated segmentation, punctuation, speaker information among others. QASR is suitable for training and evaluating speech recognition systems, acoustics- and/or linguistics- based Arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other NLP modules for spoken data. In addition to QASR transcription, we release a dataset of 130M words to aid in designing and training a better language model. We show that end-to-end automatic speech recognition trained on QASR reports a competitive word error rate compared to the previous MGB-2 corpus. We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript. We also report the first baseline for Arabic punctuation restoration. We make the corpus available for the research community.},
	urldate = {2022-03-18},
	journal = {arXiv:2106.13000 [cs, eess]},
	author = {Mubarak, Hamdy and Hussein, Amir and Chowdhury, Shammur Absar and Ali, Ahmed},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.13000},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: Speech Corpus, Spoken Conversation, ASR, Dialect Identification, Punctuation Restoration, Speaker Verification, NER, Named Entity, Arabic, Speaker gender, Turn-taking Accepted in ACL 2021},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/TP3I3UFP/Mubarak et al. - 2021 - QASR QCRI Aljazeera Speech Resource -- A Large Sc.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/YNLNMJ7J/2106.html:text/html},
}

@article{alfter_language_2015,
	title = {Language {Segmentation}},
	url = {http://arxiv.org/abs/1510.01717},
	abstract = {Language segmentation consists in finding the boundaries where one language ends and another language begins in a text written in more than one language. This is important for all natural language processing tasks. The problem can be solved by training language models on language data. However, in the case of low- or no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform better than supervised methods when it is difficult or impossible to train supervised approaches. A special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language. I compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words. The weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. The results look promising, but there is room for improvement and a more thorough investigation should be undertaken.},
	urldate = {2022-03-24},
	journal = {arXiv:1510.01717 [cs]},
	author = {Alfter, David},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.01717},
	keywords = {Computer Science - Computation and Language, notion},
	annote = {Comment: Master Thesis},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/M8HR47EI/Alfter - 2015 - Language Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/D7W7X4G3/1510.html:text/html},
}

@inproceedings{shon_adi17_2020,
	address = {Barcelona, Spain},
	title = {{ADI17}: {A} {Fine}-{Grained} {Arabic} {Dialect} {Identification} {Dataset}},
	isbn = {978-1-5090-6631-5},
	shorttitle = {{ADI17}},
	url = {https://ieeexplore.ieee.org/document/9052982/},
	doi = {10.1109/ICASSP40776.2020.9052982},
	urldate = {2022-03-24},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Shon, Suwon and Ali, Ahmed and Samih, Younes and Mubarak, Hamdy and Glass, James},
	month = may,
	year = {2020},
	keywords = {notion},
	pages = {8244--8248},
}

@article{abdul-mageed_nadi_2020,
	title = {{NADI} 2020: {The} {First} {Nuanced} {Arabic} {Dialect} {Identification} {Shared} {Task}},
	shorttitle = {{NADI} 2020},
	url = {http://arxiv.org/abs/2010.11334},
	abstract = {We present the results and findings of the First Nuanced Arabic Dialect Identification Shared Task (NADI). This Shared Task includes two subtasks: country-level dialect identification (Subtask 1) and province-level sub-dialect identification (Subtask 2). The data for the shared task covers a total of 100 provinces from 21 Arab countries and are collected from the Twitter domain. As such, NADI is the first shared task to target naturally-occurring fine-grained dialectal text at the sub-country level. A total of 61 teams from 25 countries registered to participate in the tasks, thus reflecting the interest of the community in this area. We received 47 submissions for Subtask 1 from 18 teams and 9 submissions for Subtask 2 from 9 teams.},
	urldate = {2022-03-24},
	journal = {arXiv:2010.11334 [cs]},
	author = {Abdul-Mageed, Muhammad and Zhang, Chiyu and Bouamor, Houda and Habash, Nizar},
	month = nov,
	year = {2020},
	note = {arXiv: 2010.11334},
	keywords = {Computer Science - Computation and Language, notion, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted in The Fifth Arabic Natural Language Processing Workshop (WANLP 2020)},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/ENSMPTLV/Abdul-Mageed et al. - 2020 - NADI 2020 The First Nuanced Arabic Dialect Identi.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/WRAY3FJ8/2010.html:text/html},
}

@inproceedings{salameh_fine-grained_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Fine-{Grained} {Arabic} {Dialect} {Identification}},
	url = {https://aclanthology.org/C18-1113},
	abstract = {Previous work on the problem of Arabic Dialect Identification typically targeted coarse-grained five dialect classes plus Standard Arabic (6-way classification). This paper presents the first results on a fine-grained dialect classification task covering 25 specific cities from across the Arab World, in addition to Standard Arabic – a very challenging task. We build several classification systems and explore a large space of features. Our results show that we can identify the exact city of a speaker at an accuracy of 67.9\% for sentences with an average length of 7 words (a 9\% relative error reduction over the state-of-the-art technique for Arabic dialect identification) and reach more than 90\% when we consider 16 words. We also report on additional insights from a data analysis of similarity and difference across Arabic dialects.},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Salameh, Mohammad and Bouamor, Houda and Habash, Nizar},
	month = aug,
	year = {2018},
	keywords = {notion},
	pages = {1332--1344},
}

@article{fan_exploring_2020,
	title = {Exploring wav2vec 2.0 on speaker verification and language identification},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2012.06185},
	doi = {10.48550/ARXIV.2012.06185},
	abstract = {Wav2vec 2.0 is a recently proposed self-supervised framework for speech representation learning. It follows a two-stage training process of pre-training and fine-tuning, and performs well in speech recognition tasks especially ultra-low resource cases. In this work, we attempt to extend self-supervised framework to speaker verification and language identification. First, we use some preliminary experiments to indicate that wav2vec 2.0 can capture the information about the speaker and language. Then we demonstrate the effectiveness of wav2vec 2.0 on the two tasks respectively. For speaker verification, we obtain a new state-of-the-art result, Equal Error Rate (EER) of 3.61\% on the VoxCeleb1 dataset. For language identification, we obtain an EER of 12.02\% on 1 second condition and an EER of 3.47\% on full-length condition of the AP17-OLR dataset. Finally, we utilize one model to achieve the unified modeling by the multi-task learning for the two tasks.},
	urldate = {2022-03-25},
	author = {Fan, Zhiyun and Li, Meng and Zhou, Shiyu and Xu, Bo},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {notion, Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Self-supervised, speaker verification, language identification, multi-task learning, wav2vec 2.0},
}

@article{zaidan_arabic_2014,
	title = {Arabic {Dialect} {Identification}},
	volume = {40},
	issn = {0891-2017, 1530-9312},
	url = {https://direct.mit.edu/coli/article/40/1/171-202/1458},
	doi = {10.1162/COLI_a_00169},
	abstract = {The written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-trivial manner from the various spoken regional dialects of Arabic—the true “native languages” of Arabic speakers. Those dialects, in turn, differ quite a bit from each other. However, due to MSA's prevalence in written form, almost all Arabic data sets have predominantly MSA content. In this article, we describe the creation of a novel Arabic resource with dialect annotations. We have created a large monolingual data set rich in dialectal Arabic content called the Arabic On-line Commentary Data set (Zaidan and Callison-Burch 2011). We describe our annotation effort to identify the dialect level (and dialect itself) in each of more than 100,000 sentences from the data set by crowdsourcing the annotation task, and delve into interesting annotator behaviors (like over-identification of one's own dialect). Using this new annotated data set, we consider the task of Arabic dialect identification: Given the word sequence forming an Arabic sentence, determine the variety of Arabic in which it is written. We use the data to train and evaluate automatic classifiers for dialect identification, and establish that classifiers using dialectal data significantly and dramatically outperform baselines that use MSA-only data, achieving near-human classification accuracy. Finally, we apply our classifiers to discover dialectical data from a large Web crawl consisting of 3.5 million pages mined from on-line Arabic newspapers.},
	language = {en},
	number = {1},
	urldate = {2022-03-26},
	journal = {Computational Linguistics},
	author = {Zaidan, Omar F. and Callison-Burch, Chris},
	month = mar,
	year = {2014},
	keywords = {notion, Arabic, Dialect Identification, Background},
	pages = {171--202},
	file = {Full Text:/Users/myounes/Zotero/storage/SN8PTTQN/Zaidan and Callison-Burch - 2014 - Arabic Dialect Identification.pdf:application/pdf},
}

@inproceedings{novotney_unsupervised_2011,
	title = {Unsupervised {Arabic} dialect adaptation with self-training},
	url = {https://www.isca-speech.org/archive/interspeech_2011/novotney11_interspeech.html},
	doi = {10.21437/Interspeech.2011-226},
	language = {en},
	urldate = {2022-03-27},
	booktitle = {Interspeech 2011},
	publisher = {ISCA},
	author = {Novotney, Scott and Schwartz, Rich and Khudanpur, Sanjeev},
	month = aug,
	year = {2011},
	keywords = {notion},
	pages = {541--544},
}

@inproceedings{habash_guidelines_2008,
	title = {Guidelines for annotation of {Arabic} dialectness},
	booktitle = {Proceedings of the {LREC} {Workshop} on {HLT} \& {NLP} within the {Arabic} world},
	author = {Habash, Nizar and Rambow, Owen and Diab, Mona and Kanjawi-Faraj, Reem},
	year = {2008},
	keywords = {notion},
	pages = {49--53},
}

@inproceedings{boril_arabic_2012,
	title = {Arabic dialect identification - "is the secret in the silence?" and other observations},
	shorttitle = {Arabic dialect identification - "is the secret in the silence?},
	url = {https://www.isca-speech.org/archive/interspeech_2012/boril12_interspeech.html},
	doi = {10.21437/Interspeech.2012-18},
	language = {en},
	urldate = {2022-03-28},
	booktitle = {Interspeech 2012},
	publisher = {ISCA},
	author = {Bořil, Hynek and Sangwan, Abhijeet and Hansen, John H. L.},
	month = sep,
	year = {2012},
	keywords = {notion},
	pages = {30--33},
	file = {Submitted Version:/Users/myounes/Zotero/storage/BSESN3A6/Bořil et al. - 2012 - Arabic dialect identification - is the secret in .pdf:application/pdf},
}

@article{lin_transformer-based_2020,
	title = {Transformer-based {Arabic} {Dialect} {Identification}},
	url = {http://arxiv.org/abs/2011.00699},
	abstract = {This paper presents a dialect identification (DID) system based on the transformer neural network architecture. The conventional convolutional neural network (CNN)-based systems use the shorter receptive fields. We believe that long range information is equally important for language and DID, and self-attention mechanism in transformer captures the long range dependencies. In addition, to reduce the computational complexity, self-attention with downsampling is used to process the acoustic features. This process extracts sparse, yet informative features. Our experimental results show that transformer outperforms CNN-based networks on the Arabic dialect identification (ADI) dataset. We also report that the score-level fusion of CNN and transformer-based systems obtains an overall accuracy of 86.29\% on the ADI17 database.},
	urldate = {2022-03-28},
	journal = {arXiv:2011.00699 [eess]},
	author = {Lin, Wanqiu and Madhavi, Maulik and Das, Rohan Kumar and Li, Haizhou},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.00699},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: Accepted for publication in International Conference on Asian Language Processing (IALP) 2020},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/VZREAJKC/Lin et al. - 2020 - Transformer-based Arabic Dialect Identification.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/KMRVXKRW/2011.html:text/html},
}

@inproceedings{biadsy_spoken_2009,
	address = {Athens, Greece},
	title = {Spoken {Arabic} dialect identification using phonotactic modeling},
	url = {http://portal.acm.org/citation.cfm?doid=1621774.1621784},
	doi = {10.3115/1621774.1621784},
	language = {en},
	urldate = {2022-03-29},
	booktitle = {Proceedings of the {EACL} 2009 {Workshop} on {Computational} {Approaches} to {Semitic} {Languages} - {Semitic} '09},
	publisher = {Association for Computational Linguistics},
	author = {Biadsy, Fadi and Hirschberg, Julia and Habash, Nizar},
	year = {2009},
	keywords = {notion},
	pages = {53},
	file = {Full Text:/Users/myounes/Zotero/storage/SYC6DH68/Biadsy et al. - 2009 - Spoken Arabic dialect identification using phonota.pdf:application/pdf},
}

@article{tjandra_improved_2021,
	title = {Improved {Language} {Identification} {Through} {Cross}-{Lingual} {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2107.04082},
	abstract = {Language identification greatly impacts the success of downstream tasks such as automatic speech recognition. Recently, self-supervised speech representations learned by wav2vec 2.0 have been shown to be very effective for a range of speech tasks. We extend previous self-supervised work on language identification by experimenting with pre-trained models which were learned on real-world unconstrained speech in multiple languages and not just on English. We show that models pre-trained on many languages perform better and enable language identification systems that require very little labeled data to perform well. Results on a 26 languages setup show that with only 10 minutes of labeled data per language, a cross-lingually pre-trained model can achieve over 89.2\% accuracy.},
	urldate = {2022-04-02},
	journal = {arXiv:2107.04082 [cs, eess]},
	author = {Tjandra, Andros and Choudhury, Diptanu Gon and Zhang, Frank and Singh, Kritika and Conneau, Alexis and Baevski, Alexei and Sela, Assaf and Saraf, Yatharth and Auli, Michael},
	month = oct,
	year = {2021},
	note = {arXiv: 2107.04082},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/K5TXEPGY/Tjandra et al. - 2021 - Improved Language Identification Through Cross-Lin.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/BGSQ7VPL/2107.html:text/html},
}

@inproceedings{biadsy_dialect_2011,
	title = {Dialect and {Accent} {Recognition} {Using} {Phonetic}-{Segmentation} {Supervectors}.},
	author = {Biadsy, Fadi and Hirschberg, Julia and Ellis, Daniel},
	month = jan,
	year = {2011},
	pages = {745--748},
}

@article{ali_arabic_2021,
	title = {Arabic {Code}-{Switching} {Speech} {Recognition} using {Monolingual} {Data}},
	url = {http://arxiv.org/abs/2107.01573},
	abstract = {Code-switching in automatic speech recognition (ASR) is an important challenge due to globalization. Recent research in multilingual ASR shows potential improvement over monolingual systems. We study key issues related to multilingual modeling for ASR through a series of large-scale ASR experiments. Our innovative framework deploys a multi-graph approach in the weighted finite state transducers (WFST) framework. We compare our WFST decoding strategies with a transformer sequence to sequence system trained on the same data. Given a code-switching scenario between Arabic and English languages, our results show that the WFST decoding approaches were more suitable for the intersentential code-switching datasets. In addition, the transformer system performed better for intrasentential code-switching task. With this study, we release an artificially generated development and test sets, along with ecological code-switching test set, to benchmark the ASR performance.},
	urldate = {2022-04-02},
	journal = {arXiv:2107.01573 [cs, eess]},
	author = {Ali, Ahmed and Chowdhury, Shammur and Hussein, Amir and Hifny, Yasser},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.01573},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: Accepted in Interspeech 2021, speech recognition, code-switching, ASR, transformer, WFST, graph approach},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/F2AIWGBP/Ali et al. - 2021 - Arabic Code-Switching Speech Recognition using Mon.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/XRRSJ3H8/2107.html:text/html},
}

@inproceedings{alhakeem_confidence_2021,
	title = {Confidence {Learning} from {Noisy} {Labels} for {Arabic} {Dialect} {Identification}},
	doi = {10.1109/ITC-CSCC52171.2021.9741073},
	booktitle = {2021 36th {International} {Technical} {Conference} on {Circuits}/{Systems}, {Computers} and {Communications} ({ITC}-{CSCC})},
	author = {Alhakeem, Zainab and Kang, Hong-Goo},
	year = {2021},
	keywords = {notion},
	pages = {1--4},
}

@inproceedings{ali_mgb-5_2019,
	title = {The {MGB}-5 {Challenge}: {Recognition} and {Dialect} {Identification} of {Dialectal} {Arabic} {Speech}},
	doi = {10.1109/ASRU46091.2019.9003960},
	booktitle = {2019 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	author = {Ali, Ahmed and Shon, Suwon and Samih, Younes and Mubarak, Hamdy and Abdelali, Ahmed and Glass, James and Renals, Steve and Choukri, Khalid},
	year = {2019},
	pages = {1026--1033},
}

@article{shon_convolutional_2018,
	title = {Convolutional {Neural} {Networks} and {Language} {Embeddings} for {End}-to-{End} {Dialect} {Recognition}},
	url = {http://arxiv.org/abs/1803.04567},
	abstract = {Dialect identification (DID) is a special case of general language identification (LID), but a more challenging problem due to the linguistic similarity between dialects. In this paper, we propose an end-to-end DID system and a Siamese neural network to extract language embeddings. We use both acoustic and linguistic features for the DID task on the Arabic dialectal speech dataset: Multi-Genre Broadcast 3 (MGB-3). The end-to-end DID system was trained using three kinds of acoustic features: Mel-Frequency Cepstral Coefficients (MFCCs), log Mel-scale Filter Bank energies (FBANK) and spectrogram energies. We also investigated a dataset augmentation approach to achieve robust performance with limited data resources. Our linguistic feature research focused on learning similarities and dissimilarities between dialects using the Siamese network, so that we can reduce feature dimensionality as well as improve DID performance. The best system using a single feature set achieves 73\% accuracy, while a fusion system using multiple features yields 78\% on the MGB-3 dialect test set consisting of 5 dialects. The experimental results indicate that FBANK features achieve slightly better results than MFCCs. Dataset augmentation via speed perturbation appears to add significant robustness to the system. Although the Siamese network with language embeddings did not achieve as good a result as the end-to-end DID system, the two approaches had good synergy when combined together in a fused system.},
	urldate = {2022-04-03},
	journal = {arXiv:1803.04567 [cs, eess]},
	author = {Shon, Suwon and Ali, Ahmed and Glass, James},
	month = apr,
	year = {2018},
	note = {arXiv: 1803.04567},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: Speaker Odyssey 2018, The Speaker and Language Recognition Workshop},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/R2Y9RW9Q/Shon et al. - 2018 - Convolutional Neural Networks and Language Embeddi.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/JHECTLD6/1803.html:text/html},
}

@inproceedings{ramesh_self-supervised_2021,
	title = {Self-{Supervised} {Phonotactic} {Representations} for {Language} {Identification}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/ramesh21_interspeech.html},
	doi = {10.21437/Interspeech.2021-1310},
	language = {en},
	urldate = {2022-04-03},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Ramesh, G. and Kumar, C. Shiva and Murty, K. Sri Rama},
	month = aug,
	year = {2021},
	pages = {1514--1518},
}

@article{lopez-moreno_use_2016,
	title = {On the use of deep feedforward neural networks for automatic language identification},
	volume = {40},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S088523081530036X},
	doi = {10.1016/j.csl.2016.03.001},
	language = {en},
	urldate = {2022-04-06},
	journal = {Computer Speech \& Language},
	author = {Lopez-Moreno, Ignacio and Gonzalez-Dominguez, Javier and Martinez, David and Plchot, Oldřich and Gonzalez-Rodriguez, Joaquin and Moreno, Pedro J.},
	month = nov,
	year = {2016},
	keywords = {notion},
	pages = {46--59},
	file = {Full Text:/Users/myounes/Zotero/storage/EWVDA69Q/Lopez-Moreno et al. - 2016 - On the use of deep feedforward neural networks for.pdf:application/pdf},
}

@inproceedings{lounnas_building_2019,
	title = {Building a {Speech} {Corpus} based on {Arabic} {Podcasts} for {Language} and {Dialect} {Identification}},
	author = {Lounnas, Khaled and Abbas, Mourad and Lichouri, Mohamed},
	month = sep,
	year = {2019},
	keywords = {notion, dataset},
}

@article{wang_fine-tuned_2021,
	title = {A {Fine}-tuned {Wav2vec} 2.0/{HuBERT} {Benchmark} {For} {Speech} {Emotion} {Recognition}, {Speaker} {Verification} and {Spoken} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2111.02735},
	abstract = {Self-supervised speech representations such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, self-supervised models have not been totally proved to produce better performance on tasks other than ASR. In this work, we explore partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks : Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. We also compare pre-trained models with/without ASR fine-tuning. With simple down-stream frameworks, the best scores reach 79.58\% weighted accuracy for Speech Emotion Recognition on IEMOCAP, 2.36\% equal error rate for Speaker Verification on VoxCeleb1, 87.51\% accuracy for Intent Classification and 75.32\% F1 for Slot Filling on SLURP, thus setting a new state-of-the-art for these three benchmarks, proving that fine-tuned wav2vec 2.0 and HuBERT models can better learn prosodic, voice-print and semantic representations.},
	urldate = {2022-04-12},
	journal = {arXiv:2111.02735 [cs, eess]},
	author = {Wang, Yingzhi and Boumadane, Abdelmoumene and Heba, Abdelwahab},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.02735},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion, Computer Science - Neural and Evolutionary Computing, finetunning},
	annote = {Comment: 5 pages, 2 figures},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/4K64UB22/Wang et al. - 2021 - A Fine-tuned Wav2vec 2.0HuBERT Benchmark For Spee.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/6B7TVBBZ/2111.html:text/html},
}

@misc{noauthor_xls-r_nodate,
	title = {{XLS}-{R}: {Self}-supervised speech processing for 128 languages},
	url = {https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages/},
	month = nov,
	keywords = {notion},
}

@article{cornegruta_modelling_2016,
	title = {Modelling {Radiological} {Language} with {Bidirectional} {Long} {Short}-{Term} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1609.08409},
	abstract = {Motivated by the need to automate medical information extraction from free-text radiological reports, we present a bi-directional long short-term memory (BiLSTM) neural network architecture for modelling radiological language. The model has been used to address two NLP tasks: medical named-entity recognition (NER) and negation detection. We investigate whether learning several types of word embeddings improves BiLSTM's performance on those tasks. Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser. Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks.},
	urldate = {2022-04-14},
	journal = {arXiv:1609.08409 [cs, stat]},
	author = {Cornegruta, Savelie and Bakewell, Robert and Withey, Samuel and Montana, Giovanni},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.08409},
	keywords = {BiLTSM, Computer Science - Computation and Language, notion, Statistics - Machine Learning},
	annote = {Comment: LOUHI 2016 conference proceedings},
	annote = {Comment: LOUHI 2016 conference proceedings},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/UZW9QZ94/Cornegruta et al. - 2016 - Modelling Radiological Language with Bidirectional.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/CSU4NBUI/1609.html:text/html},
}

@article{miao_lstm-tdnn_2019,
	title = {{LSTM}-{TDNN} with convolutional front-end for {Dialect} {Identification} in the 2019 {Multi}-{Genre} {Broadcast} {Challenge}},
	url = {http://arxiv.org/abs/1912.09003},
	abstract = {This paper presents a novel Dialect Identification (DID) system developed for the Fifth Edition of the Multi-Genre Broadcast challenge, the task of Fine-grained Arabic Dialect Identification (MGB-5 ADI Challenge). The system improves upon traditional DNN x-vector performance by employing a Convolutional and Long Short Term Memory-Recurrent (CLSTM) architecture to combine the benefits of a convolutional neural network front-end for feature extraction and a back-end recurrent neural to capture longer temporal dependencies. Furthermore we investigate intensive augmentation of one low resource dialect in the highly unbalanced training set using time-scale modification (TSM). This converts an utterance to several time-stretched or time-compressed versions, subsequently used to train the CLSTM system without using any other corpus. In this paper, we also investigate speech augmentation using MUSAN and the RIR datasets to increase the quantity and diversity of the existing training data in the normal way. Results show firstly that the CLSTM architecture outperforms a traditional DNN x-vector implementation. Secondly, adopting TSM-based speed perturbation yields a small performance improvement for the unbalanced data, finally that traditional data augmentation techniques yield further benefit, in line with evidence from related speaker and language recognition tasks. Our system achieved 2nd place ranking out of 15 entries in the MGB-5 ADI challenge, presented at ASRU 2019.},
	urldate = {2022-04-15},
	journal = {arXiv:1912.09003 [cs, eess]},
	author = {Miao, Xiaoxiao and McLoughlin, Ian},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.09003},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/9QI9V676/Miao and McLoughlin - 2019 - LSTM-TDNN with convolutional front-end for Dialect.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/PZDIFEX6/1912.html:text/html},
}

@inproceedings{miao_new_2019,
	title = {A {New} {Time}-{Frequency} {Attention} {Mechanism} for {TDNN} and {CNN}-{LSTM}-{TDNN}, with {Application} to {Language} {Identification}},
	url = {https://www.isca-speech.org/archive/interspeech_2019/miao19b_interspeech.html},
	doi = {10.21437/Interspeech.2019-1256},
	language = {en},
	urldate = {2022-04-15},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Miao, Xiaoxiao and McLoughlin, Ian and Yan, Yonghong},
	month = sep,
	year = {2019},
	keywords = {notion},
	pages = {4080--4084},
}

@inproceedings{pohjalainen_spectral_2016,
	address = {Amsterdam The Netherlands},
	title = {Spectral and {Cepstral} {Audio} {Noise} {Reduction} {Techniques} in {Speech} {Emotion} {Recognition}},
	isbn = {978-1-4503-3603-1},
	url = {https://dl.acm.org/doi/10.1145/2964284.2967306},
	doi = {10.1145/2964284.2967306},
	language = {en},
	urldate = {2022-04-15},
	booktitle = {Proceedings of the 24th {ACM} international conference on {Multimedia}},
	publisher = {ACM},
	author = {Pohjalainen, Jouni and Fabien Ringeval, Fabien and Zhang, Zixing and Schuller, Björn},
	month = oct,
	year = {2016},
	keywords = {notion, noise},
	pages = {670--674},
	file = {Submitted Version:/Users/myounes/Zotero/storage/B54E6AVC/Pohjalainen et al. - 2016 - Spectral and Cepstral Audio Noise Reduction Techni.pdf:application/pdf},
}

@article{soon_evaluating_2020,
	title = {Evaluating the {Effect} of {Multiple} {Filters} in {Automatic} {Language} {Identification} without {Lexical} {Knowledge}},
	volume = {11},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=11&Issue=10&Code=IJACSA&SerialNo=79},
	doi = {10.14569/IJACSA.2020.0111079},
	language = {en},
	number = {10},
	urldate = {2022-04-15},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Soon, Guan-Lip and Samsudin, Nur-Hana and Lim, Dennis},
	year = {2020},
	keywords = {notion, noise},
	file = {Full Text:/Users/myounes/Zotero/storage/P4FQ7Q2V/Soon et al. - 2020 - Evaluating the Effect of Multiple Filters in Autom.pdf:application/pdf},
}

@article{zhang_survey_2021,
	title = {A {Survey} on {Negative} {Transfer}},
	url = {http://arxiv.org/abs/2009.00909},
	abstract = {Transfer learning (TL) utilizes data or knowledge from one or more source domains to facilitate the learning in a target domain. It is particularly useful when the target domain has very few or no labeled data, due to annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of TL is not always guaranteed. Negative transfer (NT), i.e., leveraging source domain data/knowledge undesirably reduces the learning performance in the target domain, has been a long-standing and challenging problem in TL. Various approaches have been proposed in the literature to handle it. However, there does not exist a systematic survey on the formulation of NT, the factors leading to NT, and the algorithms that mitigate NT. This paper fills this gap, by first introducing the definition of NT and its factors, then reviewing about fifty representative approaches for overcoming NT, according to four categories: secure transfer, domain similarity estimation, distant transfer, and NT mitigation. NT in related fields, e.g., multi-task learning, lifelong learning, and adversarial attacks, are also discussed.},
	urldate = {2022-04-25},
	journal = {arXiv:2009.00909 [cs, stat]},
	author = {Zhang, Wen and Deng, Lingfei and Zhang, Lei and Wu, Dongrui},
	month = aug,
	year = {2021},
	note = {arXiv: 2009.00909},
	keywords = {Computer Science - Machine Learning, notion, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, transfer learning},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/LHBKUYPV/Zhang et al. - 2021 - A Survey on Negative Transfer.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/9FB88LQX/2009.html:text/html},
}

@article{shon_unsupervised_2018,
	title = {Unsupervised {Representation} {Learning} of {Speech} for {Dialect} {Identification}},
	url = {http://arxiv.org/abs/1809.04458},
	abstract = {In this paper, we explore the use of a factorized hierarchical variational autoencoder (FHVAE) model to learn an unsupervised latent representation for dialect identification (DID). An FHVAE can learn a latent space that separates the more static attributes within an utterance from the more dynamic attributes by encoding them into two different sets of latent variables. Useful factors for dialect identification, such as phonetic or linguistic content, are encoded by a segmental latent variable, while irrelevant factors that are relatively constant within a sequence, such as a channel or a speaker information, are encoded by a sequential latent variable. The disentanglement property makes the segmental latent variable less susceptible to channel and speaker variation, and thus reduces degradation from channel domain mismatch. We demonstrate that on fully-supervised DID tasks, an end-to-end model trained on the features extracted from the FHVAE model achieves the best performance, compared to the same model trained on conventional acoustic features and an i-vector based system. Moreover, we also show that the proposed approach can leverage a large amount of unlabeled data for FHVAE training to learn domain-invariant features for DID, and significantly improve the performance in a low-resource condition, where the labels for the in-domain data are not available.},
	urldate = {2022-04-26},
	journal = {arXiv:1809.04458 [cs, eess]},
	author = {Shon, Suwon and Hsu, Wei-Ning and Glass, James},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.04458},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: Accepted at SLT 2018},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/GEZZ9CAZ/Shon et al. - 2018 - Unsupervised Representation Learning of Speech for.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/688TJ3I2/1809.html:text/html},
}

@article{zhang_languagedialect_2018,
	title = {Language/{Dialect} {Recognition} {Based} on {Unsupervised} {Deep} {Learning}},
	volume = {26},
	doi = {10.1109/TASLP.2018.2797420},
	number = {5},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zhang, Qian and Hansen, John H. L.},
	year = {2018},
	pages = {873--882},
}

@article{tseng_mandarin-english_2021,
	title = {Mandarin-{English} {Code}-switching {Speech} {Recognition} with {Self}-supervised {Speech} {Representation} {Models}},
	url = {http://arxiv.org/abs/2110.03504},
	abstract = {Code-switching (CS) is common in daily conversations where more than one language is used within a sentence. The difficulties of CS speech recognition lie in alternating languages and the lack of transcribed data. Therefore, this paper uses the recently successful self-supervised learning (SSL) methods to leverage many unlabeled speech data without CS. We show that hidden representations of SSL models offer frame-level language identity even if the models are trained with English speech only. Jointly training CTC and language identification modules with self-supervised speech representations improves CS speech recognition performance. Furthermore, using multilingual speech data for pre-training obtains the best CS speech recognition.},
	urldate = {2022-04-28},
	journal = {arXiv:2110.03504 [cs, eess]},
	author = {Tseng, Liang-Hsuan and Fu, Yu-Kuan and Chang, Heng-Jui and Lee, Hung-yi},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.03504},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, notion},
	annote = {Comment: Submitted to ICASSP 2022},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/BLAVWQRI/Tseng et al. - 2021 - Mandarin-English Code-switching Speech Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/DG8P34BV/2110.html:text/html},
}

@misc{hershey_cnn_2017,
	title = {{CNN} {Architectures} for {Large}-{Scale} {Audio} {Classification}},
	url = {http://arxiv.org/abs/1609.09430},
	abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
	urldate = {2022-10-28},
	publisher = {arXiv},
	author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
	month = jan,
	year = {2017},
	note = {Number: arXiv:1609.09430
arXiv:1609.09430 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, notion, Statistics - Machine Learning},
	annote = {Comment: Accepted for publication at ICASSP 2017 Changes: Added definitions of mAP, AUC, and d-prime. Updated mAP/AUC/d-prime numbers for Audio Set based on changes of latest Audio Set revision. Changed wording to fit 4 page limit with new additions},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/MLL4EMQV/Hershey et al. - 2017 - CNN Architectures for Large-Scale Audio Classifica.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/VK4XWPQX/1609.html:text/html},
}

@phdthesis{pascal_fivian_speech_2021,
	address = {Switzerland},
	title = {Speech {Classification} using wav2vec 2.0},
	url = {https://www.zhaw.ch/storage/engineering/institute-zentren/cai/BA21_Speech_Classification_Reiser_Fivian.pdf},
	school = {ZHAW School of Engineering},
	author = {{Pascal Fivian} and {Dominique Reiser}},
	month = jun,
	year = {2021},
	keywords = {notion},
}

@misc{jay_alammar_illustrated_nodate,
	title = {The {Illustrated} {Transformer}},
	url = {https://jalammar.github.io/illustrated-transformer/},
	author = {{Jay Alammar}},
	keywords = {notion},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-11-20},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/myounes/Zotero/storage/ET27GNNZ/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/myounes/Zotero/storage/982GV3C8/1706.html:text/html},
}
