Wed Oct 5 20:12:51 AEDT 2022
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 05/10/2022 20:12:58

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_100f_devdata
train_filename: test_u_100f
evaluation_filename: train_u_50f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 2
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
per_device_train_batch_size: 10
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/test_u_100f.csv
--> data_test_fp: data/train_u_50f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_100f_devdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_100f_devdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> Creating blank confusion matrix ... -----------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 5 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.7253, -0.7630, -0.8254,  ...,  0.0000,  0.0000,  0.0000],
        [-1.4101, -1.1821, -0.9439,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.3682,  1.3732,  1.2511,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-1.1085, -0.8714, -0.7600,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2329, -0.2041, -0.1487,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.3997,  1.4579,  1.0367,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 3, 2, 0, 3, 3, 2, 3, 1, 0])}
Training DataCustom Files: 398
Training Data Files: 40
Test Data Sample
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.weight', 'quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.codevectors', 'project_hid.weight', 'quantizer.weight_proj.weight', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'projector.weight', 'projector.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'input_values': tensor([[-0.0670, -0.2170, -0.1622,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.7912,  0.6228,  0.7462,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1637, -0.2559, -0.3329,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.2837,  0.2537, -0.1200,  ...,  0.0000,  0.0000,  0.0000],
        [-1.1001, -1.1458, -1.1482,  ...,  0.0000,  0.0000,  0.0000],
        [ 3.3494,  3.9602,  4.3515,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 0, 0, 3, 0, 3, 3, 1, 1, 1])}
Test CustomData Files: 195
Test Data Files: 20
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
GPUs Used :  2 GPUs!
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 198660
start train
Traceback (most recent call last):
  File "run_basic.py", line 830, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 632, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_basic.py", line 656, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_basic.py", line 698, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1793, in forward
    outputs = self.wav2vec2(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1304, in forward
    encoder_outputs = self.encoder(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 782, in forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 235, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 96, in forward
    outputs = run_function(*args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 778, in custom_forward
    return module(*inputs, output_attentions)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 675, in forward
    hidden_states, attn_weights, _ = self.attention(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 613, in forward
    attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 7.15 GiB (GPU 0; 31.75 GiB total capacity; 20.25 GiB already allocated; 4.86 GiB free; 25.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Wed Oct 5 20:23:14 AEDT 2022
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 05/10/2022 20:23:21

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_100f_devdata
train_filename: test_u_100f
evaluation_filename: train_u_50f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 2
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
per_device_train_batch_size: 10
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/test_u_100f.csv
--> data_test_fp: data/train_u_50f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_100f_devdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_100f_devdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> Creating blank confusion matrix ... -----------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 5 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-6.9867e-03, -1.0245e-02, -5.3576e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9295e-02,  1.9548e-02,  1.7013e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.4715e-02,  7.7301e-03, -3.9325e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.2691e+00,  2.2280e+00,  1.7782e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.0935e-03,  8.2778e-03, -2.5447e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.0404e-02, -7.0723e-02, -8.1236e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 3, 1, 1, 2, 2, 1, 3, 3])}
Training DataCustom Files: 398
Training Data Files: 40
Test Data Sample
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_q.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.weight', 'projector.weight', 'projector.bias', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'input_values': tensor([[-0.4114, -0.3216, -0.2536,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0669, -0.0643, -0.0773,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0986, -0.0721,  0.1168,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0480,  0.0305, -0.1928,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0627,  0.0385,  0.0145,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8380, -0.8204, -0.7205,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 2, 1, 3, 3, 3, 2, 3, 0, 0])}
Test CustomData Files: 195
Test Data Files: 20
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
GPUs Used :  2 GPUs!
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 198660
start train
Traceback (most recent call last):
  File "run_basic.py", line 830, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 632, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_basic.py", line 656, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_basic.py", line 698, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1793, in forward
    outputs = self.wav2vec2(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1304, in forward
    encoder_outputs = self.encoder(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 782, in forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 235, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 96, in forward
    outputs = run_function(*args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 778, in custom_forward
    return module(*inputs, output_attentions)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 675, in forward
    hidden_states, attn_weights, _ = self.attention(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 613, in forward
    attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 7.15 GiB (GPU 0; 31.75 GiB total capacity; 20.25 GiB already allocated; 4.86 GiB free; 25.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Wed Oct 5 20:49:26 AEDT 2022
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 05/10/2022 20:49:30

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_100f_devdata
train_filename: test_u_100f
evaluation_filename: train_u_50f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 2
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/test_u_100f.csv
--> data_test_fp: data/train_u_50f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_100f_devdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_100f_devdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> Creating blank confusion matrix ... -----------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 5 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0276, -0.0339,  0.0972,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0476, -0.0567, -0.0039,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8491, -0.9186, -1.2556,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2792,  0.6510,  0.8251,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 3, 0, 2])}
Training DataCustom Files: 398
Training Data Files: 100
Test Data Sample
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.bias', 'projector.weight', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'input_values': tensor([[ 2.1443,  1.0528, -0.1578,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2548,  0.2276,  0.2028,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0619,  0.1052,  0.0936,  ...,  0.0000,  0.0000,  0.0000],
        [-0.7555, -0.5338, -0.4640,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 1, 3])}
Test CustomData Files: 195
Test Data Files: 49
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
GPUs Used :  2 GPUs!
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 198660
start train
start validation
Epoch 0 Train Acc 26.75% Val Acc 24.836734771728516% Train Loss 0.6938591599464417 Val Loss 1.3861247301101685
Trainable Parameters : 198660
start train
start validation
Epoch 1 Train Acc 25.75% Val Acc 25.163265228271484% Train Loss 0.6937351226806641 Val Loss 1.3885248899459839
Trainable Parameters : 198660
start train
start validation
Epoch 2 Train Acc 29.75% Val Acc 26.183673858642578% Train Loss 0.6908717751502991 Val Loss 1.3921856880187988
Trainable Parameters : 198660
start train
start validation
Epoch 3 Train Acc 32.0% Val Acc 23.979591369628906% Train Loss 0.687953770160675 Val Loss 1.3971894979476929
Trainable Parameters : 198660
start train
start validation
Epoch 4 Train Acc 31.0% Val Acc 23.122447967529297% Train Loss 0.6848595142364502 Val Loss 1.4063589572906494
Trainable Parameters : 198660
start train
start validation
Epoch 5 Train Acc 32.5% Val Acc 21.59183692932129% Train Loss 0.6792348623275757 Val Loss 1.41541588306427
Trainable Parameters : 198660
start train
start validation
Epoch 6 Train Acc 39.25% Val Acc 23.306121826171875% Train Loss 0.6744768619537354 Val Loss 1.4332448244094849
Trainable Parameters : 198660
start train
start validation
Epoch 7 Train Acc 39.75% Val Acc 21.59183692932129% Train Loss 0.6711426377296448 Val Loss 1.4447318315505981
Trainable Parameters : 198660
start train
start validation
Epoch 8 Train Acc 43.5% Val Acc 22.448978424072266% Train Loss 0.6719842553138733 Val Loss 1.4545320272445679
Trainable Parameters : 198660
