Mon Oct 10 14:38:48 AEDT 2022
------------------------------------------------------------------------
                         run_w2v.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_w2vlid.py
Started: 10/10/2022 14:38:51

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-w2vlid
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_500f_devdata
train_filename: test_u_500f
evaluation_filename: train_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 12
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/test_u_500f.csv
--> data_test_fp: data/train_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_500f_devdata_local/wav2vec-ADI17-w2vlid
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_500f_devdata_local/wav2vec-ADI17-w2vlid_finetuned_results.csv
--> pretrained_mod: log0/wav2vec2-base-lang-id

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 5 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Downloading:   0%|          | 0.00/214 [00:00<?, ?B/s]Downloading: 100%|██████████| 214/214 [00:00<00:00, 415kB/s]Check data has been processed correctly... 
Train Data Sample

{'input_values': tensor([[ 0.1120,  0.1594,  0.0665,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2497,  0.2766,  0.3362,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1222, -0.1254, -0.1511,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0609,  0.0708,  0.0716,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0481, -0.0390, -0.0538,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0142,  0.0212,  0.0222,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 2, 1, 3, 2, 0, 1, 3, 0, 3, 1])}
Training DataCustom Files: 1963
Training Data Files: 164
Test Data Sample
You are using a model of type hubert to instantiate a model of type wav2vec2. This is not supported for all configurations of models and can yield errors.
{'input_values': tensor([[-0.0030, -0.0028, -0.0034,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0151, -0.0169, -0.0137,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0373,  0.0366,  0.0316,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0020,  0.0029,  0.0041,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0292, -0.0401, -0.0326,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0086, -0.0043, -0.0050,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 3, 0, 3, 3, 3, 0, 0, 1, 3, 3])}
Test CustomData Files: 398
Test Data Files: 34
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
Traceback (most recent call last):
  File "run_w2vlid.py", line 462, in <module>
    model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2225, in from_pretrained
    model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2356, in _load_pretrained_model
    raise ValueError(
ValueError: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?
Wed Oct 12 12:55:59 AEDT 2022
Traceback (most recent call last):
  File "run_w2vlid.py", line 37, in <module>
    from transformers import (
ImportError: cannot import name 'HubertForSequencrClassification' from 'transformers' (/home/z5208494/.local/lib/python3.8/site-packages/transformers/__init__.py)
Wed Oct 12 13:06:12 AEDT 2022
------------------------------------------------------------------------
                         run_w2v.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_w2vlid.py
Started: 12/10/2022 13:06:18

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-w2vlid
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_500f_devdata
train_filename: dev_u_500f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 12
gradient_accumulation_steps: 2
learning_rate: 0.0001
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/dev_u_500f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_500f_devdata_local/wav2vec-ADI17-w2vlid
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_500f_devdata_local/wav2vec-ADI17-w2vlid_finetuned_results.csv
--> pretrained_mod: log0/wav2vec2-base-lang-id

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 5 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.0213,  0.0199,  0.0182,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0642, -0.0667, -0.0716,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0086,  0.0000, -0.0038,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.2654,  0.2115,  0.1515,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0143, -0.0133, -0.0134,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0746, -0.0557, -0.0458,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 2, 2, 2, 0, 3, 2, 1, 3, 3, 1])}
Training DataCustom Files: 1963
Training Data Files: 164
Test Data Sample
{'input_values': tensor([[ 0.0791,  0.0901,  0.0983,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2048, -0.1595, -0.1084,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0263, -0.0270, -0.0301,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0110,  0.0104,  0.0099,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1726,  0.1834,  0.1984,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0183,  0.0173,  0.0174,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 3, 0])}
Test CustomData Files: 398
Test Data Files: 34
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
Traceback (most recent call last):
  File "run_w2vlid.py", line 468, in <module>
    for param in model.wav2vec2.feature_extractor.parameters():
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'HubertForSequenceClassification' object has no attribute 'wav2vec2'
Wed Oct 12 13:23:12 AEDT 2022
------------------------------------------------------------------------
                         run_w2v.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_w2vlid.py
Started: 12/10/2022 13:23:17

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-w2vlid
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_500f_devdata
train_filename: dev_u_500f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 12
gradient_accumulation_steps: 2
learning_rate: 0.0001
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/dev_u_500f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_500f_devdata_local/wav2vec-ADI17-w2vlid
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_500f_devdata_local/wav2vec-ADI17-w2vlid_finetuned_results.csv
--> pretrained_mod: log0/wav2vec2-base-lang-id

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 5 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.0000e+00,  1.5259e-04,  4.2725e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0040e-02,  7.3853e-03,  9.2773e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.5634e-02,  5.6580e-02,  4.9866e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 8.1268e-02,  7.0068e-02,  6.0242e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9699e-01, -2.1964e-01, -2.2647e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6926e-03,  4.3640e-03,  2.1973e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 3, 1, 1, 0, 1, 0, 3, 3, 2, 0])}
Training DataCustom Files: 1963
Training Data Files: 164
Test Data Sample
{'input_values': tensor([[ 0.0148,  0.0197,  0.0132,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0655,  0.0826,  0.1005,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0110,  0.0104,  0.0099,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0059,  0.0106,  0.0145,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0068, -0.0081, -0.0100,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2609, -0.2853, -0.3143,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 0, 3, 0, 0, 1, 0, 0, 2, 2, 1])}
Test CustomData Files: 398
Test Data Files: 34
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  4 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 198660
Epoch 0 Train Acc 26.103656768798828% Val Acc 19.882352828979492% Train Loss 0.6902620792388916 Val Loss 1.41440749168396
Trainable Parameters : 198660
Epoch 1 Train Acc 28.975608825683594% Val Acc 19.617647171020508% Train Loss 0.6879306435585022 Val Loss 1.4110229015350342
Trainable Parameters : 198660
Epoch 2 Train Acc 31.31097412109375% Val Acc 19.705883026123047% Train Loss 0.6842342615127563 Val Loss 1.4135956764221191
Trainable Parameters : 198660
Epoch 3 Train Acc 34.32316970825195% Val Acc 24.05882453918457% Train Loss 0.679792582988739 Val Loss 1.4123725891113281
Trainable Parameters : 198660
Epoch 4 Train Acc 35.530487060546875% Val Acc 21.794116973876953% Train Loss 0.6741059422492981 Val Loss 1.4251384735107422
Trainable Parameters : 198660
Epoch 5 Train Acc 37.689022064208984% Val Acc 26.205883026123047% Train Loss 0.6681141257286072 Val Loss 1.4399832487106323
Trainable Parameters : 198660
Epoch 6 Train Acc 40.939022064208984% Val Acc 24.02941131591797% Train Loss 0.6604041457176208 Val Loss 1.426928162574768
Trainable Parameters : 198660
Epoch 7 Train Acc 40.96950912475586% Val Acc 27.823530197143555% Train Loss 0.6531890034675598 Val Loss 1.4336949586868286
Trainable Parameters : 198660
Epoch 8 Train Acc 42.225608825683594% Val Acc 27.176469802856445% Train Loss 0.6451115608215332 Val Loss 1.420206069946289
Trainable Parameters : 198660
Epoch 9 Train Acc 44.3719482421875% Val Acc 25.52941131591797% Train Loss 0.6358951926231384 Val Loss 1.4522089958190918
Trainable Parameters : 198660
Epoch 10 Train Acc 46.1097526550293% Val Acc 26.05882453918457% Train Loss 0.6269308924674988 Val Loss 1.445499062538147
Trainable Parameters : 198660
Epoch 11 Train Acc 46.52438735961914% Val Acc 30.176469802856445% Train Loss 0.6164723634719849 Val Loss 1.4660392999649048
Trainable Parameters : 198660
Epoch 12 Train Acc 49.02438735961914% Val Acc 26.47058868408203% Train Loss 0.6078134775161743 Val Loss 1.4422900676727295
Trainable Parameters : 198660
Epoch 13 Train Acc 49.835365295410156% Val Acc 32.55882263183594% Train Loss 0.596936047077179 Val Loss 1.470699429512024
Trainable Parameters : 198660
Epoch 14 Train Acc 52.46950912475586% Val Acc 25.47058868408203% Train Loss 0.5860484838485718 Val Loss 1.4771369695663452
Trainable Parameters : 198660
Epoch 15 Train Acc 52.896339416503906% Val Acc 28.352941513061523% Train Loss 0.5739303827285767 Val Loss 1.4452149868011475
Trainable Parameters : 198660
Epoch 16 Train Acc 54.975608825683594% Val Acc 35.0% Train Loss 0.5631163120269775 Val Loss 1.5007609128952026
Trainable Parameters : 198660
Epoch 17 Train Acc 58.06707000732422% Val Acc 32.64706039428711% Train Loss 0.5502413511276245 Val Loss 1.4733283519744873
Trainable Parameters : 198660
Epoch 18 Train Acc 57.749996185302734% Val Acc 31.117647171020508% Train Loss 0.5389372706413269 Val Loss 1.4632470607757568
Trainable Parameters : 198660
Epoch 19 Train Acc 59.8841438293457% Val Acc 31.852941513061523% Train Loss 0.5251258611679077 Val Loss 1.5155029296875
Trainable Parameters : 198660
Epoch 20 Train Acc 60.44511795043945% Val Acc 35.52941131591797% Train Loss 0.5136579275131226 Val Loss 1.501571536064148
Trainable Parameters : 198660
Epoch 21 Train Acc 61.81097412109375% Val Acc 35.588233947753906% Train Loss 0.4982825815677643 Val Loss 1.5079383850097656
Trainable Parameters : 198660
Epoch 22 Train Acc 63.286582946777344% Val Acc 33.588233947753906% Train Loss 0.48363572359085083 Val Loss 1.6781255006790161
Trainable Parameters : 198660
Epoch 23 Train Acc 65.20121765136719% Val Acc 37.70588302612305% Train Loss 0.47256365418434143 Val Loss 1.5294599533081055
Trainable Parameters : 198660
Epoch 24 Train Acc 65.25609588623047% Val Acc 36.5% Train Loss 0.4602258503437042 Val Loss 1.5560730695724487
Trainable Parameters : 198660
Epoch 25 Train Acc 67.05487823486328% Val Acc 34.088233947753906% Train Loss 0.44558873772621155 Val Loss 1.5123642683029175
Trainable Parameters : 198660
Epoch 26 Train Acc 66.87804412841797% Val Acc 39.882354736328125% Train Loss 0.4365961253643036 Val Loss 1.6826996803283691
Trainable Parameters : 198660
Epoch 27 Train Acc 68.41463470458984% Val Acc 37.02941131591797% Train Loss 0.42451104521751404 Val Loss 1.6403610706329346
Trainable Parameters : 198660
Epoch 28 Train Acc 69.23170471191406% Val Acc 38.47058868408203% Train Loss 0.4131091237068176 Val Loss 1.6459577083587646
Trainable Parameters : 198660
Epoch 29 Train Acc 69.18901824951172% Val Acc 39.94117736816406% Train Loss 0.4063904881477356 Val Loss 1.6873323917388916
Trainable Parameters : 198660
Epoch 30 Train Acc 70.03048706054688% Val Acc 38.29411697387695% Train Loss 0.396374374628067 Val Loss 1.702885389328003
Trainable Parameters : 198660
Epoch 31 Train Acc 69.59146118164062% Val Acc 32.11764907836914% Train Loss 0.3890043795108795 Val Loss 1.7446902990341187
Trainable Parameters : 198660
Epoch 32 Train Acc 70.9756088256836% Val Acc 38.52941131591797% Train Loss 0.38080617785453796 Val Loss 1.8227739334106445
Trainable Parameters : 198660
Epoch 33 Train Acc 72.46951293945312% Val Acc 39.235294342041016% Train Loss 0.3724316358566284 Val Loss 1.8547704219818115
Trainable Parameters : 198660
Epoch 34 Train Acc 71.70121765136719% Val Acc 39.14706039428711% Train Loss 0.36434710025787354 Val Loss 1.8210631608963013
Trainable Parameters : 198660
Epoch 35 Train Acc 72.16463470458984% Val Acc 39.235294342041016% Train Loss 0.3570711612701416 Val Loss 1.7021896839141846
Trainable Parameters : 198660
Epoch 36 Train Acc 73.76219177246094% Val Acc 37.05882263183594% Train Loss 0.352958619594574 Val Loss 2.1024415493011475
Trainable Parameters : 198660
Epoch 37 Train Acc 73.32926177978516% Val Acc 41.94117736816406% Train Loss 0.35062897205352783 Val Loss 1.796749234199524
Trainable Parameters : 198660
Epoch 38 Train Acc 74.92073059082031% Val Acc 41.70588302612305% Train Loss 0.3392708897590637 Val Loss 1.836034893989563
Trainable Parameters : 198660
Epoch 39 Train Acc 73.90853118896484% Val Acc 35.29411697387695% Train Loss 0.33446934819221497 Val Loss 2.2372496128082275
Trainable Parameters : 198660
Epoch 40 Train Acc 74.69512176513672% Val Acc 39.82352828979492% Train Loss 0.3334873616695404 Val Loss 1.98609459400177
Trainable Parameters : 198660
Epoch 41 Train Acc 74.43901824951172% Val Acc 38.235294342041016% Train Loss 0.3268790543079376 Val Loss 1.942789912223816
Trainable Parameters : 198660
Epoch 42 Train Acc 75.14024353027344% Val Acc 39.47058868408203% Train Loss 0.327160120010376 Val Loss 1.923485517501831
Trainable Parameters : 198660
Epoch 43 Train Acc 76.10365295410156% Val Acc 39.94117736816406% Train Loss 0.3180408477783203 Val Loss 2.098264694213867
Trainable Parameters : 198660
Epoch 44 Train Acc 75.17682647705078% Val Acc 35.5% Train Loss 0.3159846067428589 Val Loss 2.4544315338134766
Trainable Parameters : 198660
Epoch 45 Train Acc 76.29877471923828% Val Acc 42.44117736816406% Train Loss 0.31679171323776245 Val Loss 1.9264391660690308
Trainable Parameters : 198660
Epoch 46 Train Acc 75.79267883300781% Val Acc 39.44117736816406% Train Loss 0.31503716111183167 Val Loss 2.141829252243042
Trainable Parameters : 198660
Epoch 47 Train Acc 77.48170471191406% Val Acc 38.735294342041016% Train Loss 0.30695778131484985 Val Loss 1.9865673780441284
Trainable Parameters : 198660
Epoch 48 Train Acc 77.15853118896484% Val Acc 37.088233947753906% Train Loss 0.3070427179336548 Val Loss 2.319103956222534
Trainable Parameters : 198660
Epoch 49 Train Acc 76.73170471191406% Val Acc 41.411766052246094% Train Loss 0.305555135011673 Val Loss 2.2059080600738525
Trainable Parameters : 198660
Epoch 50 Train Acc 76.29267883300781% Val Acc 37.47058868408203% Train Loss 0.30919113755226135 Val Loss 2.3663458824157715
Trainable Parameters : 198660
Epoch 51 Train Acc 76.65243530273438% Val Acc 39.55882263183594% Train Loss 0.3021240234375 Val Loss 2.2229809761047363
Trainable Parameters : 198660
Epoch 52 Train Acc 77.0243911743164% Val Acc 36.735294342041016% Train Loss 0.3014920651912689 Val Loss 2.4420955181121826
Trainable Parameters : 198660
Epoch 53 Train Acc 77.36585235595703% Val Acc 42.61764907836914% Train Loss 0.2994837462902069 Val Loss 2.2112691402435303
Trainable Parameters : 198660
Epoch 54 Train Acc 76.95121765136719% Val Acc 37.52941131591797% Train Loss 0.29491159319877625 Val Loss 2.278988838195801
Trainable Parameters : 198660
Epoch 55 Train Acc 77.56097412109375% Val Acc 36.97058868408203% Train Loss 0.30187368392944336 Val Loss 2.1488583087921143
Trainable Parameters : 198660
Epoch 56 Train Acc 77.23170471191406% Val Acc 40.20588302612305% Train Loss 0.28867238759994507 Val Loss 2.0254318714141846
Trainable Parameters : 198660
Epoch 57 Train Acc 78.70121765136719% Val Acc 36.47058868408203% Train Loss 0.2895206809043884 Val Loss 2.4088354110717773
Trainable Parameters : 198660
Epoch 58 Train Acc 76.57316589355469% Val Acc 39.14706039428711% Train Loss 0.2969280779361725 Val Loss 2.150362730026245
Trainable Parameters : 198660
Epoch 59 Train Acc 77.01219177246094% Val Acc 39.02941131591797% Train Loss 0.2935299277305603 Val Loss 2.3718528747558594
Trainable Parameters : 198660
Epoch 60 Train Acc 76.93901824951172% Val Acc 38.911766052246094% Train Loss 0.3052382171154022 Val Loss 2.603091239929199
Trainable Parameters : 198660
Epoch 61 Train Acc 79.01219177246094% Val Acc 36.32352828979492% Train Loss 0.28704971075057983 Val Loss 2.4391140937805176
Trainable Parameters : 198660
Epoch 62 Train Acc 78.66463470458984% Val Acc 37.52941131591797% Train Loss 0.2768515348434448 Val Loss 2.4299871921539307
Trainable Parameters : 198660
Epoch 63 Train Acc 78.65243530273438% Val Acc 36.735294342041016% Train Loss 0.2802997827529907 Val Loss 2.9086754322052
Trainable Parameters : 198660
Epoch 64 Train Acc 77.07926177978516% Val Acc 33.588233947753906% Train Loss 0.2985472083091736 Val Loss 2.9442508220672607
Trainable Parameters : 198660
Epoch 65 Train Acc 79.67073059082031% Val Acc 40.52941131591797% Train Loss 0.27544909715652466 Val Loss 2.4133834838867188
Trainable Parameters : 198660
Epoch 66 Train Acc 77.77438354492188% Val Acc 40.94117736816406% Train Loss 0.29160594940185547 Val Loss 2.5865793228149414
Trainable Parameters : 198660
Epoch 67 Train Acc 79.15243530273438% Val Acc 40.64706039428711% Train Loss 0.2733903229236603 Val Loss 2.5412580966949463
Trainable Parameters : 198660
Epoch 68 Train Acc 79.68292236328125% Val Acc 37.235294342041016% Train Loss 0.2814112603664398 Val Loss 2.8438925743103027
Trainable Parameters : 198660
Epoch 69 Train Acc 80.94512176513672% Val Acc 40.11764907836914% Train Loss 0.26025328040122986 Val Loss 2.4679372310638428
Trainable Parameters : 198660
Epoch 70 Train Acc 79.7682876586914% Val Acc 42.70588302612305% Train Loss 0.27265697717666626 Val Loss 2.151682138442993
Trainable Parameters : 198660
Epoch 71 Train Acc 79.493896484375% Val Acc 39.411766052246094% Train Loss 0.27279844880104065 Val Loss 2.322542190551758
Trainable Parameters : 198660
Epoch 72 Train Acc 79.1219482421875% Val Acc 41.17647171020508% Train Loss 0.2787136435508728 Val Loss 2.1948776245117188
Trainable Parameters : 198660
Epoch 73 Train Acc 78.9756088256836% Val Acc 39.02941131591797% Train Loss 0.27264589071273804 Val Loss 2.244004964828491
Trainable Parameters : 198660
Epoch 74 Train Acc 80.21951293945312% Val Acc 39.20588302612305% Train Loss 0.2646985650062561 Val Loss 2.429375648498535
Trainable Parameters : 198660
Epoch 75 Train Acc 81.21951293945312% Val Acc 41.088233947753906% Train Loss 0.2618328034877777 Val Loss 2.5018224716186523
Trainable Parameters : 198660
Epoch 76 Train Acc 80.57316589355469% Val Acc 37.735294342041016% Train Loss 0.2555428743362427 Val Loss 2.217539072036743
Trainable Parameters : 198660
Epoch 77 Train Acc 81.32926177978516% Val Acc 39.882354736328125% Train Loss 0.25359487533569336 Val Loss 2.3633484840393066
Trainable Parameters : 198660
Epoch 78 Train Acc 79.84146118164062% Val Acc 42.382354736328125% Train Loss 0.2650681734085083 Val Loss 2.413543224334717
Trainable Parameters : 198660
Epoch 79 Train Acc 80.42682647705078% Val Acc 41.735294342041016% Train Loss 0.2580777108669281 Val Loss 2.5286505222320557
Trainable Parameters : 198660
Epoch 80 Train Acc 81.45121765136719% Val Acc 40.14706039428711% Train Loss 0.24736742675304413 Val Loss 2.2808022499084473
Trainable Parameters : 198660
Epoch 81 Train Acc 81.37804412841797% Val Acc 40.735294342041016% Train Loss 0.2539149820804596 Val Loss 2.3523457050323486
Trainable Parameters : 198660
Epoch 82 Train Acc 82.38414001464844% Val Acc 43.55882263183594% Train Loss 0.2514724135398865 Val Loss 2.2161524295806885
Trainable Parameters : 198660
Epoch 83 Train Acc 81.73780059814453% Val Acc 39.764705657958984% Train Loss 0.24547670781612396 Val Loss 2.4003567695617676
Trainable Parameters : 198660
Epoch 84 Train Acc 79.71340942382812% Val Acc 35.264705657958984% Train Loss 0.25176382064819336 Val Loss 2.92675518989563
Trainable Parameters : 198660
Epoch 85 Train Acc 82.19512176513672% Val Acc 39.70588302612305% Train Loss 0.23521727323532104 Val Loss 2.508793354034424
Trainable Parameters : 198660
Epoch 86 Train Acc 81.7682876586914% Val Acc 39.911766052246094% Train Loss 0.24615447223186493 Val Loss 2.212386131286621
Trainable Parameters : 198660
Epoch 87 Train Acc 80.78048706054688% Val Acc 42.411766052246094% Train Loss 0.24788053333759308 Val Loss 2.283247232437134
Trainable Parameters : 198660
Epoch 88 Train Acc 81.64024353027344% Val Acc 43.14706039428711% Train Loss 0.2392112761735916 Val Loss 2.5275328159332275
Trainable Parameters : 198660
Epoch 89 Train Acc 81.57926177978516% Val Acc 42.29411697387695% Train Loss 0.23871374130249023 Val Loss 2.405252695083618
Trainable Parameters : 198660
Epoch 90 Train Acc 82.1463394165039% Val Acc 38.47058868408203% Train Loss 0.24173174798488617 Val Loss 2.6986889839172363
Trainable Parameters : 198660
Epoch 91 Train Acc 81.57926177978516% Val Acc 42.94117736816406% Train Loss 0.2480141669511795 Val Loss 2.3098747730255127
Trainable Parameters : 198660
Epoch 92 Train Acc 82.31097412109375% Val Acc 41.764705657958984% Train Loss 0.2375042736530304 Val Loss 2.5204460620880127
Trainable Parameters : 198660
Epoch 93 Train Acc 82.58536529541016% Val Acc 36.97058868408203% Train Loss 0.23540842533111572 Val Loss 2.837473154067993
Trainable Parameters : 198660
Epoch 94 Train Acc 81.79877471923828% Val Acc 40.61764907836914% Train Loss 0.23916487395763397 Val Loss 2.3210232257843018
Trainable Parameters : 198660
Epoch 95 Train Acc 82.73780059814453% Val Acc 38.47058868408203% Train Loss 0.2297379970550537 Val Loss 2.4347522258758545
Trainable Parameters : 198660
Epoch 96 Train Acc 82.4695053100586% Val Acc 42.382354736328125% Train Loss 0.2401176393032074 Val Loss 2.4968457221984863
Trainable Parameters : 198660
Epoch 97 Train Acc 82.28658294677734% Val Acc 41.911766052246094% Train Loss 0.23350390791893005 Val Loss 2.504171133041382
Trainable Parameters : 198660
Epoch 98 Train Acc 82.4756088256836% Val Acc 38.235294342041016% Train Loss 0.23053701221942902 Val Loss 2.7045280933380127
Trainable Parameters : 198660
Configuration saved in ../output/umbrella_500f_devdata_local/wav2vec-ADI17-w2vlid/config.json
Model weights saved in ../output/umbrella_500f_devdata_local/wav2vec-ADI17-w2vlid/pytorch_model.bin
Epoch 99 Train Acc 82.5% Val Acc 40.61764907836914% Train Loss 0.24176064133644104 Val Loss 2.676893472671509

------> EVALUATING MODEL... ------------------------------------------ 

/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
CONFUSION MATRIX
[[0.25125628 0.         0.         0.        ]
 [0.25125628 0.         0.         0.        ]
 [0.24623116 0.         0.         0.        ]
 [0.25125628 0.         0.         0.        ]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.25      1.00      0.40       100
           1       0.00      0.00      0.00       100
           2       0.00      0.00      0.00        98
           3       0.00      0.00      0.00       100

    accuracy                           0.25       398
   macro avg       0.06      0.25      0.10       398
weighted avg       0.06      0.25      0.10       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 12/10/2022 18:31:53
