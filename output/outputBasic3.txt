Tue Sep 27 15:22:55 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.bias', 'project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 27/09/2022 15:23:01

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: adi17_test_umbrella_label
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/adi17_test_umbrella_label.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0562, -0.0742, -0.1016,  ...,  0.4345,  0.3095,  0.1099]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1])}
Test Data Sample
{'input_values': tensor([[-0.7819, -0.7655, -0.6722,  ..., -0.1168, -0.1613, -0.3895]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0])}
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Epoch 0 Train Acc 0.0 Val Acc 0.0 Train Loss 0.7087967991828918 Val Loss 1.5260415077209473
EPOCH unfeeze : 1
Epoch 1 Train Acc 0.0 Val Acc 0.0 Train Loss 0.7108123302459717 Val Loss 1.526991605758667
EPOCH unfeeze : 2
Epoch 2 Train Acc 0.0 Val Acc 0.0 Train Loss 0.6996273994445801 Val Loss 1.528416395187378
EPOCH unfeeze : 3
Epoch 3 Train Acc 0.0 Val Acc 0.0 Train Loss 0.6928229331970215 Val Loss 1.5311505794525146
EPOCH unfeeze : 4
Epoch 4 Train Acc 0.0 Val Acc 0.0 Train Loss 0.6963802576065063 Val Loss 1.5337073802947998
EPOCH unfeeze : 5
Epoch 5 Train Acc 0.0 Val Acc 0.0 Train Loss 0.6878999471664429 Val Loss 1.5367457866668701
EPOCH unfeeze : 6
Epoch 6 Train Acc 25.0 Val Acc 0.0 Train Loss 0.6769359707832336 Val Loss 1.5403082370758057
EPOCH unfeeze : 7
Epoch 7 Train Acc 75.0 Val Acc 0.0 Train Loss 0.6649466753005981 Val Loss 1.5456724166870117
EPOCH unfeeze : 8
Epoch 8 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6548283100128174 Val Loss 1.5514922142028809
EPOCH unfeeze : 9
Epoch 9 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6498414874076843 Val Loss 1.5596790313720703
EPOCH unfeeze : 0
Updated Parameters at Epoch 10 Trainable Parameters : 85648900
Epoch 10 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6449804306030273 Val Loss 1.567958116531372
EPOCH unfeeze : 1
Epoch 11 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6235455274581909 Val Loss 1.5775164365768433
EPOCH unfeeze : 2
Epoch 12 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6060752868652344 Val Loss 1.5888575315475464
EPOCH unfeeze : 3
Epoch 13 Train Acc 100.0 Val Acc 0.0 Train Loss 0.58696448802948 Val Loss 1.5992780923843384
EPOCH unfeeze : 4
Epoch 14 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5708843469619751 Val Loss 1.6115949153900146
EPOCH unfeeze : 5
Epoch 15 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5450071692466736 Val Loss 1.625081181526184
EPOCH unfeeze : 6
Epoch 16 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5202533006668091 Val Loss 1.6420165300369263
EPOCH unfeeze : 7
Epoch 17 Train Acc 100.0 Val Acc 0.0 Train Loss 0.49743106961250305 Val Loss 1.6603918075561523
EPOCH unfeeze : 8
Epoch 18 Train Acc 100.0 Val Acc 0.0 Train Loss 0.47935834527015686 Val Loss 1.6776986122131348
EPOCH unfeeze : 9
Epoch 19 Train Acc 100.0 Val Acc 0.0 Train Loss 0.44610124826431274 Val Loss 1.7073395252227783
EPOCH unfeeze : 0
Updated Parameters at Epoch 20 Trainable Parameters : 85648900
Epoch 20 Train Acc 100.0 Val Acc 0.0 Train Loss 0.41571033000946045 Val Loss 1.7340821027755737
EPOCH unfeeze : 1
Epoch 21 Train Acc 100.0 Val Acc 0.0 Train Loss 0.38279253244400024 Val Loss 1.7613571882247925
EPOCH unfeeze : 2
Epoch 22 Train Acc 100.0 Val Acc 0.0 Train Loss 0.3484609127044678 Val Loss 1.810295820236206
EPOCH unfeeze : 3
Epoch 23 Train Acc 100.0 Val Acc 0.0 Train Loss 0.3154881000518799 Val Loss 1.845731496810913
EPOCH unfeeze : 4
Epoch 24 Train Acc 100.0 Val Acc 0.0 Train Loss 0.2941872477531433 Val Loss 1.9107773303985596
EPOCH unfeeze : 5
Epoch 25 Train Acc 100.0 Val Acc 0.0 Train Loss 0.2592778205871582 Val Loss 1.980428695678711
EPOCH unfeeze : 6
Epoch 26 Train Acc 100.0 Val Acc 0.0 Train Loss 0.22645652294158936 Val Loss 2.0602314472198486
EPOCH unfeeze : 7
Epoch 27 Train Acc 100.0 Val Acc 0.0 Train Loss 0.2037983387708664 Val Loss 2.1197800636291504
EPOCH unfeeze : 8
Epoch 28 Train Acc 100.0 Val Acc 0.0 Train Loss 0.18989138305187225 Val Loss 2.216841459274292
EPOCH unfeeze : 9
Epoch 29 Train Acc 100.0 Val Acc 0.0 Train Loss 0.1711321920156479 Val Loss 2.2920050621032715
EPOCH unfeeze : 0
Updated Parameters at Epoch 30 Trainable Parameters : 85648900
Epoch 30 Train Acc 100.0 Val Acc 0.0 Train Loss 0.15214785933494568 Val Loss 2.4017605781555176
EPOCH unfeeze : 1
Epoch 31 Train Acc 100.0 Val Acc 0.0 Train Loss 0.13571244478225708 Val Loss 2.4591546058654785
EPOCH unfeeze : 2
Epoch 32 Train Acc 100.0 Val Acc 0.0 Train Loss 0.12596653401851654 Val Loss 2.54740047454834
EPOCH unfeeze : 3
Epoch 33 Train Acc 100.0 Val Acc 0.0 Train Loss 0.11467276513576508 Val Loss 2.644365072250366
EPOCH unfeeze : 4
Epoch 34 Train Acc 100.0 Val Acc 0.0 Train Loss 0.10621195286512375 Val Loss 2.7146060466766357
EPOCH unfeeze : 5
Epoch 35 Train Acc 100.0 Val Acc 0.0 Train Loss 0.09903925657272339 Val Loss 2.7855517864227295Configuration saved in ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

EPOCH unfeeze : 6
Epoch 36 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0912611186504364 Val Loss 2.8495447635650635
EPOCH unfeeze : 7
Epoch 37 Train Acc 100.0 Val Acc 0.0 Train Loss 0.08562135696411133 Val Loss 2.9284403324127197
EPOCH unfeeze : 8
Epoch 38 Train Acc 100.0 Val Acc 0.0 Train Loss 0.08061045408248901 Val Loss 2.9599716663360596
EPOCH unfeeze : 9
Epoch 39 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0765649750828743 Val Loss 3.0291926860809326
EPOCH unfeeze : 0
Updated Parameters at Epoch 40 Trainable Parameters : 85648900
Epoch 40 Train Acc 100.0 Val Acc 0.0 Train Loss 0.07182343304157257 Val Loss 3.085151195526123
EPOCH unfeeze : 1
Epoch 41 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06866040825843811 Val Loss 3.131984233856201
EPOCH unfeeze : 2
Epoch 42 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06486959755420685 Val Loss 3.177067995071411
EPOCH unfeeze : 3
Epoch 43 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06337349116802216 Val Loss 3.250088691711426
EPOCH unfeeze : 4
Epoch 44 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06026591360569 Val Loss 3.2889723777770996
EPOCH unfeeze : 5
Epoch 45 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05747305974364281 Val Loss 3.327399969100952
EPOCH unfeeze : 6
Epoch 46 Train Acc 100.0 Val Acc 0.0 Train Loss 0.054976899176836014 Val Loss 3.366220474243164
EPOCH unfeeze : 7
Epoch 47 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05267704278230667 Val Loss 3.410440683364868
EPOCH unfeeze : 8
Epoch 48 Train Acc 100.0 Val Acc 0.0 Train Loss 0.051002487540245056 Val Loss 3.455428123474121
EPOCH unfeeze : 9
Epoch 49 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04920780658721924 Val Loss 3.466325521469116
EPOCH unfeeze : 0
Updated Parameters at Epoch 50 Trainable Parameters : 85648900
Epoch 50 Train Acc 100.0 Val Acc 0.0 Train Loss 0.047749247401952744 Val Loss 3.535318374633789
EPOCH unfeeze : 1
Epoch 51 Train Acc 100.0 Val Acc 0.0 Train Loss 0.045891404151916504 Val Loss 3.5783205032348633
EPOCH unfeeze : 2
Epoch 52 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0442257784307003 Val Loss 3.622802734375
EPOCH unfeeze : 3
Epoch 53 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04277888685464859 Val Loss 3.643927574157715
EPOCH unfeeze : 4
Epoch 54 Train Acc 100.0 Val Acc 0.0 Train Loss 0.041036952286958694 Val Loss 3.6846797466278076
EPOCH unfeeze : 5
Epoch 55 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03976356238126755 Val Loss 3.7226312160491943
EPOCH unfeeze : 6
Epoch 56 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03827755153179169 Val Loss 3.7611470222473145
EPOCH unfeeze : 7
Epoch 57 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03694625198841095 Val Loss 3.7945666313171387
EPOCH unfeeze : 8
Epoch 58 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0355549156665802 Val Loss 3.8307671546936035
EPOCH unfeeze : 9
Epoch 59 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03459226340055466 Val Loss 3.8640952110290527
EPOCH unfeeze : 0
Updated Parameters at Epoch 60 Trainable Parameters : 85648900
Epoch 60 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03340781480073929 Val Loss 3.8975977897644043
EPOCH unfeeze : 1
Epoch 61 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03229064866900444 Val Loss 3.9408037662506104
EPOCH unfeeze : 2
Epoch 62 Train Acc 100.0 Val Acc 0.0 Train Loss 0.031116368249058723 Val Loss 3.9695348739624023
EPOCH unfeeze : 3
Epoch 63 Train Acc 100.0 Val Acc 0.0 Train Loss 0.030355753377079964 Val Loss 4.0110673904418945
EPOCH unfeeze : 4
Epoch 64 Train Acc 100.0 Val Acc 0.0 Train Loss 0.029118208214640617 Val Loss 4.0475616455078125
EPOCH unfeeze : 5
Epoch 65 Train Acc 100.0 Val Acc 0.0 Train Loss 0.028195075690746307 Val Loss 4.08488655090332
EPOCH unfeeze : 6
Epoch 66 Train Acc 100.0 Val Acc 0.0 Train Loss 0.027379585430026054 Val Loss 4.114682197570801
EPOCH unfeeze : 7
Epoch 67 Train Acc 100.0 Val Acc 0.0 Train Loss 0.026383262127637863 Val Loss 4.140738487243652
EPOCH unfeeze : 8
Epoch 68 Train Acc 100.0 Val Acc 0.0 Train Loss 0.025515960529446602 Val Loss 4.187495231628418
EPOCH unfeeze : 9
Epoch 69 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02461308427155018 Val Loss 4.225523948669434
EPOCH unfeeze : 0
Updated Parameters at Epoch 70 Trainable Parameters : 85648900
Epoch 70 Train Acc 100.0 Val Acc 0.0 Train Loss 0.023891020566225052 Val Loss 4.255650043487549
EPOCH unfeeze : 1
Epoch 71 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02296169102191925 Val Loss 4.287836074829102
EPOCH unfeeze : 2
Epoch 72 Train Acc 100.0 Val Acc 0.0 Train Loss 0.022290119901299477 Val Loss 4.320640563964844
EPOCH unfeeze : 3
Epoch 73 Train Acc 100.0 Val Acc 0.0 Train Loss 0.021445881575345993 Val Loss 4.355945110321045
EPOCH unfeeze : 4
Epoch 74 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02076605148613453 Val Loss 4.392763137817383
EPOCH unfeeze : 5
Epoch 75 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02009618654847145 Val Loss 4.424246788024902
EPOCH unfeeze : 6
Epoch 76 Train Acc 100.0 Val Acc 0.0 Train Loss 0.019420746713876724 Val Loss 4.455912113189697
EPOCH unfeeze : 7
Epoch 77 Train Acc 100.0 Val Acc 0.0 Train Loss 0.018783677369356155 Val Loss 4.49210262298584
EPOCH unfeeze : 8
Epoch 78 Train Acc 100.0 Val Acc 0.0 Train Loss 0.018121084198355675 Val Loss 4.52272891998291
EPOCH unfeeze : 9
Epoch 79 Train Acc 100.0 Val Acc 0.0 Train Loss 0.017497561872005463 Val Loss 4.559128284454346
EPOCH unfeeze : 0
Updated Parameters at Epoch 80 Trainable Parameters : 85648900
Epoch 80 Train Acc 100.0 Val Acc 0.0 Train Loss 0.017004648223519325 Val Loss 4.589871406555176
EPOCH unfeeze : 1
Epoch 81 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01644042879343033 Val Loss 4.625794410705566
EPOCH unfeeze : 2
Epoch 82 Train Acc 100.0 Val Acc 0.0 Train Loss 0.015866903588175774 Val Loss 4.663669586181641
EPOCH unfeeze : 3
Epoch 83 Train Acc 100.0 Val Acc 0.0 Train Loss 0.015345440246164799 Val Loss 4.70015287399292
EPOCH unfeeze : 4
Epoch 84 Train Acc 100.0 Val Acc 0.0 Train Loss 0.014880696311593056 Val Loss 4.733863353729248
EPOCH unfeeze : 5
Epoch 85 Train Acc 100.0 Val Acc 0.0 Train Loss 0.014337108470499516 Val Loss 4.764553070068359
EPOCH unfeeze : 6
Epoch 86 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01390763744711876 Val Loss 4.79881477355957
EPOCH unfeeze : 7
Epoch 87 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01339037623256445 Val Loss 4.830770492553711
EPOCH unfeeze : 8
Epoch 88 Train Acc 100.0 Val Acc 0.0 Train Loss 0.012974007055163383 Val Loss 4.860138893127441
EPOCH unfeeze : 9
Epoch 89 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01252572052180767 Val Loss 4.895771503448486
EPOCH unfeeze : 0
Updated Parameters at Epoch 90 Trainable Parameters : 85648900
Epoch 90 Train Acc 100.0 Val Acc 0.0 Train Loss 0.012112200260162354 Val Loss 4.925513744354248
EPOCH unfeeze : 1
Epoch 91 Train Acc 100.0 Val Acc 0.0 Train Loss 0.011690251529216766 Val Loss 4.9643144607543945
EPOCH unfeeze : 2
Epoch 92 Train Acc 100.0 Val Acc 0.0 Train Loss 0.011298210360109806 Val Loss 4.999320983886719
EPOCH unfeeze : 3
Epoch 93 Train Acc 100.0 Val Acc 0.0 Train Loss 0.010981508530676365 Val Loss 5.030466079711914
EPOCH unfeeze : 4
Epoch 94 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01063395757228136 Val Loss 5.0611371994018555
EPOCH unfeeze : 5
Epoch 95 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01026891265064478 Val Loss 5.09915828704834
EPOCH unfeeze : 6
Epoch 96 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009946011006832123 Val Loss 5.132020950317383
EPOCH unfeeze : 7
Epoch 97 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009621426463127136 Val Loss 5.166330337524414
EPOCH unfeeze : 8
Epoch 98 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009285018779337406 Val Loss 5.195380210876465
EPOCH unfeeze : 9
Epoch 99 Train Acc 100.0 Val Acc 0.0 Train Loss 0.00901694968342781 Val Loss 5.222615718841553

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       4.0
           1       0.00      0.00      0.00       0.0

    accuracy                           0.00       4.0
   macro avg       0.00      0.00      0.00       4.0
weighted avg       0.00      0.00      0.00       4.0


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 27/09/2022 15:24:10
100%|██████████| 4/4 [00:00<00:00, 22.10it/s]Tue Sep 27 15:46:39 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.bias', 'quantizer.codevectors', 'project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_q.weight', 'quantizer.weight_proj.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 27/09/2022 15:46:43

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: adi17_test_umbrella_label
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/adi17_test_umbrella_label.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[0.1438, 0.1425, 0.1435,  ..., 0.0000, 0.0000, 0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1])}
Test Data Sample
{'input_values': tensor([[-0.0621, -0.0449, -0.0356,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0])}
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Traceback (most recent call last):
  File "run_basic.py", line 963, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 786, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_basic.py", line 807, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_basic.py", line 853, in _compute_loss
    acc = multi_acc(prediction, labels.reshape(
  File "run_basic.py", line 754, in multi_acc
    acc = torch.round(acc * 100, 4)
TypeError: round() received an invalid combination of arguments - got (Tensor, int), but expected one of:
 * (Tensor input, *, Tensor out)
 * (Tensor input, *, int decimals, Tensor out)

Tue Sep 27 17:46:43 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_hid.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 27/09/2022 17:46:47

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: adi17_test_umbrella_label
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/adi17_test_umbrella_label.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.5558, -0.5387, -0.5220,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1])}
Test Data Sample
{'input_values': tensor([[-0.6328, -0.1130,  0.3310,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0])}
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Traceback (most recent call last):
  File "run_basic.py", line 963, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 791, in fit
    print(f"Epoch {epoch} Train Acc {train_acc} Val Acc {val_acc} Train Loss {torch.round(train_loss,4)} Val Loss {torch.round(val_loss,4)}")
TypeError: round() received an invalid combination of arguments - got (Tensor, int), but expected one of:
 * (Tensor input, *, Tensor out)
 * (Tensor input, *, int decimals, Tensor out)

Tue Sep 27 18:24:03 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_hid.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 27/09/2022 18:24:07

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: adi17_test_umbrella_label
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/adi17_test_umbrella_label.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[0.1438, 0.1425, 0.1435,  ..., 0.0000, 0.0000, 0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1])}
Test Data Sample
{'input_values': tensor([[0.0261, 0.0750, 0.1125,  ..., 0.0000, 0.0000, 0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0])}
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Epoch 0 Train Acc 0.0 Val Acc 100.0 Train Loss 0.7699435949325562 Val Loss 1.2746820449829102
EPOCH unfeeze : 1
Epoch 1 Train Acc 0.0 Val Acc 100.0 Train Loss 0.7617188692092896 Val Loss 1.268558144569397
EPOCH unfeeze : 2
Epoch 2 Train Acc 0.0 Val Acc 100.0 Train Loss 0.7404196262359619 Val Loss 1.2629417181015015
EPOCH unfeeze : 3
Epoch 3 Train Acc 0.0 Val Acc 100.0 Train Loss 0.7129544019699097 Val Loss 1.2524957656860352
EPOCH unfeeze : 4
Epoch 4 Train Acc 0.0 Val Acc 100.0 Train Loss 0.6806078553199768 Val Loss 1.25892972946167
EPOCH unfeeze : 5
Epoch 5 Train Acc 100.0 Val Acc 100.0 Train Loss 0.6387083530426025 Val Loss 1.2768349647521973
EPOCH unfeeze : 6
Epoch 6 Train Acc 100.0 Val Acc 75.0 Train Loss 0.5892533659934998 Val Loss 1.297603726387024
EPOCH unfeeze : 7
Epoch 7 Train Acc 100.0 Val Acc 50.0 Train Loss 0.5384565591812134 Val Loss 1.3303823471069336
EPOCH unfeeze : 8
Epoch 8 Train Acc 100.0 Val Acc 25.0 Train Loss 0.4874166250228882 Val Loss 1.3683841228485107
EPOCH unfeeze : 9
Epoch 9 Train Acc 100.0 Val Acc 25.0 Train Loss 0.43367794156074524 Val Loss 1.3985373973846436
EPOCH unfeeze : 0
Updated Parameters at Epoch 10 Trainable Parameters : 85648900
Epoch 10 Train Acc 100.0 Val Acc 0.0 Train Loss 0.38253241777420044 Val Loss 1.539139747619629
EPOCH unfeeze : 1
Epoch 11 Train Acc 100.0 Val Acc 0.0 Train Loss 0.3352128267288208 Val Loss 1.6356233358383179
EPOCH unfeeze : 2
Epoch 12 Train Acc 100.0 Val Acc 0.0 Train Loss 0.2950836420059204 Val Loss 1.6689374446868896
EPOCH unfeeze : 3
Epoch 13 Train Acc 100.0 Val Acc 0.0 Train Loss 0.25943028926849365 Val Loss 1.7847518920898438
EPOCH unfeeze : 4
Epoch 14 Train Acc 100.0 Val Acc 0.0 Train Loss 0.23211728036403656 Val Loss 1.7912743091583252
EPOCH unfeeze : 5
Epoch 15 Train Acc 100.0 Val Acc 0.0 Train Loss 0.21276351809501648 Val Loss 1.8344398736953735
EPOCH unfeeze : 6
Epoch 16 Train Acc 100.0 Val Acc 0.0 Train Loss 0.18857687711715698 Val Loss 1.9503216743469238
EPOCH unfeeze : 7
Epoch 17 Train Acc 100.0 Val Acc 0.0 Train Loss 0.1717260628938675 Val Loss 2.074460029602051
EPOCH unfeeze : 8
Epoch 18 Train Acc 100.0 Val Acc 0.0 Train Loss 0.15841493010520935 Val Loss 2.154055118560791
EPOCH unfeeze : 9
Epoch 19 Train Acc 100.0 Val Acc 0.0 Train Loss 0.14715476334095 Val Loss 2.1657352447509766
EPOCH unfeeze : 0
Updated Parameters at Epoch 20 Trainable Parameters : 85648900
Epoch 20 Train Acc 100.0 Val Acc 0.0 Train Loss 0.13546821475028992 Val Loss 2.245781660079956
EPOCH unfeeze : 1
Epoch 21 Train Acc 100.0 Val Acc 0.0 Train Loss 0.12714731693267822 Val Loss 2.423642158508301
EPOCH unfeeze : 2
Epoch 22 Train Acc 100.0 Val Acc 0.0 Train Loss 0.11861516535282135 Val Loss 2.4737112522125244
EPOCH unfeeze : 3
Epoch 23 Train Acc 100.0 Val Acc 0.0 Train Loss 0.11045687645673752 Val Loss 2.518559217453003
EPOCH unfeeze : 4
Epoch 24 Train Acc 100.0 Val Acc 0.0 Train Loss 0.1072402223944664 Val Loss 2.568880558013916
EPOCH unfeeze : 5
Epoch 25 Train Acc 100.0 Val Acc 0.0 Train Loss 0.09972988069057465 Val Loss 2.539306402206421
EPOCH unfeeze : 6
Epoch 26 Train Acc 100.0 Val Acc 0.0 Train Loss 0.09269887208938599 Val Loss 2.5878515243530273
EPOCH unfeeze : 7
Epoch 27 Train Acc 100.0 Val Acc 0.0 Train Loss 0.08803694695234299 Val Loss 2.669095516204834
EPOCH unfeeze : 8
Epoch 28 Train Acc 100.0 Val Acc 0.0 Train Loss 0.08652931451797485 Val Loss 2.5797531604766846
EPOCH unfeeze : 9
Epoch 29 Train Acc 100.0 Val Acc 0.0 Train Loss 0.08150920271873474 Val Loss 2.662647247314453
EPOCH unfeeze : 0
Updated Parameters at Epoch 30 Trainable Parameters : 85648900
Epoch 30 Train Acc 100.0 Val Acc 0.0 Train Loss 0.07838980108499527 Val Loss 2.780792713165283
EPOCH unfeeze : 1
Epoch 31 Train Acc 100.0 Val Acc 0.0 Train Loss 0.07463326305150986 Val Loss 2.7935492992401123
EPOCH unfeeze : 2
Epoch 32 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06986522674560547 Val Loss 2.8152284622192383
EPOCH unfeeze : 3
Epoch 33 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06717463582754135 Val Loss 2.903146743774414
EPOCH unfeeze : 4
Epoch 34 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06420165300369263 Val Loss 2.951214551925659
EPOCH unfeeze : 5
Epoch 35 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06173386052250862 Val Loss 2.91005277633667Tue Sep 27 18:41:51 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_q.weight', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
Configuration saved in ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

EPOCH unfeeze : 6
Epoch 36 Train Acc 100.0 Val Acc 0.0 Train Loss 0.060015689581632614 Val Loss 3.025819778442383
EPOCH unfeeze : 7
Epoch 37 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05755801126360893 Val Loss 3.005110263824463
EPOCH unfeeze : 8
Epoch 38 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05517945438623428 Val Loss 3.071652412414551
EPOCH unfeeze : 9
Epoch 39 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05252590775489807 Val Loss 3.1634573936462402
EPOCH unfeeze : 0
Updated Parameters at Epoch 40 Trainable Parameters : 85648900
Epoch 40 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05000273138284683 Val Loss 3.1582202911376953
EPOCH unfeeze : 1
Epoch 41 Train Acc 100.0 Val Acc 0.0 Train Loss 0.048287708312273026 Val Loss 3.2545878887176514
EPOCH unfeeze : 2
Epoch 42 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04698340222239494 Val Loss 3.2704811096191406
EPOCH unfeeze : 3
Epoch 43 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0450124517083168 Val Loss 3.341635227203369
EPOCH unfeeze : 4
Epoch 44 Train Acc 100.0 Val Acc 0.0 Train Loss 0.043436646461486816 Val Loss 3.3716635704040527
EPOCH unfeeze : 5
Epoch 45 Train Acc 100.0 Val Acc 0.0 Train Loss 0.042154330760240555 Val Loss 3.4142210483551025
EPOCH unfeeze : 6
Epoch 46 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04079751670360565 Val Loss 3.4491775035858154
EPOCH unfeeze : 7
Epoch 47 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0389447882771492 Val Loss 3.4917032718658447
EPOCH unfeeze : 8
Epoch 48 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03756101056933403 Val Loss 3.5356619358062744
EPOCH unfeeze : 9
Epoch 49 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03642662242054939 Val Loss 3.5690457820892334
EPOCH unfeeze : 0
Updated Parameters at Epoch 50 Trainable Parameters : 85648900
Epoch 50 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03549656644463539 Val Loss 3.6275181770324707
EPOCH unfeeze : 1
Epoch 51 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03378000110387802 Val Loss 3.6456596851348877
EPOCH unfeeze : 2
Epoch 52 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03250811994075775 Val Loss 3.6665706634521484
EPOCH unfeeze : 3
Epoch 53 Train Acc 100.0 Val Acc 0.0 Train Loss 0.031435757875442505 Val Loss 3.7259716987609863
EPOCH unfeeze : 4
Epoch 54 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03019372746348381 Val Loss 3.765462636947632
EPOCH unfeeze : 5
Epoch 55 Train Acc 100.0 Val Acc 0.0 Train Loss 0.029220644384622574 Val Loss 3.8105859756469727
EPOCH unfeeze : 6
Epoch 56 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02842717245221138 Val Loss 3.8788890838623047
EPOCH unfeeze : 7
Epoch 57 Train Acc 100.0 Val Acc 0.0 Train Loss 0.027196073904633522 Val Loss 3.924988031387329
EPOCH unfeeze : 8
Epoch 58 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02620687149465084 Val Loss 3.954038143157959
EPOCH unfeeze : 9
Epoch 59 Train Acc 100.0 Val Acc 0.0 Train Loss 0.025322064757347107 Val Loss 3.9858951568603516
EPOCH unfeeze : 0
Updated Parameters at Epoch 60 Trainable Parameters : 85648900
Epoch 60 Train Acc 100.0 Val Acc 0.0 Train Loss 0.024470573291182518 Val Loss 4.023141860961914
EPOCH unfeeze : 1
Epoch 61 Train Acc 100.0 Val Acc 0.0 Train Loss 0.023610254749655724 Val Loss 4.067887306213379
EPOCH unfeeze : 2
Epoch 62 Train Acc 100.0 Val Acc 0.0 Train Loss 0.022798122838139534 Val Loss 4.100013732910156
EPOCH unfeeze : 3
Epoch 63 Train Acc 100.0 Val Acc 0.0 Train Loss 0.021977081894874573 Val Loss 4.132033348083496
EPOCH unfeeze : 4
Epoch 64 Train Acc 100.0 Val Acc 0.0 Train Loss 0.021246258169412613 Val Loss 4.1685404777526855
EPOCH unfeeze : 5
Epoch 65 Train Acc 100.0 Val Acc 0.0 Train Loss 0.020433643832802773 Val Loss 4.210210800170898
EPOCH unfeeze : 6
Epoch 66 Train Acc 100.0 Val Acc 0.0 Train Loss 0.019803153350949287 Val Loss 4.237498760223389
EPOCH unfeeze : 7
Epoch 67 Train Acc 100.0 Val Acc 0.0 Train Loss 0.019075006246566772 Val Loss 4.290325164794922
EPOCH unfeeze : 8
Epoch 68 Train Acc 100.0 Val Acc 0.0 Train Loss 0.018384622409939766 Val Loss 4.329775333404541
EPOCH unfeeze : 9
Epoch 69 Train Acc 100.0 Val Acc 0.0 Train Loss 0.017808791249990463 Val Loss 4.364768981933594
EPOCH unfeeze : 0
Updated Parameters at Epoch 70 Trainable Parameters : 85648900
Epoch 70 Train Acc 100.0 Val Acc 0.0 Train Loss 0.017121203243732452 Val Loss 4.411673545837402
EPOCH unfeeze : 1
Epoch 71 Train Acc 100.0 Val Acc 0.0 Train Loss 0.016513509675860405 Val Loss 4.455008506774902
EPOCH unfeeze : 2
Epoch 72 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01591586135327816 Val Loss 4.481276988983154
EPOCH unfeeze : 3
Epoch 73 Train Acc 100.0 Val Acc 0.0 Train Loss 0.015366259030997753 Val Loss 4.532350063323975
EPOCH unfeeze : 4
Epoch 74 Train Acc 100.0 Val Acc 0.0 Train Loss 0.014818400144577026 Val Loss 4.603313446044922
EPOCH unfeeze : 5
Epoch 75 Train Acc 100.0 Val Acc 0.0 Train Loss 0.014305997639894485 Val Loss 4.6505537033081055
EPOCH unfeeze : 6
Epoch 76 Train Acc 100.0 Val Acc 0.0 Train Loss 0.013793348334729671 Val Loss 4.6825761795043945
EPOCH unfeeze : 7
Epoch 77 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01331609208136797 Val Loss 4.7142486572265625
EPOCH unfeeze : 8
Epoch 78 Train Acc 100.0 Val Acc 0.0 Train Loss 0.012848285026848316 Val Loss 4.757743835449219
EPOCH unfeeze : 9
Epoch 79 Train Acc 100.0 Val Acc 0.0 Train Loss 0.012396308593451977 Val Loss 4.797051906585693
EPOCH unfeeze : 0
Updated Parameters at Epoch 80 Trainable Parameters : 85648900
Epoch 80 Train Acc 100.0 Val Acc 0.0 Train Loss 0.011965654790401459 Val Loss 4.831730365753174
EPOCH unfeeze : 1
Epoch 81 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01155044510960579 Val Loss 4.87357234954834
EPOCH unfeeze : 2
Epoch 82 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01116134412586689 Val Loss 4.899181365966797
EPOCH unfeeze : 3
Epoch 83 Train Acc 100.0 Val Acc 0.0 Train Loss 0.010760894045233727 Val Loss 4.930504322052002
EPOCH unfeeze : 4
Epoch 84 Train Acc 100.0 Val Acc 0.0 Train Loss 0.010397350415587425 Val Loss 4.976637840270996
EPOCH unfeeze : 5
Epoch 85 Train Acc 100.0 Val Acc 0.0 Train Loss 0.010017765685915947 Val Loss 5.01441764831543
EPOCH unfeeze : 6
Epoch 86 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009675650857388973 Val Loss 5.0466485023498535
EPOCH unfeeze : 7
Epoch 87 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009339604526758194 Val Loss 5.103852272033691
EPOCH unfeeze : 8
Epoch 88 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009009046480059624 Val Loss 5.127779960632324
EPOCH unfeeze : 9
Epoch 89 Train Acc 100.0 Val Acc 0.0 Train Loss 0.008709587156772614 Val Loss 5.177919387817383
EPOCH unfeeze : 0
Updated Parameters at Epoch 90 Trainable Parameters : 85648900
Epoch 90 Train Acc 100.0 Val Acc 0.0 Train Loss 0.008401161059737206 Val Loss 5.210419654846191
EPOCH unfeeze : 1
Epoch 91 Train Acc 100.0 Val Acc 0.0 Train Loss 0.008123459294438362 Val Loss 5.241835117340088
EPOCH unfeeze : 2
Epoch 92 Train Acc 100.0 Val Acc 0.0 Train Loss 0.007840489968657494 Val Loss 5.278937339782715
EPOCH unfeeze : 3
Epoch 93 Train Acc 100.0 Val Acc 0.0 Train Loss 0.007567099295556545 Val Loss 5.31137228012085
EPOCH unfeeze : 4
Epoch 94 Train Acc 100.0 Val Acc 0.0 Train Loss 0.007315906696021557 Val Loss 5.35195255279541
EPOCH unfeeze : 5
Epoch 95 Train Acc 100.0 Val Acc 0.0 Train Loss 0.007075243629515171 Val Loss 5.387192249298096
EPOCH unfeeze : 6
Epoch 96 Train Acc 100.0 Val Acc 0.0 Train Loss 0.006826221477240324 Val Loss 5.418123722076416
EPOCH unfeeze : 7
Epoch 97 Train Acc 100.0 Val Acc 0.0 Train Loss 0.00660497322678566 Val Loss 5.456672191619873
EPOCH unfeeze : 8
Epoch 98 Train Acc 100.0 Val Acc 0.0 Train Loss 0.006374996155500412 Val Loss 5.489117622375488
EPOCH unfeeze : 9
Epoch 99 Train Acc 100.0 Val Acc 0.0 Train Loss 0.006163178943097591 Val Loss 5.522960662841797

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:00<00:00,  2.49it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.77it/s]100%|██████████| 4/4 [00:02<00:00,  1.53it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       4.0
           1       0.00      0.00      0.00       0.0

    accuracy                           0.00       4.0
   macro avg       0.00      0.00      0.00       4.0
weighted avg       0.00      0.00      0.00       4.0


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 27/09/2022 18:50:15
100%|██████████| 4/4 [00:02<00:00,  1.58it/s]------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 27/09/2022 18:41:55

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: adi17_test_umbrella_label
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/adi17_test_umbrella_label.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0235, -0.0310, -0.0424,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1])}
Test Data Sample
{'input_values': tensor([[-0.6328, -0.1130,  0.3310,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0])}
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Epoch 0 Train Acc 25.0 Val Acc 25.0 Train Loss 0.6779129505157471 Val Loss 1.4507466554641724
EPOCH unfeeze : 1
Epoch 1 Train Acc 50.0 Val Acc 0.0 Train Loss 0.6710060834884644 Val Loss 1.4641697406768799
EPOCH unfeeze : 2
Epoch 2 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6604161262512207 Val Loss 1.4960445165634155
EPOCH unfeeze : 3
Epoch 3 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6328456997871399 Val Loss 1.4975488185882568
EPOCH unfeeze : 4
Epoch 4 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6218172311782837 Val Loss 1.50077223777771
EPOCH unfeeze : 5
Epoch 5 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5980443954467773 Val Loss 1.5074446201324463
EPOCH unfeeze : 6
Epoch 6 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5604893565177917 Val Loss 1.5121185779571533
EPOCH unfeeze : 7
Epoch 7 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5326868891716003 Val Loss 1.518691062927246
EPOCH unfeeze : 8
Epoch 8 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5082906484603882 Val Loss 1.5233880281448364
EPOCH unfeeze : 9
Epoch 9 Train Acc 100.0 Val Acc 0.0 Train Loss 0.4672837555408478 Val Loss 1.5270631313323975
EPOCH unfeeze : 0
Updated Parameters at Epoch 10 Trainable Parameters : 85648900
Epoch 10 Train Acc 100.0 Val Acc 0.0 Train Loss 0.42589330673217773 Val Loss 1.5523349046707153
EPOCH unfeeze : 1
Epoch 11 Train Acc 100.0 Val Acc 0.0 Train Loss 0.3820972442626953 Val Loss 1.5660738945007324
EPOCH unfeeze : 2
Epoch 12 Train Acc 100.0 Val Acc 0.0 Train Loss 0.3358542025089264 Val Loss 1.5970330238342285
EPOCH unfeeze : 3
Epoch 13 Train Acc 100.0 Val Acc 0.0 Train Loss 0.30097290873527527 Val Loss 1.6208099126815796
EPOCH unfeeze : 4
Epoch 14 Train Acc 100.0 Val Acc 0.0 Train Loss 0.2742471694946289 Val Loss 1.6707086563110352
EPOCH unfeeze : 5
Epoch 15 Train Acc 100.0 Val Acc 0.0 Train Loss 0.2511580288410187 Val Loss 1.6964647769927979
EPOCH unfeeze : 6
Epoch 16 Train Acc 100.0 Val Acc 0.0 Train Loss 0.23487553000450134 Val Loss 1.7835776805877686
EPOCH unfeeze : 7
Epoch 17 Train Acc 100.0 Val Acc 0.0 Train Loss 0.2144639790058136 Val Loss 1.8423434495925903
EPOCH unfeeze : 8
Epoch 18 Train Acc 100.0 Val Acc 0.0 Train Loss 0.20150545239448547 Val Loss 1.8768067359924316
EPOCH unfeeze : 9
Epoch 19 Train Acc 100.0 Val Acc 0.0 Train Loss 0.18745677173137665 Val Loss 1.8765206336975098
EPOCH unfeeze : 0
Updated Parameters at Epoch 20 Trainable Parameters : 85648900
Epoch 20 Train Acc 100.0 Val Acc 0.0 Train Loss 0.17435194551944733 Val Loss 1.9816097021102905
EPOCH unfeeze : 1
Epoch 21 Train Acc 100.0 Val Acc 0.0 Train Loss 0.1649690568447113 Val Loss 1.9149436950683594
EPOCH unfeeze : 2
Epoch 22 Train Acc 100.0 Val Acc 0.0 Train Loss 0.1571214348077774 Val Loss 2.020273447036743
EPOCH unfeeze : 3
Epoch 23 Train Acc 100.0 Val Acc 0.0 Train Loss 0.14759041368961334 Val Loss 2.106203317642212
EPOCH unfeeze : 4
Epoch 24 Train Acc 100.0 Val Acc 0.0 Train Loss 0.1366538405418396 Val Loss 2.2041122913360596
EPOCH unfeeze : 5
Epoch 25 Train Acc 100.0 Val Acc 0.0 Train Loss 0.12996092438697815 Val Loss 2.194053888320923
EPOCH unfeeze : 6
Epoch 26 Train Acc 100.0 Val Acc 0.0 Train Loss 0.12401919811964035 Val Loss 2.1827988624572754
EPOCH unfeeze : 7
Epoch 27 Train Acc 100.0 Val Acc 0.0 Train Loss 0.11761578172445297 Val Loss 2.3690271377563477
EPOCH unfeeze : 8
Epoch 28 Train Acc 100.0 Val Acc 0.0 Train Loss 0.11334551870822906 Val Loss 2.3733677864074707
EPOCH unfeeze : 9
Epoch 29 Train Acc 100.0 Val Acc 0.0 Train Loss 0.109959676861763 Val Loss 2.3639817237854004
EPOCH unfeeze : 0
Updated Parameters at Epoch 30 Trainable Parameters : 85648900
Epoch 30 Train Acc 100.0 Val Acc 0.0 Train Loss 0.10058595985174179 Val Loss 2.487671375274658
EPOCH unfeeze : 1
Epoch 31 Train Acc 100.0 Val Acc 0.0 Train Loss 0.09544813632965088 Val Loss 2.5184528827667236
EPOCH unfeeze : 2
Epoch 32 Train Acc 100.0 Val Acc 0.0 Train Loss 0.09316237270832062 Val Loss 2.548016309738159
EPOCH unfeeze : 3
Epoch 33 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0867144986987114 Val Loss 2.6783065795898438
EPOCH unfeeze : 4
Epoch 34 Train Acc 100.0 Val Acc 0.0 Train Loss 0.08353022485971451 Val Loss 2.810884952545166
EPOCH unfeeze : 5
Epoch 35 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0800328403711319 Val Loss 2.7175378799438477Configuration saved in ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

EPOCH unfeeze : 6
Epoch 36 Train Acc 100.0 Val Acc 0.0 Train Loss 0.07643350213766098 Val Loss 2.726620674133301
EPOCH unfeeze : 7
Epoch 37 Train Acc 100.0 Val Acc 0.0 Train Loss 0.07384516298770905 Val Loss 2.7674694061279297
EPOCH unfeeze : 8
Epoch 38 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0712563693523407 Val Loss 2.961143970489502
EPOCH unfeeze : 9
Epoch 39 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0672994926571846 Val Loss 3.0281636714935303
EPOCH unfeeze : 0
Updated Parameters at Epoch 40 Trainable Parameters : 85648900
Epoch 40 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06470160186290741 Val Loss 3.0721540451049805
EPOCH unfeeze : 1
Epoch 41 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06405183672904968 Val Loss 3.135429620742798
EPOCH unfeeze : 2
Epoch 42 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06013436242938042 Val Loss 3.2034239768981934
EPOCH unfeeze : 3
Epoch 43 Train Acc 100.0 Val Acc 0.0 Train Loss 0.057822443544864655 Val Loss 3.2564523220062256
EPOCH unfeeze : 4
Epoch 44 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05656491592526436 Val Loss 3.2150192260742188
EPOCH unfeeze : 5
Epoch 45 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05337855592370033 Val Loss 3.3541147708892822
EPOCH unfeeze : 6
Epoch 46 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0510612353682518 Val Loss 3.392057180404663
EPOCH unfeeze : 7
Epoch 47 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0500645637512207 Val Loss 3.4252872467041016
EPOCH unfeeze : 8
Epoch 48 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04777545481920242 Val Loss 3.4661502838134766
EPOCH unfeeze : 9
Epoch 49 Train Acc 100.0 Val Acc 0.0 Train Loss 0.045812610536813736 Val Loss 3.5184690952301025
EPOCH unfeeze : 0
Updated Parameters at Epoch 50 Trainable Parameters : 85648900
Epoch 50 Train Acc 100.0 Val Acc 0.0 Train Loss 0.044237881898880005 Val Loss 3.5869596004486084
EPOCH unfeeze : 1
Epoch 51 Train Acc 100.0 Val Acc 0.0 Train Loss 0.042589038610458374 Val Loss 3.6204733848571777
EPOCH unfeeze : 2
Epoch 52 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04223966225981712 Val Loss 3.6660876274108887
EPOCH unfeeze : 3
Epoch 53 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03975352644920349 Val Loss 3.7113661766052246
EPOCH unfeeze : 4
Epoch 54 Train Acc 100.0 Val Acc 0.0 Train Loss 0.037999920547008514 Val Loss 3.7618589401245117
EPOCH unfeeze : 5
Epoch 55 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03661511093378067 Val Loss 3.811135768890381
EPOCH unfeeze : 6
Epoch 56 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03554927185177803 Val Loss 3.756700038909912
EPOCH unfeeze : 7
Epoch 57 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03490987420082092 Val Loss 3.8380677700042725
EPOCH unfeeze : 8
Epoch 58 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03297862410545349 Val Loss 3.925501823425293
EPOCH unfeeze : 9
Epoch 59 Train Acc 100.0 Val Acc 0.0 Train Loss 0.031696200370788574 Val Loss 3.9344398975372314
EPOCH unfeeze : 0
Updated Parameters at Epoch 60 Trainable Parameters : 85648900
Epoch 60 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03064289316534996 Val Loss 3.986100435256958
EPOCH unfeeze : 1
Epoch 61 Train Acc 100.0 Val Acc 0.0 Train Loss 0.029515836387872696 Val Loss 4.022651195526123
EPOCH unfeeze : 2
Epoch 62 Train Acc 100.0 Val Acc 0.0 Train Loss 0.028562095016241074 Val Loss 4.042974948883057
EPOCH unfeeze : 3
Epoch 63 Train Acc 100.0 Val Acc 0.0 Train Loss 0.027706189081072807 Val Loss 4.091213703155518
EPOCH unfeeze : 4
Epoch 64 Train Acc 100.0 Val Acc 0.0 Train Loss 0.026539243757724762 Val Loss 4.119760513305664
EPOCH unfeeze : 5
Epoch 65 Train Acc 100.0 Val Acc 0.0 Train Loss 0.025689255446195602 Val Loss 4.176583290100098
EPOCH unfeeze : 6
Epoch 66 Train Acc 100.0 Val Acc 0.0 Train Loss 0.024691691622138023 Val Loss 4.232728958129883
EPOCH unfeeze : 7
Epoch 67 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0237454641610384 Val Loss 4.2757954597473145
EPOCH unfeeze : 8
Epoch 68 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02290480025112629 Val Loss 4.285281181335449
EPOCH unfeeze : 9
Epoch 69 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02210666984319687 Val Loss 4.346578121185303
EPOCH unfeeze : 0
Updated Parameters at Epoch 70 Trainable Parameters : 85648900
Epoch 70 Train Acc 100.0 Val Acc 0.0 Train Loss 0.021443292498588562 Val Loss 4.296043395996094
EPOCH unfeeze : 1
Epoch 71 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02072833478450775 Val Loss 4.423110008239746
EPOCH unfeeze : 2
Epoch 72 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01979796402156353 Val Loss 4.445687294006348
EPOCH unfeeze : 3
Epoch 73 Train Acc 100.0 Val Acc 0.0 Train Loss 0.019102465361356735 Val Loss 4.49371337890625
EPOCH unfeeze : 4
Epoch 74 Train Acc 100.0 Val Acc 0.0 Train Loss 0.018430467694997787 Val Loss 4.52863883972168
EPOCH unfeeze : 5
Epoch 75 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01780661568045616 Val Loss 4.554487228393555
EPOCH unfeeze : 6
Epoch 76 Train Acc 100.0 Val Acc 0.0 Train Loss 0.017161330208182335 Val Loss 4.600236892700195
EPOCH unfeeze : 7
Epoch 77 Train Acc 100.0 Val Acc 0.0 Train Loss 0.016566701233386993 Val Loss 4.629220485687256
EPOCH unfeeze : 8
Epoch 78 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01597549393773079 Val Loss 4.663195610046387
EPOCH unfeeze : 9
Epoch 79 Train Acc 100.0 Val Acc 0.0 Train Loss 0.015429726801812649 Val Loss 4.704554080963135
EPOCH unfeeze : 0
Updated Parameters at Epoch 80 Trainable Parameters : 85648900
Epoch 80 Train Acc 100.0 Val Acc 0.0 Train Loss 0.014868992380797863 Val Loss 4.743775844573975
EPOCH unfeeze : 1
Epoch 81 Train Acc 100.0 Val Acc 0.0 Train Loss 0.014338563196361065 Val Loss 4.80109977722168
EPOCH unfeeze : 2
Epoch 82 Train Acc 100.0 Val Acc 0.0 Train Loss 0.013852580450475216 Val Loss 4.821230888366699
EPOCH unfeeze : 3
Epoch 83 Train Acc 100.0 Val Acc 0.0 Train Loss 0.013363336212933064 Val Loss 4.861356735229492
EPOCH unfeeze : 4
Epoch 84 Train Acc 100.0 Val Acc 0.0 Train Loss 0.012879663147032261 Val Loss 4.891848564147949
EPOCH unfeeze : 5
Epoch 85 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01243651658296585 Val Loss 4.936812877655029
EPOCH unfeeze : 6
Epoch 86 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01200009137392044 Val Loss 4.961062908172607
EPOCH unfeeze : 7
Epoch 87 Train Acc 100.0 Val Acc 0.0 Train Loss 0.011592081747949123 Val Loss 4.992489337921143
EPOCH unfeeze : 8
Epoch 88 Train Acc 100.0 Val Acc 0.0 Train Loss 0.011182129383087158 Val Loss 5.023632526397705
EPOCH unfeeze : 9
Epoch 89 Train Acc 100.0 Val Acc 0.0 Train Loss 0.010796702466905117 Val Loss 5.053033828735352
EPOCH unfeeze : 0
Updated Parameters at Epoch 90 Trainable Parameters : 85648900
Epoch 90 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01042257808148861 Val Loss 5.105637550354004
EPOCH unfeeze : 1
Epoch 91 Train Acc 100.0 Val Acc 0.0 Train Loss 0.010060086846351624 Val Loss 5.129863262176514
EPOCH unfeeze : 2
Epoch 92 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009706183336675167 Val Loss 5.166949272155762
EPOCH unfeeze : 3
Epoch 93 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009373107925057411 Val Loss 5.2030229568481445
EPOCH unfeeze : 4
Epoch 94 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009052407927811146 Val Loss 5.23945426940918
EPOCH unfeeze : 5
Epoch 95 Train Acc 100.0 Val Acc 0.0 Train Loss 0.00874598789960146 Val Loss 5.272425174713135
EPOCH unfeeze : 6
Epoch 96 Train Acc 100.0 Val Acc 0.0 Train Loss 0.00843874178826809 Val Loss 5.303096771240234
EPOCH unfeeze : 7
Epoch 97 Train Acc 100.0 Val Acc 0.0 Train Loss 0.00814889371395111 Val Loss 5.342472076416016
EPOCH unfeeze : 8
Epoch 98 Train Acc 100.0 Val Acc 0.0 Train Loss 0.007871575653553009 Val Loss 5.38116455078125
EPOCH unfeeze : 9
Epoch 99 Train Acc 100.0 Val Acc 0.0 Train Loss 0.007602175697684288 Val Loss 5.406600475311279

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s] 50%|█████     | 2/4 [00:00<00:00,  2.51it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.77it/s]100%|██████████| 4/4 [00:02<00:00,  1.53it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       4.0
           1       0.00      0.00      0.00       0.0

    accuracy                           0.00       4.0
   macro avg       0.00      0.00      0.00       4.0
weighted avg       0.00      0.00      0.00       4.0


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 27/09/2022 19:08:05
100%|██████████| 4/4 [00:02<00:00,  1.57it/s]Tue Sep 27 19:12:33 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.weight_proj.weight', 'project_hid.bias', 'project_hid.weight', 'quantizer.weight_proj.bias', 'project_q.weight', 'quantizer.codevectors', 'project_q.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 27/09/2022 19:12:37

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: adi17_test_umbrella_label
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/adi17_test_umbrella_label.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 20 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-6.5257e-03,  4.0407e-05,  1.8643e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1])}
Training Data Files: 4
Test Data Sample
{'input_values': tensor([[-0.0621, -0.0449, -0.0356,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0])}
Test Data Files: 4
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Traceback (most recent call last):
  File "run_basic.py", line 965, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 788, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_basic.py", line 809, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_basic.py", line 852, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "run_basic.py", line 562, in forward
    outputs = self.wav2vec2(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1304, in forward
    encoder_outputs = self.encoder(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 782, in forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 235, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 96, in forward
    outputs = run_function(*args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 778, in custom_forward
    return module(*inputs, output_attentions)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 675, in forward
    hidden_states, attn_weights, _ = self.attention(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 613, in forward
    attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 11.44 GiB (GPU 0; 31.75 GiB total capacity; 19.05 GiB already allocated; 2.39 GiB free; 28.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Thu Sep 29 12:55:34 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_hid.weight', 'quantizer.weight_proj.weight', 'project_q.weight', 'quantizer.codevectors', 'project_q.bias', 'project_hid.bias', 'quantizer.weight_proj.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 29/09/2022 12:55:42

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: adi17_test_umbrella_label
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/adi17_test_umbrella_label.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 20 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-6.5257e-03,  4.0407e-05,  1.8643e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1])}
Training Data Files: 4
Test Data Sample
{'input_values': tensor([[-0.0621, -0.0449, -0.0356,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0])}
Test Data Files: 4
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Traceback (most recent call last):
  File "run_basic.py", line 965, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 788, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_basic.py", line 809, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_basic.py", line 852, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "run_basic.py", line 562, in forward
    outputs = self.wav2vec2(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1304, in forward
    encoder_outputs = self.encoder(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 782, in forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 235, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 96, in forward
    outputs = run_function(*args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 778, in custom_forward
    return module(*inputs, output_attentions)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 675, in forward
    hidden_states, attn_weights, _ = self.attention(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 613, in forward
    attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 11.44 GiB (GPU 0; 31.75 GiB total capacity; 19.05 GiB already allocated; 2.39 GiB free; 28.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Thu Sep 29 13:12:53 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.codevectors', 'project_q.weight', 'project_hid.bias', 'project_hid.weight', 'project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 29/09/2022 13:13:01

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-initialtest
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: adi17_test_umbrella_label
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/adi17_test_umbrella_label.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/wav2vec-ADI17-initialtest_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 15 s
Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[0.1438, 0.1425, 0.1435,  ..., 0.0000, 0.0000, 0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1])}
Training DataCustom Files: 4
Training Data Files: 4
Test Data Sample
{'input_values': tensor([[-0.6328, -0.1130,  0.3310,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0])}
Test CustomData Files: 4
Test Data Files: 4
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Traceback (most recent call last):
  File "run_basic.py", line 967, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 790, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_basic.py", line 818, in _train
    loss.backward()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 146, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 6.44 GiB (GPU 0; 31.75 GiB total capacity; 23.40 GiB already allocated; 768.00 MiB free; 29.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
