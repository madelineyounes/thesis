Wed Nov 16 17:27:56 AEDT 2022
2022-11-16 17:27:58.496943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 17:27:58.876475: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-16 17:27:59.009575: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 17:28:00.466477: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:28:00.468233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:28:00.468244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_400f.py
Started: 16/11/2022 17:28:12

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-400f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_400f
train_filename: u_train_400f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_400f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_400f_local/ADI17-xlsr-araic-400f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_400f_local/ADI17-xlsr-araic-400f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Traceback (most recent call last):
  File "run_xlsr_400f.py", line 378, in <module>
    traincustomdata = CustomDataset(
  File "/home/z5208494/thesis/customData.py", line 35, in __init__
    self.data_frame = pd.read_csv(csv_fp, delimiter=',')
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File data/u_train_400f.csv does not exist: 'data/u_train_400f.csv'
Wed Nov 16 17:34:23 AEDT 2022
2022-11-16 17:34:25.076319: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 17:34:25.292864: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-16 17:34:25.327132: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 17:34:26.670356: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:34:26.673033: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:34:26.673043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_400f.py
Started: 16/11/2022 17:34:38

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-400f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_400f
train_filename: u_train_400f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_400f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_400f_local/ADI17-xlsr-araic-400f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_400f_local/ADI17-xlsr-araic-400f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.3166,  0.2724,  0.2364,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1019, -0.0325, -0.0260,  ...,  0.2522,  0.1488,  0.0436],
        [-0.0590, -0.0481, -0.0137,  ...,  0.1936,  0.2788,  0.2331],
        ...,
        [ 0.0141,  0.0182,  0.0238,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1816,  0.7830,  0.1300,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1418, -0.1883, -0.2600,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 3, 1, 1, 2, 3, 3, 1, 3, 1, 3, 1, 3, 1, 2, 3, 0, 0, 1, 3, 0, 3, 1,
        1, 0, 3, 3, 3, 1, 2, 1, 2, 2, 1, 0, 2, 0, 3, 0])}
Training DataCustom Files: 1442
Training Data Files: 37
Val Data Sample
{'input_values': tensor([[ 0.5509,  0.5225,  0.4898,  ..., -1.3552, -1.4405, -1.5172],
        [-0.2826, -0.4019, -0.3931,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8807, -1.0383, -1.0760,  ..., -0.7737,  2.4600,  0.0100],
        ...,
        [ 1.9325,  1.6524,  1.6133,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2125,  0.4137,  0.5540,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0471,  0.0627,  0.1170,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 1, 3, 2, 0, 0, 3, 1, 1, 2, 0, 3, 1, 3, 1, 3, 1, 0, 3, 3, 0, 0, 0,
        0, 0, 3, 1, 1, 0, 0, 0, 3, 1, 2, 1, 1, 3, 0, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.1725,  0.2181,  0.0275,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0652, -0.1658, -0.1846,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1627, -0.0069, -0.2367,  ..., -0.4048, -0.6739, -0.5310],
        ...,
        [-0.8079, -0.6294, -0.4277,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3453,  0.5344,  0.6483,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2707, -0.1339, -0.1568,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 3, 1, 1, 3, 1, 0, 0, 3, 1, 3, 2, 3, 2, 3, 1, 3, 0, 0, 2, 2, 3, 2, 0,
        3, 0, 1, 0, 3, 1, 0, 1, 3, 0, 3, 2, 1, 1, 1, 3])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 0 Train Acc 28.27027130126953% Val Acc 25.899999618530273% Train Loss 0.692345380783081 Val Loss 1.3901041746139526
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 1 Train Acc 27.08108139038086% Val Acc 26.100000381469727% Train Loss 0.6922370791435242 Val Loss 1.3906114101409912
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 2 Train Acc 27.135135650634766% Val Acc 24.80000114440918% Train Loss 0.6911507844924927 Val Loss 1.3910826444625854
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 3 Train Acc 27.189189910888672% Val Acc 25.399999618530273% Train Loss 0.6892751455307007 Val Loss 1.3881977796554565
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 4 Train Acc 27.297298431396484% Val Acc 23.600000381469727% Train Loss 0.6880514621734619 Val Loss 1.3870726823806763
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 5 Train Acc 30.594594955444336% Val Acc 28.600000381469727% Train Loss 0.6825287342071533 Val Loss 1.3858600854873657
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 6 Train Acc 37.297298431396484% Val Acc 26.399999618530273% Train Loss 0.6794092059135437 Val Loss 1.3906080722808838
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 7 Train Acc 36.91891860961914% Val Acc 34.60000228881836% Train Loss 0.6755489110946655 Val Loss 1.3722240924835205
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 8 Train Acc 39.89189147949219% Val Acc 34.60000228881836% Train Loss 0.6715443730354309 Val Loss 1.3700789213180542
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 9 Train Acc 41.054054260253906% Val Acc 34.20000076293945% Train Loss 0.6592694520950317 Val Loss 1.3627713918685913
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 10 Train Acc 43.945945739746094% Val Acc 33.20000076293945% Train Loss 0.6404479146003723 Val Loss 1.3772937059402466
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 11 Train Acc 47.32432556152344% Val Acc 35.0% Train Loss 0.612055242061615 Val Loss 1.3650740385055542
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 12 Train Acc 50.59459686279297% Val Acc 32.79999923706055% Train Loss 0.591350257396698 Val Loss 1.4474419355392456
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 13 Train Acc 58.10810852050781% Val Acc 38.400001525878906% Train Loss 0.5483683347702026 Val Loss 1.4190815687179565
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 14 Train Acc 60.43243408203125% Val Acc 38.29999923706055% Train Loss 0.5069069862365723 Val Loss 1.4193017482757568
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 15 Train Acc 64.59459686279297% Val Acc 44.400001525878906% Train Loss 0.46435707807540894 Val Loss 1.403704047203064
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 16 Train Acc 68.72972869873047% Val Acc 51.10000228881836% Train Loss 0.42102015018463135 Val Loss 1.282577395439148
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 17 Train Acc 72.08108520507812% Val Acc 45.10000228881836% Train Loss 0.3753833472728729 Val Loss 1.4590930938720703
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 18 Train Acc 77.78378295898438% Val Acc 48.60000228881836% Train Loss 0.32521170377731323 Val Loss 1.4102386236190796
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 19 Train Acc 79.59459686279297% Val Acc 48.29999923706055% Train Loss 0.2920664846897125 Val Loss 1.4920623302459717
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 20 Train Acc 83.10810852050781% Val Acc 47.900001525878906% Train Loss 0.25444644689559937 Val Loss 1.5364484786987305
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 21 Train Acc 86.56756591796875% Val Acc 42.20000076293945% Train Loss 0.20600450038909912 Val Loss 2.0150227546691895
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 22 Train Acc 86.67567443847656% Val Acc 51.29999923706055% Train Loss 0.19561365246772766 Val Loss 1.6669445037841797
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 23 Train Acc 88.9459457397461% Val Acc 56.5% Train Loss 0.17168106138706207 Val Loss 1.4785348176956177
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 24 Train Acc 91.75675964355469% Val Acc 51.400001525878906% Train Loss 0.13026194274425507 Val Loss 1.8744277954101562
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 25 Train Acc 92.27027130126953% Val Acc 54.0% Train Loss 0.12387385219335556 Val Loss 1.6865084171295166
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 26 Train Acc 94.8108139038086% Val Acc 57.400001525878906% Train Loss 0.08637885749340057 Val Loss 1.760630488395691
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 27 Train Acc 95.10810852050781% Val Acc 60.79999923706055% Train Loss 0.08423329889774323 Val Loss 1.6022621393203735
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 28 Train Acc 95.54054260253906% Val Acc 54.5% Train Loss 0.0754929929971695 Val Loss 1.7746490240097046
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 29 Train Acc 95.8648681640625% Val Acc 56.400001525878906% Train Loss 0.0617995411157608 Val Loss 1.9680875539779663
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 30 Train Acc 94.35135650634766% Val Acc 54.5% Train Loss 0.09485362470149994 Val Loss 1.9460468292236328
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 31 Train Acc 96.67567443847656% Val Acc 53.29999923706055% Train Loss 0.05618918314576149 Val Loss 2.2889699935913086
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 32 Train Acc 97.8108139038086% Val Acc 61.79999923706055% Train Loss 0.041094888001680374 Val Loss 1.842616319656372
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 33 Train Acc 98.70270538330078% Val Acc 53.900001525878906% Train Loss 0.028697030618786812 Val Loss 2.3354685306549072
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 34 Train Acc 97.78378295898438% Val Acc 53.900001525878906% Train Loss 0.05380808934569359 Val Loss 2.41589617729187
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 35 Train Acc 97.59459686279297% Val Acc 54.900001525878906% Train Loss 0.04289704188704491 Val Loss 2.3204290866851807
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 36 Train Acc 97.21621704101562% Val Acc 57.10000228881836% Train Loss 0.044707294553518295 Val Loss 2.1367287635803223
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 37 Train Acc 98.29730224609375% Val Acc 62.20000076293945% Train Loss 0.0310147013515234 Val Loss 1.7099964618682861
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 38 Train Acc 98.43243408203125% Val Acc 60.20000076293945% Train Loss 0.03120994009077549 Val Loss 2.221484899520874
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 39 Train Acc 96.78378295898438% Val Acc 60.79999923706055% Train Loss 0.05429821461439133 Val Loss 2.0809707641601562
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 40 Train Acc 97.89189147949219% Val Acc 60.10000228881836% Train Loss 0.039538294076919556 Val Loss 1.965789794921875
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 41 Train Acc 99.0540542602539% Val Acc 52.10000228881836% Train Loss 0.020182877779006958 Val Loss 2.9000167846679688
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 42 Train Acc 98.0% Val Acc 60.10000228881836% Train Loss 0.04191870242357254 Val Loss 2.1589696407318115
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 43 Train Acc 97.9189224243164% Val Acc 57.20000076293945% Train Loss 0.04218776151537895 Val Loss 2.3053507804870605
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 44 Train Acc 98.0540542602539% Val Acc 60.0% Train Loss 0.030295180156826973 Val Loss 2.19913911819458
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 45 Train Acc 98.27027130126953% Val Acc 60.60000228881836% Train Loss 0.03433112055063248 Val Loss 2.1305601596832275
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 46 Train Acc 98.35135650634766% Val Acc 56.0% Train Loss 0.02446352131664753 Val Loss 2.3808534145355225
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 47 Train Acc 97.9189224243164% Val Acc 61.60000228881836% Train Loss 0.03675483167171478 Val Loss 2.158970355987549
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 48 Train Acc 96.8648681640625% Val Acc 57.900001525878906% Train Loss 0.04759310558438301 Val Loss 2.2853384017944336
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Epoch 49 Train Acc 97.72972869873047% Val Acc 57.0% Train Loss 0.03359157219529152 Val Loss 2.5511789321899414
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 50 Train Acc 98.32432556152344% Val Acc 61.5% Train Loss 0.032352179288864136 Val Loss 2.241718053817749
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 51 Train Acc 97.72972869873047% Val Acc 59.5% Train Loss 0.032461438328027725 Val Loss 2.4302427768707275
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 52 Train Acc 98.16216278076172% Val Acc 65.5% Train Loss 0.029651526361703873 Val Loss 1.9822396039962769
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 53 Train Acc 98.40541076660156% Val Acc 58.0% Train Loss 0.028139127418398857 Val Loss 2.4536755084991455
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 54 Train Acc 98.78378295898438% Val Acc 62.70000076293945% Train Loss 0.01851416379213333 Val Loss 2.107708692550659
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 55 Train Acc 97.37837982177734% Val Acc 60.79999923706055% Train Loss 0.03367761895060539 Val Loss 2.3732688426971436
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 56 Train Acc 98.64865112304688% Val Acc 59.70000076293945% Train Loss 0.023977521806955338 Val Loss 2.396925687789917
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 57 Train Acc 98.48648834228516% Val Acc 56.0% Train Loss 0.019609998911619186 Val Loss 2.829453468322754
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 58 Train Acc 97.89189147949219% Val Acc 58.20000076293945% Train Loss 0.035029564052820206 Val Loss 2.2321836948394775
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 59 Train Acc 98.13513946533203% Val Acc 60.60000228881836% Train Loss 0.02887645922601223 Val Loss 2.0482404232025146
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 60 Train Acc 98.13513946533203% Val Acc 58.10000228881836% Train Loss 0.026592686772346497 Val Loss 2.4216413497924805
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 61 Train Acc 99.59459686279297% Val Acc 62.79999923706055% Train Loss 0.011301247403025627 Val Loss 2.029606342315674
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 62 Train Acc 99.43243408203125% Val Acc 63.70000076293945% Train Loss 0.013016829267144203 Val Loss 2.3519177436828613
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 63 Train Acc 99.72972869873047% Val Acc 52.900001525878906% Train Loss 0.00555946072563529 Val Loss 3.027456760406494
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 64 Train Acc 98.59459686279297% Val Acc 55.79999923706055% Train Loss 0.026595376431941986 Val Loss 2.7046518325805664
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 65 Train Acc 98.43243408203125% Val Acc 59.20000076293945% Train Loss 0.024714486673474312 Val Loss 2.5010287761688232
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 66 Train Acc 97.8108139038086% Val Acc 61.400001525878906% Train Loss 0.03069881536066532 Val Loss 2.2860019207000732
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 67 Train Acc 99.27027130126953% Val Acc 59.70000076293945% Train Loss 0.015022729523479939 Val Loss 2.5705769062042236
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 68 Train Acc 99.59459686279297% Val Acc 58.400001525878906% Train Loss 0.009132185950875282 Val Loss 2.802621841430664
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 69 Train Acc 99.37837982177734% Val Acc 56.10000228881836% Train Loss 0.012071901932358742 Val Loss 2.757719039916992
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 70 Train Acc 99.40541076660156% Val Acc 45.5% Train Loss 0.014942060224711895 Val Loss 3.899923324584961
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 71 Train Acc 98.9189224243164% Val Acc 55.79999923706055% Train Loss 0.018687698990106583 Val Loss 2.902655839920044
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 72 Train Acc 98.67567443847656% Val Acc 56.0% Train Loss 0.02479967288672924 Val Loss 2.9208943843841553
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 73 Train Acc 98.89189147949219% Val Acc 62.10000228881836% Train Loss 0.017376678064465523 Val Loss 2.330251455307007
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 74 Train Acc 99.54054260253906% Val Acc 56.400001525878906% Train Loss 0.011063303798437119 Val Loss 2.7645909786224365
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 75 Train Acc 99.21621704101562% Val Acc 56.79999923706055% Train Loss 0.014389107935130596 Val Loss 3.001518726348877
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 76 Train Acc 98.56757354736328% Val Acc 63.29999923706055% Train Loss 0.02112233079969883 Val Loss 2.374119520187378
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 77 Train Acc 99.16216278076172% Val Acc 57.400001525878906% Train Loss 0.014576478861272335 Val Loss 2.8491549491882324
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 78 Train Acc 98.29730224609375% Val Acc 60.79999923706055% Train Loss 0.025453638285398483 Val Loss 2.6386168003082275
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 79 Train Acc 98.62162780761719% Val Acc 53.29999923706055% Train Loss 0.02934538759291172 Val Loss 3.0075223445892334
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 80 Train Acc 98.59459686279297% Val Acc 55.29999923706055% Train Loss 0.022060364484786987 Val Loss 2.9288012981414795
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 81 Train Acc 98.83783721923828% Val Acc 57.79999923706055% Train Loss 0.021119773387908936 Val Loss 2.4871435165405273
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 82 Train Acc 98.54054260253906% Val Acc 57.79999923706055% Train Loss 0.026707476004958153 Val Loss 2.4708950519561768
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 83 Train Acc 98.62162780761719% Val Acc 61.29999923706055% Train Loss 0.019178351387381554 Val Loss 2.6011431217193604
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 84 Train Acc 97.8648681640625% Val Acc 61.0% Train Loss 0.03274843096733093 Val Loss 2.1656548976898193
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 85 Train Acc 99.67567443847656% Val Acc 61.60000228881836% Train Loss 0.006915568839758635 Val Loss 2.6111912727355957
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 86 Train Acc 98.97297668457031% Val Acc 58.60000228881836% Train Loss 0.016349229961633682 Val Loss 2.8450427055358887
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 87 Train Acc 99.75675964355469% Val Acc 59.70000076293945% Train Loss 0.003010485088452697 Val Loss 2.8286635875701904
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 88 Train Acc 98.72972869873047% Val Acc 59.79999923706055% Train Loss 0.01984947733581066 Val Loss 2.75384521484375
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 89 Train Acc 99.51351928710938% Val Acc 58.70000076293945% Train Loss 0.00997789204120636 Val Loss 2.6043241024017334
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 90 Train Acc 98.64865112304688% Val Acc 54.60000228881836% Train Loss 0.02199823595583439 Val Loss 2.9747207164764404
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 91 Train Acc 99.35135650634766% Val Acc 58.79999923706055% Train Loss 0.014575156383216381 Val Loss 2.730186939239502
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 92 Train Acc 99.51351928710938% Val Acc 62.20000076293945% Train Loss 0.01220098789781332 Val Loss 2.6365716457366943
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 93 Train Acc 98.9189224243164% Val Acc 60.70000076293945% Train Loss 0.02052849344909191 Val Loss 2.3319454193115234
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 94 Train Acc 98.0540542602539% Val Acc 57.29999923706055% Train Loss 0.03023906983435154 Val Loss 2.754376173019409
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 95 Train Acc 98.72972869873047% Val Acc 59.60000228881836% Train Loss 0.01944839395582676 Val Loss 2.6850390434265137
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 96 Train Acc 98.9189224243164% Val Acc 54.79999923706055% Train Loss 0.01990162767469883 Val Loss 2.892153024673462
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 97 Train Acc 99.40541076660156% Val Acc 58.10000228881836% Train Loss 0.010868752375245094 Val Loss 2.8826119899749756
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 98 Train Acc 99.02703094482422% Val Acc 63.29999923706055% Train Loss 0.015998074784874916 Val Loss 2.3674943447113037
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Configuration saved in /srv/scratch/z5208494/output/u_train_400f_local/ADI17-xlsr-araic-400f/config.json
Model weights saved in /srv/scratch/z5208494/output/u_train_400f_local/ADI17-xlsr-araic-400f/pytorch_model.bin
Epoch 99 Train Acc 98.67567443847656% Val Acc 57.10000228881836% Train Loss 0.022295283153653145 Val Loss 2.809966802597046

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:55.70000076293945% Loss:2.773695707321167
CONFUSION MATRIX
[[49 12 16 23]
 [ 1 29 25 45]
 [ 4  2 78 14]
 [ 1  3 31 65]]
CONFUSION MATRIX NORMALISED
[[0.12311558 0.03015075 0.04020101 0.05778894]
 [0.00251256 0.07286432 0.06281407 0.11306533]
 [0.01005025 0.00502513 0.1959799  0.03517588]
 [0.00251256 0.00753769 0.07788945 0.16331658]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.89      0.49      0.63       100
           1       0.63      0.29      0.40       100
           2       0.52      0.80      0.63        98
           3       0.44      0.65      0.53       100

    accuracy                           0.56       398
   macro avg       0.62      0.56      0.55       398
weighted avg       0.62      0.56      0.55       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 16/11/2022 18:24:25
