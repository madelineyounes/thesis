Wed Nov 16 17:27:55 AEDT 2022
2022-11-16 17:27:57.266295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 17:27:57.714352: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-16 17:27:57.874376: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 17:27:59.840549: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:27:59.842214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:27:59.842231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_800f.py
Started: 16/11/2022 17:28:12

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-800f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_800f
train_filename: u_train_800f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_800f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_800f_local/ADI17-xlsr-araic-800f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_800f_local/ADI17-xlsr-araic-800f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Traceback (most recent call last):
  File "run_xlsr_800f.py", line 378, in <module>
    traincustomdata = CustomDataset(
  File "/home/z5208494/thesis/customData.py", line 35, in __init__
    self.data_frame = pd.read_csv(csv_fp, delimiter=',')
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File data/u_train_800f.csv does not exist: 'data/u_train_800f.csv'
Wed Nov 16 17:34:24 AEDT 2022
2022-11-16 17:34:25.137406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 17:34:25.343498: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-16 17:34:25.377866: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 17:34:26.737028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:34:26.738074: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:34:26.738086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_800f.py
Started: 16/11/2022 17:34:38

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-800f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_800f
train_filename: u_train_800f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_800f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_800f_local/ADI17-xlsr-araic-800f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_800f_local/ADI17-xlsr-araic-800f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 5.4302e+00,  5.3288e+00,  5.3900e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.8428e-03,  5.3074e-04,  9.3399e-05,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.3672e-01, -1.4349e-03,  6.5143e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.7180e-01, -2.9627e-01, -1.7557e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.6274e-01, -7.8213e-01, -6.6948e-01,  ..., -2.4079e-01,
         -1.3960e-01, -7.6897e-01],
        [ 1.0246e-01,  1.5084e-01,  1.2428e-01,  ..., -1.5272e-01,
         -1.2474e-01, -9.2958e-02]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 2, 1, 0, 0, 3, 3, 1, 1, 1, 0, 1, 2, 2, 3, 3, 2, 3, 3, 1, 1, 2, 0, 1,
        1, 1, 2, 2, 2, 1, 3, 0, 3, 0, 2, 0, 3, 0, 3, 1])}
Training DataCustom Files: 2884
Training Data Files: 73
Val Data Sample
{'input_values': tensor([[-0.9829, -0.9170, -0.8455,  ...,  0.4252,  0.2786,  0.1605],
        [ 0.3675,  0.3036,  0.2163,  ..., -0.5345, -0.5723, -0.6104],
        [ 0.4566,  0.3284,  0.1787,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0396,  0.0817, -0.1430,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0547, -0.1318, -0.1880,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0249,  0.0167,  0.0096,  ..., -1.1141, -1.0186, -1.2358]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 1, 0, 3, 1, 1, 3, 1, 1, 3, 3, 0, 3, 2, 1, 0, 2, 0, 2, 1, 0, 3, 1, 0,
        2, 1, 3, 2, 0, 2, 1, 3, 0, 2, 3, 0, 2, 0, 1, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'projector.weight', 'classifier.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.7674,  0.3624,  0.1466,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.7907,  1.8049,  1.7620,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0652,  0.1165,  0.0860,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0655,  0.2089,  0.3651,  ...,  0.0000,  0.0000,  0.0000],
        [ 2.9811,  2.6068,  0.7240,  ...,  1.1444,  0.7092,  1.9465],
        [ 0.0859,  0.0620,  0.0274,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 0, 1, 0, 3, 1, 2, 0, 0, 1, 0, 2, 3, 2, 0, 2, 1, 3, 1, 3, 0, 1, 1,
        0, 2, 0, 1, 1, 3, 3, 2, 1, 3, 2, 3, 3, 0, 3, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 0 Train Acc 24.123287200927734% Val Acc 28.399999618530273% Train Loss 0.699431300163269 Val Loss 1.3855420351028442
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 1 Train Acc 25.863014221191406% Val Acc 31.200000762939453% Train Loss 0.6944395303726196 Val Loss 1.382624864578247
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 2 Train Acc 30.328767776489258% Val Acc 30.200000762939453% Train Loss 0.6888648271560669 Val Loss 1.3844451904296875
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 3 Train Acc 33.904109954833984% Val Acc 32.5% Train Loss 0.6823680996894836 Val Loss 1.380709171295166
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 4 Train Acc 39.095890045166016% Val Acc 33.70000076293945% Train Loss 0.6735995411872864 Val Loss 1.3590447902679443
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 5 Train Acc 43.79452133178711% Val Acc 37.900001525878906% Train Loss 0.6448192000389099 Val Loss 1.3584917783737183
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 6 Train Acc 47.575340270996094% Val Acc 39.0% Train Loss 0.5997613072395325 Val Loss 1.332535743713379
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 7 Train Acc 56.23287582397461% Val Acc 44.900001525878906% Train Loss 0.5390635132789612 Val Loss 1.3007625341415405
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 8 Train Acc 62.24657440185547% Val Acc 48.5% Train Loss 0.4796505272388458 Val Loss 1.2331466674804688
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 9 Train Acc 67.82191467285156% Val Acc 48.400001525878906% Train Loss 0.4220964312553406 Val Loss 1.3293148279190063
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 10 Train Acc 73.12328338623047% Val Acc 53.60000228881836% Train Loss 0.3666731119155884 Val Loss 1.3285928964614868
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 11 Train Acc 77.6986312866211% Val Acc 52.5% Train Loss 0.32091280817985535 Val Loss 1.3313578367233276
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 12 Train Acc 81.7397232055664% Val Acc 55.60000228881836% Train Loss 0.25909146666526794 Val Loss 1.397139310836792
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 13 Train Acc 84.68492889404297% Val Acc 64.30000305175781% Train Loss 0.22646096348762512 Val Loss 1.0861022472381592
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 14 Train Acc 87.28767395019531% Val Acc 59.900001525878906% Train Loss 0.18194164335727692 Val Loss 1.390633225440979
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 15 Train Acc 89.54794311523438% Val Acc 65.9000015258789% Train Loss 0.15622705221176147 Val Loss 1.1636196374893188
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 16 Train Acc 91.94520568847656% Val Acc 68.0999984741211% Train Loss 0.12263006716966629 Val Loss 1.2046502828598022
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 17 Train Acc 93.4246597290039% Val Acc 57.0% Train Loss 0.10510216653347015 Val Loss 1.8992961645126343
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 18 Train Acc 93.1780776977539% Val Acc 61.60000228881836% Train Loss 0.09866372495889664 Val Loss 1.5913029909133911
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 19 Train Acc 94.91780853271484% Val Acc 60.400001525878906% Train Loss 0.07679230719804764 Val Loss 1.686345100402832
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 20 Train Acc 95.75342559814453% Val Acc 57.20000076293945% Train Loss 0.0690474733710289 Val Loss 1.7560468912124634
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 21 Train Acc 96.06848907470703% Val Acc 55.60000228881836% Train Loss 0.060927875339984894 Val Loss 1.8444805145263672
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 22 Train Acc 96.4383544921875% Val Acc 65.0999984741211% Train Loss 0.055755987763404846 Val Loss 1.5881937742233276
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 23 Train Acc 96.45205688476562% Val Acc 60.70000076293945% Train Loss 0.052991319447755814 Val Loss 1.8374007940292358
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 24 Train Acc 96.54794311523438% Val Acc 65.5% Train Loss 0.05011897161602974 Val Loss 1.6880463361740112
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 25 Train Acc 97.89041137695312% Val Acc 66.5999984741211% Train Loss 0.03275332227349281 Val Loss 1.7785602807998657
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 26 Train Acc 97.19178009033203% Val Acc 63.400001525878906% Train Loss 0.044496066868305206 Val Loss 1.9030977487564087
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 27 Train Acc 96.95890045166016% Val Acc 66.4000015258789% Train Loss 0.04759073257446289 Val Loss 1.6985071897506714
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 28 Train Acc 98.0% Val Acc 62.10000228881836% Train Loss 0.029554003849625587 Val Loss 1.921327829360962
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 29 Train Acc 97.19178009033203% Val Acc 68.9000015258789% Train Loss 0.046554405242204666 Val Loss 1.7223875522613525
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 30 Train Acc 97.89041137695312% Val Acc 61.60000228881836% Train Loss 0.03662566840648651 Val Loss 1.9879878759384155
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 31 Train Acc 97.876708984375% Val Acc 58.900001525878906% Train Loss 0.03647409752011299 Val Loss 2.3752872943878174
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 32 Train Acc 97.08219146728516% Val Acc 59.20000076293945% Train Loss 0.042418334633111954 Val Loss 2.3293676376342773
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 33 Train Acc 98.10958862304688% Val Acc 59.10000228881836% Train Loss 0.029634756967425346 Val Loss 2.3419041633605957
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 34 Train Acc 98.64383697509766% Val Acc 66.5% Train Loss 0.022863851860165596 Val Loss 1.8617738485336304
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 35 Train Acc 97.49314880371094% Val Acc 60.60000228881836% Train Loss 0.03503858670592308 Val Loss 2.068743944168091
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 36 Train Acc 98.28767395019531% Val Acc 63.400001525878906% Train Loss 0.027674978598952293 Val Loss 2.0761430263519287
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 37 Train Acc 98.35616302490234% Val Acc 67.4000015258789% Train Loss 0.03199153393507004 Val Loss 1.6511286497116089
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 38 Train Acc 98.05479431152344% Val Acc 66.0999984741211% Train Loss 0.029185784980654716 Val Loss 1.6892811059951782
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 39 Train Acc 98.28767395019531% Val Acc 59.10000228881836% Train Loss 0.030890971422195435 Val Loss 2.324228525161743
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 40 Train Acc 98.23287963867188% Val Acc 65.0% Train Loss 0.029688632115721703 Val Loss 1.8083972930908203
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 41 Train Acc 97.71232604980469% Val Acc 61.79999923706055% Train Loss 0.03087943233549595 Val Loss 2.4105584621429443
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 42 Train Acc 97.72602844238281% Val Acc 66.9000015258789% Train Loss 0.03237750753760338 Val Loss 1.8730815649032593
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 43 Train Acc 98.12328338623047% Val Acc 65.30000305175781% Train Loss 0.02938680723309517 Val Loss 2.2155239582061768
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 44 Train Acc 98.34246826171875% Val Acc 65.0% Train Loss 0.031305138021707535 Val Loss 1.8003891706466675
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 45 Train Acc 98.15068054199219% Val Acc 65.4000015258789% Train Loss 0.02861178107559681 Val Loss 1.8124223947525024
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 46 Train Acc 98.49314880371094% Val Acc 63.20000076293945% Train Loss 0.023181099444627762 Val Loss 2.4063022136688232
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 47 Train Acc 98.24657440185547% Val Acc 70.5999984741211% Train Loss 0.027219397947192192 Val Loss 1.7263603210449219
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 48 Train Acc 97.12328338623047% Val Acc 69.70000457763672% Train Loss 0.04444672167301178 Val Loss 1.6992524862289429
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Epoch 49 Train Acc 97.82191467285156% Val Acc 69.20000457763672% Train Loss 0.0330694206058979 Val Loss 1.6157715320587158
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 50 Train Acc 98.68492889404297% Val Acc 57.29999923706055% Train Loss 0.018407179042696953 Val Loss 2.7342469692230225
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 51 Train Acc 98.3013687133789% Val Acc 61.29999923706055% Train Loss 0.02370855212211609 Val Loss 2.1975696086883545
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 52 Train Acc 98.52054595947266% Val Acc 67.30000305175781% Train Loss 0.0264310110360384 Val Loss 1.7671453952789307
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 53 Train Acc 98.36986541748047% Val Acc 65.4000015258789% Train Loss 0.025825001299381256 Val Loss 1.7081425189971924
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 54 Train Acc 97.79451751708984% Val Acc 66.70000457763672% Train Loss 0.03308236226439476 Val Loss 1.8064358234405518
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 55 Train Acc 97.7397232055664% Val Acc 62.10000228881836% Train Loss 0.039188120514154434 Val Loss 2.116060495376587
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 56 Train Acc 98.5616455078125% Val Acc 65.9000015258789% Train Loss 0.023030992597341537 Val Loss 1.9843502044677734
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 57 Train Acc 98.61643981933594% Val Acc 64.5% Train Loss 0.02431238256394863 Val Loss 1.8611888885498047
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 58 Train Acc 99.1780776977539% Val Acc 60.20000076293945% Train Loss 0.013610088266432285 Val Loss 2.452906370162964
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 59 Train Acc 98.76712036132812% Val Acc 67.0% Train Loss 0.024546651169657707 Val Loss 1.8500515222549438
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 60 Train Acc 98.90410614013672% Val Acc 63.20000076293945% Train Loss 0.018948739394545555 Val Loss 2.0350587368011475
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 61 Train Acc 98.68492889404297% Val Acc 61.29999923706055% Train Loss 0.023741314187645912 Val Loss 2.2110793590545654
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 62 Train Acc 98.78082275390625% Val Acc 57.10000228881836% Train Loss 0.018896453082561493 Val Loss 2.8392653465270996
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 63 Train Acc 98.19178009033203% Val Acc 68.30000305175781% Train Loss 0.02988344617187977 Val Loss 1.7230615615844727
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 64 Train Acc 98.32876586914062% Val Acc 70.30000305175781% Train Loss 0.02616603672504425 Val Loss 1.7259219884872437
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 65 Train Acc 98.04109191894531% Val Acc 68.9000015258789% Train Loss 0.036994222551584244 Val Loss 1.759194254875183
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 66 Train Acc 99.4383544921875% Val Acc 65.0% Train Loss 0.011198407970368862 Val Loss 2.1392600536346436
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 67 Train Acc 97.6986312866211% Val Acc 61.5% Train Loss 0.037387099117040634 Val Loss 1.8678041696548462
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 68 Train Acc 98.65753173828125% Val Acc 59.900001525878906% Train Loss 0.022369399666786194 Val Loss 2.731931447982788
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 69 Train Acc 98.24657440185547% Val Acc 56.20000076293945% Train Loss 0.029537219554185867 Val Loss 2.4850029945373535
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 70 Train Acc 98.97260284423828% Val Acc 70.0% Train Loss 0.0177741851657629 Val Loss 1.8526480197906494
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 71 Train Acc 98.4246597290039% Val Acc 62.10000228881836% Train Loss 0.024615874513983727 Val Loss 1.9567047357559204
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 72 Train Acc 98.08219146728516% Val Acc 53.900001525878906% Train Loss 0.03011152148246765 Val Loss 2.4993882179260254
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 73 Train Acc 97.50685119628906% Val Acc 63.0% Train Loss 0.03591727465391159 Val Loss 1.7517578601837158
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 74 Train Acc 99.1780776977539% Val Acc 63.29999923706055% Train Loss 0.015023479238152504 Val Loss 2.417587995529175
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 75 Train Acc 98.60273742675781% Val Acc 60.60000228881836% Train Loss 0.02218998782336712 Val Loss 2.3879191875457764
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 76 Train Acc 98.58904266357422% Val Acc 63.20000076293945% Train Loss 0.025255151093006134 Val Loss 2.380711078643799
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 77 Train Acc 98.36986541748047% Val Acc 59.60000228881836% Train Loss 0.025294294580817223 Val Loss 2.486494779586792
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 78 Train Acc 98.98629760742188% Val Acc 62.60000228881836% Train Loss 0.01651586964726448 Val Loss 2.5274837017059326
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 79 Train Acc 97.60273742675781% Val Acc 68.0% Train Loss 0.04069659858942032 Val Loss 1.689979910850525
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 80 Train Acc 98.46575164794922% Val Acc 65.5% Train Loss 0.02572888322174549 Val Loss 2.041313648223877
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 81 Train Acc 97.04109191894531% Val Acc 67.70000457763672% Train Loss 0.043642278760671616 Val Loss 1.6236076354980469
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 82 Train Acc 98.90410614013672% Val Acc 69.30000305175781% Train Loss 0.017390158027410507 Val Loss 1.7909809350967407
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 83 Train Acc 98.6986312866211% Val Acc 64.4000015258789% Train Loss 0.020671812817454338 Val Loss 2.035780429840088
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 84 Train Acc 98.91780853271484% Val Acc 66.0999984741211% Train Loss 0.019767070189118385 Val Loss 2.131608724594116
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 85 Train Acc 99.16438293457031% Val Acc 65.30000305175781% Train Loss 0.017669325694441795 Val Loss 2.1125237941741943
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 86 Train Acc 98.02739715576172% Val Acc 57.79999923706055% Train Loss 0.03286760672926903 Val Loss 2.35785174369812
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 87 Train Acc 99.27397155761719% Val Acc 65.20000457763672% Train Loss 0.0118731539696455 Val Loss 1.8987406492233276
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 88 Train Acc 98.49314880371094% Val Acc 60.5% Train Loss 0.026776790618896484 Val Loss 2.06329607963562
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 89 Train Acc 97.21917724609375% Val Acc 56.79999923706055% Train Loss 0.049213044345378876 Val Loss 2.0195088386535645
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 90 Train Acc 98.1369857788086% Val Acc 57.70000076293945% Train Loss 0.02549484558403492 Val Loss 2.9663312435150146
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 91 Train Acc 98.60273742675781% Val Acc 59.5% Train Loss 0.02615276165306568 Val Loss 2.395012617111206
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 92 Train Acc 99.20548248291016% Val Acc 62.70000076293945% Train Loss 0.01488290261477232 Val Loss 2.208779811859131
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 93 Train Acc 98.68492889404297% Val Acc 67.5999984741211% Train Loss 0.026208261027932167 Val Loss 1.7152118682861328
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 94 Train Acc 97.93150329589844% Val Acc 57.60000228881836% Train Loss 0.031284842640161514 Val Loss 2.499943494796753
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 95 Train Acc 98.12328338623047% Val Acc 63.10000228881836% Train Loss 0.026937125250697136 Val Loss 2.3243227005004883
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 96 Train Acc 97.80821990966797% Val Acc 59.400001525878906% Train Loss 0.03636251017451286 Val Loss 2.317222833633423
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 97 Train Acc 98.8630142211914% Val Acc 66.0% Train Loss 0.017893239855766296 Val Loss 2.055082082748413
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 98 Train Acc 98.24657440185547% Val Acc 64.30000305175781% Train Loss 0.03207920491695404 Val Loss 2.1736037731170654
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Configuration saved in /srv/scratch/z5208494/output/u_train_800f_local/ADI17-xlsr-araic-800f/config.json
Model weights saved in /srv/scratch/z5208494/output/u_train_800f_local/ADI17-xlsr-araic-800f/pytorch_model.bin
Epoch 99 Train Acc 97.15068054199219% Val Acc 63.0% Train Loss 0.05494499206542969 Val Loss 1.553186297416687

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:65.4000015258789% Loss:1.2676118612289429
CONFUSION MATRIX
[[74  4 14  8]
 [13 45 17 25]
 [16  3 63 16]
 [ 4  5 13 78]]
CONFUSION MATRIX NORMALISED
[[0.18592965 0.01005025 0.03517588 0.0201005 ]
 [0.03266332 0.11306533 0.04271357 0.06281407]
 [0.04020101 0.00753769 0.15829146 0.04020101]
 [0.01005025 0.01256281 0.03266332 0.1959799 ]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.69      0.74      0.71       100
           1       0.79      0.45      0.57       100
           2       0.59      0.64      0.61        98
           3       0.61      0.78      0.69       100

    accuracy                           0.65       398
   macro avg       0.67      0.65      0.65       398
weighted avg       0.67      0.65      0.65       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 16/11/2022 19:06:17
