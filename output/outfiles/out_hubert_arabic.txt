Wed Nov 2 18:38:16 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_hubert_arabic.py
Started: 02/11/2022 18:38:28

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-hubert-arabic
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-hubert-arabic
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-hubert-arabic_finetuned_results.csv
--> pretrained_mod: asafaya/hubert-large-arabic-ft

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Downloading:   0%|          | 0.00/225 [00:00<?, ?B/s]Downloading: 100%|██████████| 225/225 [00:00<00:00, 149kB/s]Check data has been processed correctly... 
Train Data Sample

{'input_values': tensor([[-1.0988, -0.7967, -0.5071,  ...,  0.0000,  0.0000,  0.0000],
        [-1.2498, -2.1260, -2.6081,  ...,  0.0000,  0.0000,  0.0000],
        [-0.6742, -0.6028, -0.6243,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0080,  0.0080,  0.0080,  ..., -0.3758, -0.2798, -0.1947],
        [-1.8539, -1.6943, -1.3829,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3734, -0.2141,  0.1811,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 0, 3, 2, 0, 2, 2, 2, 1, 0, 2, 0, 3, 2, 2, 1, 2, 2, 3, 3, 2, 2, 2, 2,
        2, 3, 2, 2, 0, 1, 2, 2, 3, 2, 3, 0, 1, 0, 2, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.1110, -0.7798, -0.7432,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0207, -0.0218, -0.0224,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0577,  0.0723,  0.0820,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.4901,  0.3973,  0.1660,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0574,  0.0137,  0.0562,  ...,  0.0000,  0.0000,  0.0000],
        [ 4.7518,  3.7885,  2.7126,  ...,  0.5291,  0.6804,  0.8088]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 3, 1, 3, 1, 1, 1, 0, 3, 1, 3, 1, 0, 1, 3, 2, 3, 0, 3, 1, 3, 0, 3, 0,
        1, 0, 2, 2, 0, 3, 2, 1, 1, 3, 0, 0, 2, 0, 0, 3])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
{'input_values': tensor([[ 1.2793,  1.2673,  1.3023,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.7782,  0.7331,  0.6194,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1939,  0.1894,  0.1801,  ..., -1.3145, -1.5837, -1.8179],
        ...,
        [-0.1916, -0.2201, -0.1819,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1198, -0.1511,  0.0263,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.7907,  1.8049,  1.7620,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 0, 2, 3, 3, 2, 1, 0, 0, 1, 3, 1, 2, 2, 1, 3, 0, 3, 3, 3, 1, 1, 0, 2,
        2, 3, 0, 0, 1, 1, 3, 1, 1, 3, 1, 0, 1, 2, 2, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
Downloading:   0%|          | 0.00/1.78k [00:00<?, ?B/s]Downloading: 100%|██████████| 1.78k/1.78k [00:00<00:00, 880kB/s]
You are using a model of type hubert to instantiate a model of type wav2vec2. This is not supported for all configurations of models and can yield errors.
--> Loading pre-trained checkpoint...
Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2007, in from_pretrained
    resolved_archive_file = cached_path(
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/utils/hub.py", line 284, in cached_path
    output_path = get_from_cache(
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/utils/hub.py", line 495, in get_from_cache
    _raise_for_status(r)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/utils/hub.py", line 411, in _raise_for_status
    raise EntryNotFoundError(f"404 Client Error: Entry Not Found for url: {response.url}")
transformers.utils.hub.EntryNotFoundError: 404 Client Error: Entry Not Found for url: https://huggingface.co/asafaya/hubert-large-arabic-ft/resolve/main/pytorch_model.bin

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2041, in from_pretrained
    resolved_archive_file = cached_path(
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/utils/hub.py", line 284, in cached_path
    output_path = get_from_cache(
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/utils/hub.py", line 495, in get_from_cache
    _raise_for_status(r)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/utils/hub.py", line 411, in _raise_for_status
    raise EntryNotFoundError(f"404 Client Error: Entry Not Found for url: {response.url}")
transformers.utils.hub.EntryNotFoundError: 404 Client Error: Entry Not Found for url: https://huggingface.co/asafaya/hubert-large-arabic-ft/resolve/main/pytorch_model.bin.index.json

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run_hubert_arabic.py", line 462, in <module>
    model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2074, in from_pretrained
    raise EnvironmentError(
OSError: asafaya/hubert-large-arabic-ft does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
