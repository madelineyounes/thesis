Wed Nov 2 14:13:59 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_medium.py
Started: 02/11/2022 14:14:11

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-medium
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_10s
train_filename: u_train_10s
validation_filename: dev_u_10s
evaluation_filename: test_u_10s
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_10s.csv
--> data_test_fp: data/dev_u_10s.csv
--> data_test_fp: data/test_u_10s.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_10s_local/ADI17-xlsr-medium
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_10s_local/ADI17-xlsr-medium_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Traceback (most recent call last):
  File "run_xlsr_medium.py", line 378, in <module>
    traincustomdata = CustomDataset(
  File "/home/z5208494/thesis/customData.py", line 35, in __init__
    self.data_frame = pd.read_csv(csv_fp, delimiter=',')
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File data/u_train_10s.csv does not exist: 'data/u_train_10s.csv'
Wed Nov 2 18:20:53 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_medium.py
Started: 02/11/2022 18:21:05

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-medium
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: train_u_10s
train_filename: train_u_10s
validation_filename: dev_u_10s
evaluation_filename: test_u_10s
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/train_u_10s.csv
--> data_test_fp: data/dev_u_10s.csv
--> data_test_fp: data/test_u_10s.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/train_u_10s_local/ADI17-xlsr-medium
--> finetuned_results_fp: /srv/scratch/z5208494/output/train_u_10s_local/ADI17-xlsr-medium_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 1.9186,  1.3890,  1.0741,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3785,  0.5484, -0.6259,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3139, -1.4624, -1.4486,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-1.0222, -1.1632, -1.3629,  ...,  0.0000,  0.0000,  0.0000],
        [-1.0691, -1.1917, -1.0750,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0567,  0.0662, -0.0770,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 0, 0, 3, 2, 0, 1, 1, 0, 3, 0, 3, 1, 2, 3, 1, 1, 2, 3, 1, 3, 2, 1,
        2, 3, 0, 3, 3, 2, 3, 1, 3, 3, 0, 1, 3, 0, 3, 3])}
Training DataCustom Files: 1794
Training Data Files: 45
Val Data Sample
{'input_values': tensor([[-0.0121,  0.1162, -0.2201,  ...,  0.0000,  0.0000,  0.0000],
        [-1.2970, -0.5851, -0.7961,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0700,  0.3970,  0.2816,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.1706, -0.5543, -1.0730,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0654, -0.0546, -0.0354,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0150,  0.0116,  0.0077,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 0, 3, 0, 1, 2, 3, 0, 2, 3, 3, 0, 3, 3, 1, 2, 2, 2, 1, 1, 3, 3, 2, 2,
        3, 1, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 0, 2, 2, 3])}
Test CustomData Files: 1724
Test Data Files: 44
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.weight', 'projector.bias', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-1.4110e+00, -2.0352e+00, -2.5594e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0295e+00, -1.0219e+00, -1.0723e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6944e-02, -1.7686e-02, -1.4719e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-2.2733e-01, -2.6968e-01, -1.3321e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.7458e+00,  1.3302e+00,  8.3034e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1476e-02,  2.1211e-03, -4.7914e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 2, 0, 0, 0, 0, 2, 2, 1, 3, 0, 2, 2, 2, 2, 1, 0, 0, 3, 3, 3, 1, 3,
        0, 0, 3, 0, 2, 1, 1, 3, 2, 1, 1, 2, 2, 1, 0, 3])}
Test CustomData Files: 1992
Test Data Files: 50
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Epoch 0 Train Acc 27.0% Val Acc 25.299999237060547% Train Loss 0.6929642558097839 Val Loss 1.2182986736297607
Trainable Parameters : 264452
Epoch 1 Train Acc 27.600000381469727% Val Acc 26.099998474121094% Train Loss 0.6926838755607605 Val Loss 1.2182140350341797
Trainable Parameters : 264452
Epoch 2 Train Acc 27.822223663330078% Val Acc 26.099998474121094% Train Loss 0.691947877407074 Val Loss 1.2203867435455322
Trainable Parameters : 264452
Epoch 3 Train Acc 28.80000114440918% Val Acc 25.03999900817871% Train Loss 0.6912184357643127 Val Loss 1.225321888923645
Trainable Parameters : 264452
Epoch 4 Train Acc 29.133333206176758% Val Acc 24.31999969482422% Train Loss 0.6899312138557434 Val Loss 1.2284009456634521
Trainable Parameters : 264452
Epoch 5 Train Acc 28.91111183166504% Val Acc 24.65999984741211% Train Loss 0.68951016664505 Val Loss 1.226541519165039
Trainable Parameters : 264452
Epoch 6 Train Acc 29.26666831970215% Val Acc 21.35999870300293% Train Loss 0.6884371042251587 Val Loss 1.231682300567627
Trainable Parameters : 264452
Epoch 7 Train Acc 31.155555725097656% Val Acc 22.69999885559082% Train Loss 0.6874706745147705 Val Loss 1.2308381795883179
Trainable Parameters : 264452
Epoch 8 Train Acc 30.0% Val Acc 21.479999542236328% Train Loss 0.6866053342819214 Val Loss 1.2304461002349854
Trainable Parameters : 264452
Epoch 9 Train Acc 33.22222137451172% Val Acc 19.619998931884766% Train Loss 0.6855586171150208 Val Loss 1.2284897565841675
Trainable Parameters : 264452
Epoch 10 Train Acc 33.77777862548828% Val Acc 19.100000381469727% Train Loss 0.6842633485794067 Val Loss 1.2324498891830444
Trainable Parameters : 264452
Epoch 11 Train Acc 32.333335876464844% Val Acc 19.959999084472656% Train Loss 0.6832787394523621 Val Loss 1.2281392812728882
Trainable Parameters : 264452
Epoch 12 Train Acc 33.733333587646484% Val Acc 24.8799991607666% Train Loss 0.682004988193512 Val Loss 1.2277098894119263
Trainable Parameters : 264452
Epoch 13 Train Acc 34.755558013916016% Val Acc 23.760000228881836% Train Loss 0.6806867718696594 Val Loss 1.2287756204605103
Trainable Parameters : 264452
Epoch 14 Train Acc 35.733333587646484% Val Acc 21.719999313354492% Train Loss 0.6793308258056641 Val Loss 1.225804090499878
Trainable Parameters : 264452
Epoch 15 Train Acc 36.06666564941406% Val Acc 23.600000381469727% Train Loss 0.6778545379638672 Val Loss 1.226682424545288
Trainable Parameters : 264452
Epoch 16 Train Acc 37.0444450378418% Val Acc 21.559999465942383% Train Loss 0.6764248013496399 Val Loss 1.2362868785858154
Trainable Parameters : 264452
Epoch 17 Train Acc 37.511112213134766% Val Acc 23.18000030517578% Train Loss 0.6747457385063171 Val Loss 1.2296990156173706
Trainable Parameters : 264452
Epoch 18 Train Acc 38.266666412353516% Val Acc 21.8799991607666% Train Loss 0.6731173396110535 Val Loss 1.2255829572677612
Trainable Parameters : 264452
Epoch 19 Train Acc 36.377777099609375% Val Acc 22.8799991607666% Train Loss 0.6722874045372009 Val Loss 1.227028727531433
Trainable Parameters : 264452
Epoch 20 Train Acc 38.02222442626953% Val Acc 20.479999542236328% Train Loss 0.6688653230667114 Val Loss 1.2334167957305908
Trainable Parameters : 264452
Epoch 21 Train Acc 38.11111068725586% Val Acc 23.299999237060547% Train Loss 0.6680589914321899 Val Loss 1.2247309684753418
Trainable Parameters : 264452
Epoch 22 Train Acc 38.91111373901367% Val Acc 22.079999923706055% Train Loss 0.6660081148147583 Val Loss 1.2243167161941528
Trainable Parameters : 264452
Epoch 23 Train Acc 38.866668701171875% Val Acc 23.0% Train Loss 0.6654841303825378 Val Loss 1.2259163856506348
Trainable Parameters : 264452
Epoch 24 Train Acc 38.88888931274414% Val Acc 20.020000457763672% Train Loss 0.6624022722244263 Val Loss 1.23593270778656
Trainable Parameters : 264452
Epoch 25 Train Acc 39.9555549621582% Val Acc 23.239999771118164% Train Loss 0.6622752547264099 Val Loss 1.225353479385376
Trainable Parameters : 264452
Epoch 26 Train Acc 39.0% Val Acc 22.239999771118164% Train Loss 0.6582109332084656 Val Loss 1.2285195589065552
Trainable Parameters : 264452
Epoch 27 Train Acc 39.088890075683594% Val Acc 22.51999855041504% Train Loss 0.6577768921852112 Val Loss 1.2219165563583374
Trainable Parameters : 264452
Epoch 28 Train Acc 40.79999923706055% Val Acc 22.3799991607666% Train Loss 0.6549883484840393 Val Loss 1.2263953685760498
Trainable Parameters : 264452
Epoch 29 Train Acc 40.9555549621582% Val Acc 19.600000381469727% Train Loss 0.6514795422554016 Val Loss 1.252791166305542
Trainable Parameters : 264452
Epoch 30 Train Acc 41.155555725097656% Val Acc 23.5% Train Loss 0.6512225270271301 Val Loss 1.2209763526916504
Trainable Parameters : 264452
Epoch 31 Train Acc 42.755558013916016% Val Acc 22.18000030517578% Train Loss 0.6474303603172302 Val Loss 1.2430511713027954
Trainable Parameters : 264452
Epoch 32 Train Acc 42.35555648803711% Val Acc 26.19999885559082% Train Loss 0.6440533399581909 Val Loss 1.2130188941955566
Trainable Parameters : 264452
Epoch 33 Train Acc 42.400001525878906% Val Acc 24.84000015258789% Train Loss 0.6442435383796692 Val Loss 1.2172815799713135
Trainable Parameters : 264452
Epoch 34 Train Acc 42.622222900390625% Val Acc 25.31999969482422% Train Loss 0.6409007906913757 Val Loss 1.2052456140518188
Trainable Parameters : 264452
Epoch 35 Train Acc 43.17778015136719% Val Acc 26.279998779296875% Train Loss 0.638903796672821 Val Loss 1.2149412631988525
Trainable Parameters : 264452
Epoch 36 Train Acc 43.9555549621582% Val Acc 29.15999984741211% Train Loss 0.636932373046875 Val Loss 1.1966230869293213
Trainable Parameters : 264452
Epoch 37 Train Acc 43.66666793823242% Val Acc 24.8799991607666% Train Loss 0.6346743702888489 Val Loss 1.2262004613876343
Trainable Parameters : 264452
Epoch 38 Train Acc 44.24444580078125% Val Acc 24.8799991607666% Train Loss 0.6333383321762085 Val Loss 1.227033257484436
Trainable Parameters : 264452
Epoch 39 Train Acc 44.377777099609375% Val Acc 31.67999839782715% Train Loss 0.6285238862037659 Val Loss 1.174879550933838
Trainable Parameters : 264452
Epoch 40 Train Acc 44.46666717529297% Val Acc 27.420000076293945% Train Loss 0.6228905916213989 Val Loss 1.20651376247406
Trainable Parameters : 264452
Epoch 41 Train Acc 45.53333282470703% Val Acc 28.84000015258789% Train Loss 0.6230595707893372 Val Loss 1.2020196914672852
Trainable Parameters : 264452
Epoch 42 Train Acc 43.68888854980469% Val Acc 27.559999465942383% Train Loss 0.6222953796386719 Val Loss 1.2155704498291016
Trainable Parameters : 264452
Epoch 43 Train Acc 45.06666946411133% Val Acc 33.439998626708984% Train Loss 0.6176905035972595 Val Loss 1.165201187133789
Trainable Parameters : 264452
Epoch 44 Train Acc 46.28889083862305% Val Acc 27.920000076293945% Train Loss 0.6133123636245728 Val Loss 1.2028391361236572
Trainable Parameters : 264452
Epoch 45 Train Acc 47.4888916015625% Val Acc 29.779998779296875% Train Loss 0.6119539737701416 Val Loss 1.196929693222046
Trainable Parameters : 264452
Epoch 46 Train Acc 47.46666717529297% Val Acc 33.63999938964844% Train Loss 0.6108087301254272 Val Loss 1.165536642074585
Trainable Parameters : 264452
Epoch 47 Train Acc 46.9555549621582% Val Acc 31.559999465942383% Train Loss 0.6116139888763428 Val Loss 1.1787278652191162
Trainable Parameters : 264452
Epoch 48 Train Acc 49.0% Val Acc 28.399999618530273% Train Loss 0.6042855978012085 Val Loss 1.2306228876113892
Trainable Parameters : 264452
Epoch 49 Train Acc 46.155555725097656% Val Acc 34.779998779296875% Train Loss 0.6036208868026733 Val Loss 1.14166259765625
Trainable Parameters : 264452
Epoch 50 Train Acc 48.13333511352539% Val Acc 29.17999839782715% Train Loss 0.6014458537101746 Val Loss 1.2097326517105103
Trainable Parameters : 264452
Epoch 51 Train Acc 46.733333587646484% Val Acc 35.15999984741211% Train Loss 0.6025101542472839 Val Loss 1.1459687948226929
Trainable Parameters : 264452
Epoch 52 Train Acc 48.42222213745117% Val Acc 37.71999740600586% Train Loss 0.5960690379142761 Val Loss 1.1320329904556274
Trainable Parameters : 264452
Epoch 53 Train Acc 50.0444450378418% Val Acc 34.21999740600586% Train Loss 0.5942954421043396 Val Loss 1.165429949760437
Trainable Parameters : 264452
Epoch 54 Train Acc 48.0444450378418% Val Acc 35.86000061035156% Train Loss 0.5923380255699158 Val Loss 1.1397703886032104
Trainable Parameters : 264452
Epoch 55 Train Acc 47.35555648803711% Val Acc 34.20000076293945% Train Loss 0.5952308177947998 Val Loss 1.1536643505096436
Trainable Parameters : 264452
Epoch 56 Train Acc 48.91111373901367% Val Acc 35.040000915527344% Train Loss 0.5853729248046875 Val Loss 1.1446794271469116
Trainable Parameters : 264452
Epoch 57 Train Acc 48.20000076293945% Val Acc 32.20000076293945% Train Loss 0.5843592882156372 Val Loss 1.1823371648788452
Trainable Parameters : 264452
Epoch 58 Train Acc 48.511112213134766% Val Acc 31.439998626708984% Train Loss 0.5854721069335938 Val Loss 1.2080190181732178
Trainable Parameters : 264452
Epoch 59 Train Acc 49.31111145019531% Val Acc 33.880001068115234% Train Loss 0.5848925709724426 Val Loss 1.1766769886016846
Trainable Parameters : 264452
Epoch 60 Train Acc 50.0% Val Acc 37.87999725341797% Train Loss 0.5798053741455078 Val Loss 1.1129124164581299
Trainable Parameters : 264452
Epoch 61 Train Acc 50.28889083862305% Val Acc 32.5% Train Loss 0.5779125094413757 Val Loss 1.1822248697280884
Trainable Parameters : 264452
Epoch 62 Train Acc 50.644447326660156% Val Acc 40.71999740600586% Train Loss 0.575092613697052 Val Loss 1.0858904123306274
Trainable Parameters : 264452
Epoch 63 Train Acc 50.57777786254883% Val Acc 37.71999740600586% Train Loss 0.5735570192337036 Val Loss 1.1158925294876099
Trainable Parameters : 264452
Epoch 64 Train Acc 51.733333587646484% Val Acc 32.599998474121094% Train Loss 0.5688822269439697 Val Loss 1.20014226436615
Trainable Parameters : 264452
Epoch 65 Train Acc 50.977779388427734% Val Acc 39.07999801635742% Train Loss 0.5675750970840454 Val Loss 1.0903732776641846
Trainable Parameters : 264452
Epoch 66 Train Acc 50.155555725097656% Val Acc 35.18000030517578% Train Loss 0.5675032138824463 Val Loss 1.1585835218429565
Trainable Parameters : 264452
Epoch 67 Train Acc 51.333335876464844% Val Acc 37.91999816894531% Train Loss 0.5615285038948059 Val Loss 1.1070079803466797
Trainable Parameters : 264452
Epoch 68 Train Acc 52.57777786254883% Val Acc 34.89999771118164% Train Loss 0.5612841248512268 Val Loss 1.1694128513336182
Trainable Parameters : 264452
Epoch 69 Train Acc 52.55555725097656% Val Acc 38.73999786376953% Train Loss 0.5555696487426758 Val Loss 1.107534646987915
Trainable Parameters : 264452
Epoch 70 Train Acc 52.222225189208984% Val Acc 40.439998626708984% Train Loss 0.5562828779220581 Val Loss 1.0980314016342163
Trainable Parameters : 264452
Epoch 71 Train Acc 53.977779388427734% Val Acc 33.84000015258789% Train Loss 0.5539318919181824 Val Loss 1.1651400327682495
Trainable Parameters : 264452
Epoch 72 Train Acc 51.60000228881836% Val Acc 39.89999771118164% Train Loss 0.5575442314147949 Val Loss 1.08384370803833
Trainable Parameters : 264452
Epoch 73 Train Acc 52.266666412353516% Val Acc 34.57999801635742% Train Loss 0.5542801022529602 Val Loss 1.163641333580017
Trainable Parameters : 264452
Epoch 74 Train Acc 52.91111373901367% Val Acc 40.47999954223633% Train Loss 0.5491940379142761 Val Loss 1.0777723789215088
Trainable Parameters : 264452
Epoch 75 Train Acc 52.4888916015625% Val Acc 41.05999755859375% Train Loss 0.5560466051101685 Val Loss 1.0674431324005127
Trainable Parameters : 264452
Epoch 76 Train Acc 55.20000076293945% Val Acc 39.84000015258789% Train Loss 0.5459940433502197 Val Loss 1.0977184772491455
Trainable Parameters : 264452
Epoch 77 Train Acc 54.17778015136719% Val Acc 40.47999954223633% Train Loss 0.543373703956604 Val Loss 1.083292007446289
Trainable Parameters : 264452
Epoch 78 Train Acc 54.46666717529297% Val Acc 37.5% Train Loss 0.5440961718559265 Val Loss 1.132137656211853
Trainable Parameters : 264452
Epoch 79 Train Acc 54.57777786254883% Val Acc 41.39999771118164% Train Loss 0.5497234463691711 Val Loss 1.0552669763565063
Trainable Parameters : 264452
Epoch 80 Train Acc 54.222225189208984% Val Acc 38.47999954223633% Train Loss 0.5426878929138184 Val Loss 1.1400667428970337
Trainable Parameters : 264452
Epoch 81 Train Acc 54.46666717529297% Val Acc 42.89999771118164% Train Loss 0.5358580946922302 Val Loss 1.0416830778121948
Trainable Parameters : 264452
Epoch 82 Train Acc 54.755558013916016% Val Acc 41.89999771118164% Train Loss 0.5327426791191101 Val Loss 1.062660574913025
Trainable Parameters : 264452
Epoch 83 Train Acc 55.42222213745117% Val Acc 40.619998931884766% Train Loss 0.5315894484519958 Val Loss 1.0807210206985474
Trainable Parameters : 264452
Epoch 84 Train Acc 54.80000305175781% Val Acc 41.47999954223633% Train Loss 0.5280991792678833 Val Loss 1.0573680400848389
Trainable Parameters : 264452
Epoch 85 Train Acc 53.755558013916016% Val Acc 35.71999740600586% Train Loss 0.5359254479408264 Val Loss 1.1978729963302612
Trainable Parameters : 264452
Epoch 86 Train Acc 57.60000228881836% Val Acc 35.380001068115234% Train Loss 0.5253077745437622 Val Loss 1.2053403854370117
Trainable Parameters : 264452
Epoch 87 Train Acc 56.06666946411133% Val Acc 41.91999816894531% Train Loss 0.5299584269523621 Val Loss 1.0808660984039307
Trainable Parameters : 264452
Epoch 88 Train Acc 56.4444465637207% Val Acc 42.73999786376953% Train Loss 0.5198749303817749 Val Loss 1.0542957782745361
Trainable Parameters : 264452
Epoch 89 Train Acc 55.266666412353516% Val Acc 41.84000015258789% Train Loss 0.529703676700592 Val Loss 1.0566813945770264
Trainable Parameters : 264452
Epoch 90 Train Acc 54.333335876464844% Val Acc 41.57999801635742% Train Loss 0.5298191905021667 Val Loss 1.058738350868225
Trainable Parameters : 264452
Epoch 91 Train Acc 56.06666946411133% Val Acc 43.099998474121094% Train Loss 0.5195237398147583 Val Loss 1.0643161535263062
Trainable Parameters : 264452
Epoch 92 Train Acc 56.4888916015625% Val Acc 42.39999771118164% Train Loss 0.5138491988182068 Val Loss 1.0364712476730347
Trainable Parameters : 264452
Epoch 93 Train Acc 57.57777786254883% Val Acc 41.87999725341797% Train Loss 0.5104654431343079 Val Loss 1.0574939250946045
Trainable Parameters : 264452
Epoch 94 Train Acc 57.644447326660156% Val Acc 43.599998474121094% Train Loss 0.5110338926315308 Val Loss 1.0277862548828125
Trainable Parameters : 264452
Epoch 95 Train Acc 56.4888916015625% Val Acc 41.07999801635742% Train Loss 0.5134389400482178 Val Loss 1.1269282102584839
Trainable Parameters : 264452
Epoch 96 Train Acc 57.333335876464844% Val Acc 44.68000030517578% Train Loss 0.5104769468307495 Val Loss 1.0165300369262695
Trainable Parameters : 264452
Epoch 97 Train Acc 57.511112213134766% Val Acc 41.5% Train Loss 0.5110873579978943 Val Loss 1.0916255712509155
Trainable Parameters : 264452
Epoch 98 Train Acc 58.06666946411133% Val Acc 44.2599983215332% Train Loss 0.5120105743408203 Val Loss 1.0090751647949219
Trainable Parameters : 264452
Epoch 99 Train Acc 58.24444580078125% Val Acc 42.79999923706055% Train Loss 0.5050628185272217 Val Loss 1.0792641639709473

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:52.2599983215332% Loss:1.1352070569992065
CONFUSION MATRIX
[[290 134  44  32]
 [ 58 376  41  20]
 [ 90 146 221  40]
 [ 74 199  75 152]]
CONFUSION MATRIX NORMALISED
[[0.14558233 0.06726908 0.02208835 0.01606426]
 [0.02911647 0.18875502 0.02058233 0.01004016]
 [0.04518072 0.07329317 0.11094378 0.02008032]
 [0.03714859 0.0998996  0.0376506  0.07630522]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.57      0.58      0.57       500
           1       0.44      0.76      0.56       495
           2       0.58      0.44      0.50       497
           3       0.62      0.30      0.41       500

    accuracy                           0.52      1992
   macro avg       0.55      0.52      0.51      1992
weighted avg       0.55      0.52      0.51      1992


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 02/11/2022 19:34:09
