Wed Nov 2 22:13:54 AEDT 2022
2022-11-02 22:13:55.105538: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-02 22:13:55.313316: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-02 22:13:55.348415: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-02 22:13:56.599751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-02 22:13:56.599833: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-02 22:13:56.599842: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_dnn_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_dnn_downstream.py
Started: 02/11/2022 22:14:07

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-dnn
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-dnn
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-dnn_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.7259, -0.6973, -0.6661,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0573, -0.0418, -0.0261,  ..., -0.2451,  0.0706,  0.1648],
        [-0.0281, -0.0540, -0.0576,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.6754,  0.4766,  0.0861,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1146,  0.0947,  0.0525,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0706,  0.0627, -0.0708,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 2, 3, 2, 2, 2, 0, 3, 0, 2, 2, 2, 2, 3, 2, 0, 3, 2, 0, 2, 3, 3, 2,
        1, 2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 0])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.0241, -0.0089, -0.0665,  ..., -0.1457,  0.0797,  0.3071],
        [ 0.1866, -0.2453, -0.2470,  ...,  0.1692,  0.1082, -0.0073],
        [ 1.7705,  1.3181,  0.6450,  ...,  1.8724,  1.7984,  1.7190],
        ...,
        [ 0.2647,  0.3142,  0.3216,  ..., -0.1509, -0.1668, -0.1964],
        [-0.3601, -0.3731, -0.3780,  ...,  0.0000,  0.0000,  0.0000],
        [-0.7433, -0.9230, -1.1930,  ...,  0.2271,  0.2363,  0.0492]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([2, 3, 3, 1, 3, 0, 3, 2, 1, 3, 1, 2, 1, 1, 3, 1, 2, 1, 3, 3, 0, 2, 1, 0,
        0, 1, 3, 1, 1, 1, 2, 2, 1, 1, 2, 0, 2, 2, 3, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.2601,  0.8723,  1.0315,  ..., -0.6052, -0.5835, -0.0195],
        [ 1.0006,  0.3103, -0.6643,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0290,  0.0415,  0.0597,  ...,  1.0335,  1.4112,  0.9908],
        ...,
        [ 3.1451,  2.9490,  2.6545,  ...,  0.1215,  0.0749,  0.0308],
        [-3.5669, -2.5649, -1.3583,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8414, -1.3162, -1.0439,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 2, 1, 2, 2, 0, 2, 2, 1, 0, 1, 1, 3, 0, 2, 2, 3, 1, 2, 0, 0, 0, 2, 2,
        2, 3, 2, 2, 1, 2, 1, 0, 1, 1, 0, 1, 2, 1, 2, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 334440
Epoch 0 Train Acc 26.958173751831055% Val Acc 36.29999923706055% Train Loss 0.6836948394775391 Val Loss 1.3744558095932007
Trainable Parameters : 334440
Epoch 1 Train Acc 43.832698822021484% Val Acc 27.100000381469727% Train Loss 0.6388815641403198 Val Loss 1.3794797658920288
Trainable Parameters : 334440
Epoch 2 Train Acc 50.577945709228516% Val Acc 41.79999923706055% Train Loss 0.5806906223297119 Val Loss 1.2909481525421143
Trainable Parameters : 334440
Epoch 3 Train Acc 59.3079833984375% Val Acc 41.70000076293945% Train Loss 0.5202255845069885 Val Loss 1.2715879678726196
Trainable Parameters : 334440
Epoch 4 Train Acc 62.380226135253906% Val Acc 50.0% Train Loss 0.47486618161201477 Val Loss 1.2129255533218384
Trainable Parameters : 334440
Epoch 5 Train Acc 64.2775650024414% Val Acc 51.79999923706055% Train Loss 0.44932207465171814 Val Loss 1.1675347089767456
Trainable Parameters : 334440
Epoch 6 Train Acc 65.06463623046875% Val Acc 53.900001525878906% Train Loss 0.44001078605651855 Val Loss 1.1644866466522217
Trainable Parameters : 334440
Epoch 7 Train Acc 65.33460235595703% Val Acc 49.79999923706055% Train Loss 0.4358295500278473 Val Loss 1.173532247543335
Trainable Parameters : 334440
Epoch 8 Train Acc 64.86312103271484% Val Acc 55.900001525878906% Train Loss 0.4344829320907593 Val Loss 1.093900203704834
Trainable Parameters : 334440
Epoch 9 Train Acc 65.09125518798828% Val Acc 53.20000076293945% Train Loss 0.4357416033744812 Val Loss 1.1770401000976562
Trainable Parameters : 334440
Epoch 10 Train Acc 65.65019226074219% Val Acc 55.400001525878906% Train Loss 0.4328799247741699 Val Loss 1.1906394958496094
Trainable Parameters : 334440
Epoch 11 Train Acc 65.4562759399414% Val Acc 50.10000228881836% Train Loss 0.433936208486557 Val Loss 1.3031840324401855
Trainable Parameters : 334440
Epoch 12 Train Acc 65.18251037597656% Val Acc 51.900001525878906% Train Loss 0.433461457490921 Val Loss 1.1996287107467651
Trainable Parameters : 334440
Epoch 13 Train Acc 64.7490463256836% Val Acc 53.70000076293945% Train Loss 0.43267783522605896 Val Loss 1.084492802619934
Trainable Parameters : 334440
Epoch 14 Train Acc 65.1026611328125% Val Acc 55.400001525878906% Train Loss 0.4327503740787506 Val Loss 1.1055816411972046
Trainable Parameters : 334440
Epoch 15 Train Acc 64.90113830566406% Val Acc 51.900001525878906% Train Loss 0.4327017366886139 Val Loss 1.1427282094955444
Trainable Parameters : 334440
Epoch 16 Train Acc 65.05703735351562% Val Acc 52.900001525878906% Train Loss 0.42987075448036194 Val Loss 1.338843584060669
Trainable Parameters : 334440
Epoch 17 Train Acc 65.26615905761719% Val Acc 52.900001525878906% Train Loss 0.43323424458503723 Val Loss 1.165298581123352
Trainable Parameters : 334440
Epoch 18 Train Acc 65.66539764404297% Val Acc 53.400001525878906% Train Loss 0.42928555607795715 Val Loss 1.077077865600586
Trainable Parameters : 334440
Epoch 19 Train Acc 65.71482849121094% Val Acc 52.5% Train Loss 0.42973950505256653 Val Loss 1.1383270025253296
Trainable Parameters : 334440
Epoch 20 Train Acc 65.17870330810547% Val Acc 53.79999923706055% Train Loss 0.42924776673316956 Val Loss 1.0998846292495728
Trainable Parameters : 334440
Epoch 21 Train Acc 64.96578216552734% Val Acc 52.70000076293945% Train Loss 0.43077200651168823 Val Loss 1.1081171035766602
Trainable Parameters : 334440
Epoch 22 Train Acc 65.09886169433594% Val Acc 54.79999923706055% Train Loss 0.42855289578437805 Val Loss 1.1105833053588867
Trainable Parameters : 334440
Epoch 23 Train Acc 65.86312103271484% Val Acc 59.900001525878906% Train Loss 0.43078523874282837 Val Loss 1.033975601196289
Trainable Parameters : 334440
Epoch 24 Train Acc 65.11406707763672% Val Acc 51.400001525878906% Train Loss 0.4283957779407501 Val Loss 1.153473973274231
Trainable Parameters : 334440
Epoch 25 Train Acc 65.63878631591797% Val Acc 56.10000228881836% Train Loss 0.42676687240600586 Val Loss 1.0968513488769531
Trainable Parameters : 334440
Epoch 26 Train Acc 65.33460235595703% Val Acc 52.79999923706055% Train Loss 0.4271481931209564 Val Loss 1.1688841581344604
Trainable Parameters : 334440
Epoch 27 Train Acc 65.65779113769531% Val Acc 51.5% Train Loss 0.424691766500473 Val Loss 1.230034351348877
Trainable Parameters : 334440
Epoch 28 Train Acc 65.69581604003906% Val Acc 52.5% Train Loss 0.42371392250061035 Val Loss 1.241396188735962
Trainable Parameters : 334440
Epoch 29 Train Acc 65.76045227050781% Val Acc 48.10000228881836% Train Loss 0.4222243130207062 Val Loss 1.3899708986282349
Trainable Parameters : 334440
Epoch 30 Train Acc 66.00379943847656% Val Acc 49.900001525878906% Train Loss 0.42401111125946045 Val Loss 1.1553115844726562
Trainable Parameters : 334440
Epoch 31 Train Acc 65.99239349365234% Val Acc 54.70000076293945% Train Loss 0.4241427779197693 Val Loss 1.0900415182113647
Trainable Parameters : 334440
Epoch 32 Train Acc 65.8212890625% Val Acc 50.70000076293945% Train Loss 0.42297428846359253 Val Loss 1.3042337894439697
Trainable Parameters : 334440
Epoch 33 Train Acc 65.85931396484375% Val Acc 52.60000228881836% Train Loss 0.42180684208869934 Val Loss 1.1704208850860596
Trainable Parameters : 334440
Epoch 34 Train Acc 66.2509536743164% Val Acc 44.70000076293945% Train Loss 0.423469215631485 Val Loss 1.3138145208358765
Trainable Parameters : 334440
Epoch 35 Train Acc 66.3079833984375% Val Acc 44.79999923706055% Train Loss 0.41986095905303955 Val Loss 1.4211084842681885
Trainable Parameters : 334440
Epoch 36 Train Acc 65.94676971435547% Val Acc 53.5% Train Loss 0.41853487491607666 Val Loss 1.219825267791748
Trainable Parameters : 334440
Epoch 37 Train Acc 66.1749038696289% Val Acc 51.20000076293945% Train Loss 0.4187495708465576 Val Loss 1.1197621822357178
Trainable Parameters : 334440
Epoch 38 Train Acc 67.08745574951172% Val Acc 51.10000228881836% Train Loss 0.4136072099208832 Val Loss 1.2898448705673218
Trainable Parameters : 334440
Epoch 39 Train Acc 67.06463623046875% Val Acc 43.5% Train Loss 0.41139477491378784 Val Loss 1.3742547035217285
Trainable Parameters : 334440
Epoch 40 Train Acc 67.50189971923828% Val Acc 49.20000076293945% Train Loss 0.4070601165294647 Val Loss 1.256580114364624
Trainable Parameters : 334440
Epoch 41 Train Acc 67.71482849121094% Val Acc 46.5% Train Loss 0.4014509618282318 Val Loss 1.434922218322754
Trainable Parameters : 334440
Epoch 42 Train Acc 67.99239349365234% Val Acc 55.79999923706055% Train Loss 0.4000190794467926 Val Loss 1.1371761560440063
Trainable Parameters : 334440
Epoch 43 Train Acc 68.06463623046875% Val Acc 53.20000076293945% Train Loss 0.397863507270813 Val Loss 1.1947598457336426
Trainable Parameters : 334440
Epoch 44 Train Acc 68.2813720703125% Val Acc 48.900001525878906% Train Loss 0.3923005759716034 Val Loss 1.2791372537612915
Trainable Parameters : 334440
Epoch 45 Train Acc 69.01140594482422% Val Acc 53.5% Train Loss 0.3899230659008026 Val Loss 1.1064822673797607
Trainable Parameters : 334440
Epoch 46 Train Acc 69.06463623046875% Val Acc 44.20000076293945% Train Loss 0.38835635781288147 Val Loss 1.549765944480896
Trainable Parameters : 334440
Epoch 47 Train Acc 69.41825103759766% Val Acc 58.20000076293945% Train Loss 0.38471779227256775 Val Loss 1.0603958368301392
Trainable Parameters : 334440
Epoch 48 Train Acc 69.47528839111328% Val Acc 42.20000076293945% Train Loss 0.3775625228881836 Val Loss 1.3986510038375854
Trainable Parameters : 334440
Epoch 49 Train Acc 70.49049377441406% Val Acc 45.29999923706055% Train Loss 0.3767312169075012 Val Loss 1.314015507698059
Trainable Parameters : 334440
Epoch 50 Train Acc 70.45246887207031% Val Acc 52.70000076293945% Train Loss 0.3694809079170227 Val Loss 1.2174140214920044
Trainable Parameters : 334440
Epoch 51 Train Acc 70.43345642089844% Val Acc 53.400001525878906% Train Loss 0.3697684407234192 Val Loss 1.2127879858016968
Trainable Parameters : 334440
Epoch 52 Train Acc 70.8250961303711% Val Acc 57.29999923706055% Train Loss 0.364933043718338 Val Loss 1.036067008972168
Trainable Parameters : 334440
Epoch 53 Train Acc 71.44866943359375% Val Acc 53.5% Train Loss 0.36403945088386536 Val Loss 1.135433316230774
Trainable Parameters : 334440
Epoch 54 Train Acc 71.47908782958984% Val Acc 46.900001525878906% Train Loss 0.35671037435531616 Val Loss 1.4467475414276123
Trainable Parameters : 334440
Epoch 55 Train Acc 71.69961547851562% Val Acc 53.0% Train Loss 0.35866567492485046 Val Loss 1.3848015069961548
Trainable Parameters : 334440
Epoch 56 Train Acc 71.80988311767578% Val Acc 50.60000228881836% Train Loss 0.3537340760231018 Val Loss 1.3378217220306396
Trainable Parameters : 334440
Epoch 57 Train Acc 71.99620056152344% Val Acc 51.10000228881836% Train Loss 0.3464716970920563 Val Loss 1.2045040130615234
Trainable Parameters : 334440
Epoch 58 Train Acc 72.88593292236328% Val Acc 55.60000228881836% Train Loss 0.3458133339881897 Val Loss 1.1521466970443726
Trainable Parameters : 334440
Epoch 59 Train Acc 73.08744812011719% Val Acc 49.0% Train Loss 0.34221184253692627 Val Loss 1.268485188484192
Trainable Parameters : 334440
Epoch 60 Train Acc 73.46387481689453% Val Acc 55.0% Train Loss 0.337957501411438 Val Loss 1.1367346048355103
Trainable Parameters : 334440
Epoch 61 Train Acc 73.97718811035156% Val Acc 53.79999923706055% Train Loss 0.32858389616012573 Val Loss 1.2871248722076416
Trainable Parameters : 334440
Epoch 62 Train Acc 73.4828872680664% Val Acc 54.79999923706055% Train Loss 0.3342977464199066 Val Loss 1.172959327697754
Trainable Parameters : 334440
Epoch 63 Train Acc 74.39923858642578% Val Acc 53.400001525878906% Train Loss 0.3288443684577942 Val Loss 1.2169547080993652
Trainable Parameters : 334440
Epoch 64 Train Acc 74.2509536743164% Val Acc 53.60000228881836% Train Loss 0.3269179165363312 Val Loss 1.3153817653656006
Trainable Parameters : 334440
Epoch 65 Train Acc 74.8517074584961% Val Acc 48.10000228881836% Train Loss 0.31687065958976746 Val Loss 1.5079216957092285
Trainable Parameters : 334440
Epoch 66 Train Acc 75.60076141357422% Val Acc 53.0% Train Loss 0.3168208599090576 Val Loss 1.255644679069519
Trainable Parameters : 334440
Epoch 67 Train Acc 75.63497924804688% Val Acc 49.29999923706055% Train Loss 0.30716362595558167 Val Loss 1.4980496168136597
Trainable Parameters : 334440
Epoch 68 Train Acc 75.46007537841797% Val Acc 49.0% Train Loss 0.31065574288368225 Val Loss 1.6138778924942017
Trainable Parameters : 334440
Epoch 69 Train Acc 76.21292877197266% Val Acc 50.400001525878906% Train Loss 0.30245715379714966 Val Loss 1.2476215362548828
Trainable Parameters : 334440
Epoch 70 Train Acc 76.85931396484375% Val Acc 49.29999923706055% Train Loss 0.294924259185791 Val Loss 1.5020545721054077
Trainable Parameters : 334440
Epoch 71 Train Acc 76.42205047607422% Val Acc 55.400001525878906% Train Loss 0.29840150475502014 Val Loss 1.1720569133758545
Trainable Parameters : 334440
Epoch 72 Train Acc 76.62357330322266% Val Acc 46.60000228881836% Train Loss 0.2982131838798523 Val Loss 1.4326425790786743
Trainable Parameters : 334440
Epoch 73 Train Acc 77.30037689208984% Val Acc 44.900001525878906% Train Loss 0.2900509238243103 Val Loss 1.6247749328613281
Trainable Parameters : 334440
Epoch 74 Train Acc 77.75665283203125% Val Acc 48.10000228881836% Train Loss 0.2864829897880554 Val Loss 1.3571436405181885
Trainable Parameters : 334440
Epoch 75 Train Acc 77.37261962890625% Val Acc 55.400001525878906% Train Loss 0.28916996717453003 Val Loss 1.2087236642837524
Trainable Parameters : 334440
Epoch 76 Train Acc 78.39543914794922% Val Acc 56.0% Train Loss 0.27967044711112976 Val Loss 1.2968765497207642
Trainable Parameters : 334440
Epoch 77 Train Acc 77.9543685913086% Val Acc 50.0% Train Loss 0.280258446931839 Val Loss 1.335585355758667
Trainable Parameters : 334440
Epoch 78 Train Acc 78.17870330810547% Val Acc 49.5% Train Loss 0.276685893535614 Val Loss 1.3905085325241089
Trainable Parameters : 334440
Epoch 79 Train Acc 78.74144744873047% Val Acc 51.70000076293945% Train Loss 0.26966971158981323 Val Loss 1.2547866106033325
Trainable Parameters : 334440
Epoch 80 Train Acc 78.4410629272461% Val Acc 44.29999923706055% Train Loss 0.2759087383747101 Val Loss 1.845762848854065
Trainable Parameters : 334440
Epoch 81 Train Acc 79.50569915771484% Val Acc 50.400001525878906% Train Loss 0.2652725279331207 Val Loss 1.44739830493927
Trainable Parameters : 334440
Epoch 82 Train Acc 79.84790802001953% Val Acc 57.10000228881836% Train Loss 0.26589399576187134 Val Loss 1.4279898405075073
Trainable Parameters : 334440
Epoch 83 Train Acc 80.01901245117188% Val Acc 52.20000076293945% Train Loss 0.257243275642395 Val Loss 1.4332996606826782
Trainable Parameters : 334440
Epoch 84 Train Acc 80.47908782958984% Val Acc 49.5% Train Loss 0.2541276514530182 Val Loss 1.4243241548538208
Trainable Parameters : 334440
Epoch 85 Train Acc 80.75665283203125% Val Acc 57.70000076293945% Train Loss 0.24951551854610443 Val Loss 1.2595430612564087
Trainable Parameters : 334440
Epoch 86 Train Acc 80.55133056640625% Val Acc 43.79999923706055% Train Loss 0.24884706735610962 Val Loss 1.8862526416778564
Trainable Parameters : 334440
Epoch 87 Train Acc 80.81369018554688% Val Acc 55.60000228881836% Train Loss 0.24505023658275604 Val Loss 1.514889121055603
Trainable Parameters : 334440
Epoch 88 Train Acc 81.55513000488281% Val Acc 51.0% Train Loss 0.23975732922554016 Val Loss 1.56778085231781
Trainable Parameters : 334440
Epoch 89 Train Acc 81.64258575439453% Val Acc 45.900001525878906% Train Loss 0.24137085676193237 Val Loss 1.757036566734314
Trainable Parameters : 334440
Epoch 90 Train Acc 81.28517150878906% Val Acc 47.20000076293945% Train Loss 0.24012531340122223 Val Loss 1.653549075126648
Trainable Parameters : 334440
Epoch 91 Train Acc 82.32319641113281% Val Acc 47.70000076293945% Train Loss 0.23261886835098267 Val Loss 1.841464638710022
Trainable Parameters : 334440
Epoch 92 Train Acc 82.00760650634766% Val Acc 50.900001525878906% Train Loss 0.23728322982788086 Val Loss 1.7106266021728516
Trainable Parameters : 334440
Epoch 93 Train Acc 82.7224349975586% Val Acc 50.60000228881836% Train Loss 0.2242363691329956 Val Loss 1.516327977180481
Trainable Parameters : 334440
Epoch 94 Train Acc 82.5589370727539% Val Acc 48.400001525878906% Train Loss 0.22970359027385712 Val Loss 1.5729743242263794
Trainable Parameters : 334440
Epoch 95 Train Acc 82.26235961914062% Val Acc 51.60000228881836% Train Loss 0.22427980601787567 Val Loss 1.5775203704833984
Trainable Parameters : 334440
Epoch 96 Train Acc 83.15209197998047% Val Acc 47.79999923706055% Train Loss 0.21576392650604248 Val Loss 1.706085443496704
Trainable Parameters : 334440
Epoch 97 Train Acc 83.39923858642578% Val Acc 49.400001525878906% Train Loss 0.21862025558948517 Val Loss 1.7340452671051025
Trainable Parameters : 334440
Epoch 98 Train Acc 83.10646057128906% Val Acc 43.79999923706055% Train Loss 0.21761558949947357 Val Loss 1.8598133325576782
Trainable Parameters : 334440
Epoch 99 Train Acc 83.67300415039062% Val Acc 53.5% Train Loss 0.21279044449329376 Val Loss 1.570599913597107

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:55.400001525878906% Loss:1.2540861368179321
CONFUSION MATRIX
[[59 21 12  8]
 [ 6 55 22 17]
 [ 8 18 60 12]
 [12 22 20 46]]
CONFUSION MATRIX NORMALISED
[[0.14824121 0.05276382 0.03015075 0.0201005 ]
 [0.01507538 0.13819095 0.05527638 0.04271357]
 [0.0201005  0.04522613 0.15075377 0.03015075]
 [0.03015075 0.05527638 0.05025126 0.11557789]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.69      0.59      0.64       100
           1       0.47      0.55      0.51       100
           2       0.53      0.61      0.57        98
           3       0.55      0.46      0.50       100

    accuracy                           0.55       398
   macro avg       0.56      0.55      0.55       398
weighted avg       0.56      0.55      0.55       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 03/11/2022 02:56:17
