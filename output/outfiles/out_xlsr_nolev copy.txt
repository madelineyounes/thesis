Fri Nov 4 01:21:23 AEDT 2022
2022-11-04 01:21:26.594428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-04 01:21:27.197301: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-04 01:21:29.082107: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 01:21:29.084587: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 01:21:29.084602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr_nolev.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_nolev.py
Started: 04/11/2022 01:21:43

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-nolev
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: train_u_NOLEV
train_filename: train_u_NOLEV
validation_filename: dev_u_NOLEV
evaluation_filename: test_u_NOLEV
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/train_u_NOLEV.csv
--> data_test_fp: data/dev_u_NOLEV.csv
--> data_test_fp: data/test_u_NOLEV.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/train_u_NOLEV_local/ADI17-xlsr-nolev
--> finetuned_results_fp: /srv/scratch/z5208494/output/train_u_NOLEV_local/ADI17-xlsr-nolev_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-1.4770, -1.1272, -0.7458,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3914,  0.4247,  0.4587,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0870,  0.0755,  0.0793,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.3221, -0.1450, -0.0214,  ..., -3.4767, -3.2011, -2.9207],
        [-0.1872, -0.1676, -0.2116,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0705,  0.0724,  0.2456,  ..., -0.0207, -0.0155, -0.0103]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 0, 0, 1, 0, 1, 0, 2, 0, 1, 2, 2, 1,
        2, 0, 0, 0, 2, 2, 2, 1, 0, 0, 2, 0, 2, 0, 2, 1])}
Training DataCustom Files: 1825
Training Data Files: 46
Val Data Sample
{'input_values': tensor([[ 0.0838,  0.0777,  0.0171,  ...,  0.4001, -1.8231, -2.8941],
        [ 0.1896, -1.6520, -3.0503,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2849,  0.2869,  0.3495,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.5063,  0.5587,  0.5856,  ..., -0.0057,  0.0192,  0.0232],
        [-0.0736, -0.0320,  0.0055,  ..., -0.0050,  0.0066, -0.0894],
        [-0.2347, -0.2176, -0.0863,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 2, 1, 1, 2, 0, 1, 0, 2, 0, 0, 2, 0, 0, 1, 2, 1, 1, 2, 2, 2, 0, 1,
        1, 0, 2, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 1, 2, 2])}
Test CustomData Files: 596
Test Data Files: 15
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'projector.weight', 'projector.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-5.0571e-01, -5.6817e-01, -4.5861e-01,  ...,  7.2402e-01,
          9.9537e-01,  1.2032e+00],
        [-4.9014e-01, -5.9916e-01, -6.2701e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.0495e-01,  8.5473e-01,  9.0408e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 3.1107e-03, -1.0532e-03,  6.6340e-03,  ...,  2.8273e-01,
          1.0048e-01,  1.8408e-01],
        [-6.3282e-01, -1.1304e-01,  3.3103e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.1629e-02, -2.7196e-02, -2.3988e-02,  ...,  1.1976e+00,
          1.1479e+00,  1.0894e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 2, 1, 0, 1, 0, 0, 0, 2, 1, 2, 0, 2, 2, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0,
        1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 1, 0, 2, 0, 0, 2])}
Test CustomData Files: 298
Test Data Files: 8
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Epoch 0 Train Acc 33.673912048339844% Val Acc 37.375% Train Loss 0.69051194190979 Val Loss 1.379338264465332
Trainable Parameters : 264452
Epoch 1 Train Acc 35.130435943603516% Val Acc 36.375% Train Loss 0.6832512021064758 Val Loss 1.3594164848327637
Trainable Parameters : 264452
Epoch 2 Train Acc 37.80434799194336% Val Acc 36.75% Train Loss 0.671082615852356 Val Loss 1.326161503791809
Trainable Parameters : 264452
Epoch 3 Train Acc 41.56521987915039% Val Acc 33.125% Train Loss 0.6541634798049927 Val Loss 1.295966625213623
Trainable Parameters : 264452
Epoch 4 Train Acc 42.0% Val Acc 32.875% Train Loss 0.6356930732727051 Val Loss 1.2586545944213867
Trainable Parameters : 264452
Epoch 5 Train Acc 39.58695602416992% Val Acc 33.125% Train Loss 0.61842280626297 Val Loss 1.2241770029067993
Trainable Parameters : 264452
Epoch 6 Train Acc 39.130435943603516% Val Acc 31.625% Train Loss 0.6026580929756165 Val Loss 1.1996873617172241
Trainable Parameters : 264452
Epoch 7 Train Acc 39.45652389526367% Val Acc 32.625% Train Loss 0.5888898968696594 Val Loss 1.1777269840240479
Trainable Parameters : 264452
Epoch 8 Train Acc 39.91304397583008% Val Acc 34.5% Train Loss 0.5765703916549683 Val Loss 1.1544073820114136
Trainable Parameters : 264452
Epoch 9 Train Acc 40.15217590332031% Val Acc 33.375% Train Loss 0.5671171545982361 Val Loss 1.1378024816513062
Trainable Parameters : 264452
Epoch 10 Train Acc 42.26087188720703% Val Acc 28.875% Train Loss 0.5605500936508179 Val Loss 1.1467792987823486
Trainable Parameters : 264452
Epoch 11 Train Acc 41.78260803222656% Val Acc 34.625% Train Loss 0.5541180968284607 Val Loss 1.119642972946167
Trainable Parameters : 264452
Epoch 12 Train Acc 42.9782600402832% Val Acc 29.375% Train Loss 0.5491099953651428 Val Loss 1.1293058395385742
Trainable Parameters : 264452
Epoch 13 Train Acc 43.630435943603516% Val Acc 35.375% Train Loss 0.5437526106834412 Val Loss 1.112541675567627
Trainable Parameters : 264452
Epoch 14 Train Acc 44.19565200805664% Val Acc 35.125% Train Loss 0.5401403903961182 Val Loss 1.1134586334228516
Trainable Parameters : 264452
Epoch 15 Train Acc 44.15217590332031% Val Acc 35.625% Train Loss 0.5362748503684998 Val Loss 1.0997973680496216
Trainable Parameters : 264452
Epoch 16 Train Acc 44.9782600402832% Val Acc 35.375% Train Loss 0.5335155725479126 Val Loss 1.1058887243270874
Trainable Parameters : 264452
Epoch 17 Train Acc 47.19565200805664% Val Acc 35.25% Train Loss 0.5296854376792908 Val Loss 1.098500370979309
Trainable Parameters : 264452
Epoch 18 Train Acc 45.130435943603516% Val Acc 36.125% Train Loss 0.5278245806694031 Val Loss 1.089848279953003
Trainable Parameters : 264452
Epoch 19 Train Acc 47.0% Val Acc 34.625% Train Loss 0.5236695408821106 Val Loss 1.1099300384521484
Trainable Parameters : 264452
Epoch 20 Train Acc 47.60869598388672% Val Acc 35.75% Train Loss 0.5221896767616272 Val Loss 1.0951851606369019
Trainable Parameters : 264452
Epoch 21 Train Acc 46.89130401611328% Val Acc 36.375% Train Loss 0.5210179686546326 Val Loss 1.093787670135498
Trainable Parameters : 264452
Epoch 22 Train Acc 50.41304397583008% Val Acc 35.25% Train Loss 0.5175117254257202 Val Loss 1.104838252067566
Trainable Parameters : 264452
Epoch 23 Train Acc 48.89130401611328% Val Acc 33.125% Train Loss 0.5158073902130127 Val Loss 1.1141912937164307
Trainable Parameters : 264452
Epoch 24 Train Acc 50.41304397583008% Val Acc 34.625% Train Loss 0.512596070766449 Val Loss 1.118255376815796
Trainable Parameters : 264452
Epoch 25 Train Acc 49.4782600402832% Val Acc 35.25% Train Loss 0.5109438896179199 Val Loss 1.0916286706924438
Trainable Parameters : 264452
Epoch 26 Train Acc 51.60869598388672% Val Acc 35.75% Train Loss 0.5073900818824768 Val Loss 1.0945879220962524
Trainable Parameters : 264452
Epoch 27 Train Acc 50.65217590332031% Val Acc 41.625% Train Loss 0.5069460868835449 Val Loss 1.065543532371521
Trainable Parameters : 264452
Epoch 28 Train Acc 52.78260803222656% Val Acc 37.25% Train Loss 0.502630889415741 Val Loss 1.0915762186050415
Trainable Parameters : 264452
Epoch 29 Train Acc 52.0217399597168% Val Acc 39.375% Train Loss 0.5005263686180115 Val Loss 1.0860563516616821
Trainable Parameters : 264452
Epoch 30 Train Acc 53.06521987915039% Val Acc 37.5% Train Loss 0.4975743889808655 Val Loss 1.0811799764633179
Trainable Parameters : 264452
Epoch 31 Train Acc 52.45652389526367% Val Acc 42.625% Train Loss 0.4964032471179962 Val Loss 1.05888831615448
Trainable Parameters : 264452
Epoch 32 Train Acc 54.76087188720703% Val Acc 37.5% Train Loss 0.4934390187263489 Val Loss 1.0780807733535767
Trainable Parameters : 264452
Epoch 33 Train Acc 54.739131927490234% Val Acc 40.0% Train Loss 0.4918760359287262 Val Loss 1.0817039012908936
Trainable Parameters : 264452
Epoch 34 Train Acc 55.58695602416992% Val Acc 40.25% Train Loss 0.48808753490448 Val Loss 1.0696207284927368
Trainable Parameters : 264452
Epoch 35 Train Acc 55.60869598388672% Val Acc 38.625% Train Loss 0.48673829436302185 Val Loss 1.090951919555664
Trainable Parameters : 264452
Epoch 36 Train Acc 54.71739196777344% Val Acc 38.5% Train Loss 0.48444944620132446 Val Loss 1.0934215784072876
Trainable Parameters : 264452
Epoch 37 Train Acc 55.76087188720703% Val Acc 40.5% Train Loss 0.48108598589897156 Val Loss 1.089421272277832
Trainable Parameters : 264452
Epoch 38 Train Acc 56.434783935546875% Val Acc 41.625% Train Loss 0.4790511727333069 Val Loss 1.0698243379592896
Trainable Parameters : 264452
Epoch 39 Train Acc 57.326087951660156% Val Acc 38.125% Train Loss 0.4767698347568512 Val Loss 1.1370503902435303
Trainable Parameters : 264452
Epoch 40 Train Acc 56.5217399597168% Val Acc 46.0% Train Loss 0.47271421551704407 Val Loss 1.056995153427124
Trainable Parameters : 264452
Epoch 41 Train Acc 58.326087951660156% Val Acc 43.5% Train Loss 0.47404950857162476 Val Loss 1.0551172494888306
Trainable Parameters : 264452
Epoch 42 Train Acc 58.67391586303711% Val Acc 43.125% Train Loss 0.4707271456718445 Val Loss 1.0532867908477783
Trainable Parameters : 264452
Epoch 43 Train Acc 57.043479919433594% Val Acc 50.875% Train Loss 0.4695117473602295 Val Loss 1.0371840000152588
Trainable Parameters : 264452
Epoch 44 Train Acc 57.65217590332031% Val Acc 49.625% Train Loss 0.46752282977104187 Val Loss 1.011316180229187
Trainable Parameters : 264452
Epoch 45 Train Acc 60.34782791137695% Val Acc 41.75% Train Loss 0.45894941687583923 Val Loss 1.0818026065826416
Trainable Parameters : 264452
Epoch 46 Train Acc 59.130435943603516% Val Acc 45.5% Train Loss 0.4593302309513092 Val Loss 1.0414791107177734
Trainable Parameters : 264452
Epoch 47 Train Acc 59.56521987915039% Val Acc 41.75% Train Loss 0.4575071930885315 Val Loss 1.0835403203964233
Trainable Parameters : 264452
Epoch 48 Train Acc 60.06521987915039% Val Acc 40.75% Train Loss 0.45280084013938904 Val Loss 1.1245282888412476
Trainable Parameters : 264452
Epoch 49 Train Acc 60.30434799194336% Val Acc 47.625% Train Loss 0.450287401676178 Val Loss 1.0393929481506348
Trainable Parameters : 264452
Epoch 50 Train Acc 60.47826385498047% Val Acc 38.625% Train Loss 0.4473447799682617 Val Loss 1.1467845439910889
Trainable Parameters : 264452
Epoch 51 Train Acc 59.56521987915039% Val Acc 46.75% Train Loss 0.4512379765510559 Val Loss 1.083091378211975
Trainable Parameters : 264452
Epoch 52 Train Acc 61.36956787109375% Val Acc 48.375% Train Loss 0.4471227526664734 Val Loss 1.0335060358047485
Trainable Parameters : 264452
Epoch 53 Train Acc 61.95652389526367% Val Acc 43.25% Train Loss 0.4456191062927246 Val Loss 1.0554274320602417
Trainable Parameters : 264452
Epoch 54 Train Acc 60.28260803222656% Val Acc 53.5% Train Loss 0.4425395131111145 Val Loss 1.0053558349609375
Trainable Parameters : 264452
Epoch 55 Train Acc 61.0% Val Acc 54.875% Train Loss 0.4416895806789398 Val Loss 0.994583249092102
Trainable Parameters : 264452
Epoch 56 Train Acc 59.80434799194336% Val Acc 47.625% Train Loss 0.4444122016429901 Val Loss 1.025336503982544
Trainable Parameters : 264452
Epoch 57 Train Acc 60.5217399597168% Val Acc 49.875% Train Loss 0.4398738741874695 Val Loss 1.004660725593567
Trainable Parameters : 264452
Epoch 58 Train Acc 62.65217590332031% Val Acc 53.5% Train Loss 0.4326319694519043 Val Loss 0.9723606109619141
Trainable Parameters : 264452
Epoch 59 Train Acc 62.45652389526367% Val Acc 52.625% Train Loss 0.43268877267837524 Val Loss 0.9888051152229309
Trainable Parameters : 264452
Epoch 60 Train Acc 62.239131927490234% Val Acc 57.5% Train Loss 0.4311939775943756 Val Loss 0.975783109664917
Trainable Parameters : 264452
Epoch 61 Train Acc 62.15217590332031% Val Acc 46.625% Train Loss 0.4272311329841614 Val Loss 1.0151327848434448
Trainable Parameters : 264452
Epoch 62 Train Acc 64.06521606445312% Val Acc 53.375% Train Loss 0.42415839433670044 Val Loss 0.9552687406539917
Trainable Parameters : 264452
Epoch 63 Train Acc 62.69565200805664% Val Acc 56.25% Train Loss 0.4260384142398834 Val Loss 0.958625316619873
Trainable Parameters : 264452
Epoch 64 Train Acc 63.0217399597168% Val Acc 56.75% Train Loss 0.42364147305488586 Val Loss 0.948007345199585
Trainable Parameters : 264452
Epoch 65 Train Acc 63.97826385498047% Val Acc 51.625% Train Loss 0.4225795567035675 Val Loss 1.0027480125427246
Trainable Parameters : 264452
Epoch 66 Train Acc 63.130435943603516% Val Acc 48.125% Train Loss 0.4186268448829651 Val Loss 1.0252379179000854
Trainable Parameters : 264452
Epoch 67 Train Acc 63.130435943603516% Val Acc 55.875% Train Loss 0.418958455324173 Val Loss 0.9624671339988708
Trainable Parameters : 264452
Epoch 68 Train Acc 65.26087188720703% Val Acc 54.125% Train Loss 0.41360586881637573 Val Loss 0.9884737730026245
Trainable Parameters : 264452
Epoch 69 Train Acc 64.67391204833984% Val Acc 55.75% Train Loss 0.41330745816230774 Val Loss 0.9821925163269043
Trainable Parameters : 264452
Epoch 70 Train Acc 63.326087951660156% Val Acc 48.0% Train Loss 0.41189977526664734 Val Loss 1.0527795553207397
Trainable Parameters : 264452
Epoch 71 Train Acc 62.71739196777344% Val Acc 51.125% Train Loss 0.41848739981651306 Val Loss 1.0283249616622925
Trainable Parameters : 264452
Epoch 72 Train Acc 64.67391204833984% Val Acc 58.375% Train Loss 0.4083547592163086 Val Loss 0.9557508826255798
Trainable Parameters : 264452
Epoch 73 Train Acc 64.0% Val Acc 58.375% Train Loss 0.4084118604660034 Val Loss 0.9346149563789368
Trainable Parameters : 264452
Epoch 74 Train Acc 64.02174377441406% Val Acc 55.375% Train Loss 0.4073089063167572 Val Loss 0.9372258186340332
Trainable Parameters : 264452
Epoch 75 Train Acc 65.32608795166016% Val Acc 53.75% Train Loss 0.40417930483818054 Val Loss 1.02414870262146
Trainable Parameters : 264452
Epoch 76 Train Acc 65.30435180664062% Val Acc 52.875% Train Loss 0.4036921262741089 Val Loss 0.9792114496231079
Trainable Parameters : 264452
Epoch 77 Train Acc 65.4565200805664% Val Acc 55.375% Train Loss 0.39901837706565857 Val Loss 0.9554224610328674
Trainable Parameters : 264452
Epoch 78 Train Acc 65.80435180664062% Val Acc 54.0% Train Loss 0.3986895978450775 Val Loss 0.9665698409080505
Trainable Parameters : 264452
Epoch 79 Train Acc 66.0% Val Acc 54.75% Train Loss 0.398397296667099 Val Loss 0.9670809507369995
Trainable Parameters : 264452
Epoch 80 Train Acc 65.76087188720703% Val Acc 50.25% Train Loss 0.3947177231311798 Val Loss 1.05696702003479
Trainable Parameters : 264452
Epoch 81 Train Acc 65.30435180664062% Val Acc 52.625% Train Loss 0.39873313903808594 Val Loss 1.006670355796814
Trainable Parameters : 264452
Epoch 82 Train Acc 65.15217590332031% Val Acc 54.75% Train Loss 0.3937014639377594 Val Loss 1.0041927099227905
Trainable Parameters : 264452
Epoch 83 Train Acc 67.36956787109375% Val Acc 55.5% Train Loss 0.3905644714832306 Val Loss 0.9728118181228638
Trainable Parameters : 264452
Epoch 84 Train Acc 65.80435180664062% Val Acc 53.375% Train Loss 0.39206331968307495 Val Loss 1.0223726034164429
Trainable Parameters : 264452
Epoch 85 Train Acc 65.84782409667969% Val Acc 58.875% Train Loss 0.3918326497077942 Val Loss 0.917422354221344
Trainable Parameters : 264452
Epoch 86 Train Acc 67.08695983886719% Val Acc 48.0% Train Loss 0.38457241654396057 Val Loss 1.120705246925354
Trainable Parameters : 264452
Epoch 87 Train Acc 67.4565200805664% Val Acc 53.625% Train Loss 0.38254544138908386 Val Loss 0.9572849273681641
Trainable Parameters : 264452
Epoch 88 Train Acc 66.41304779052734% Val Acc 51.5% Train Loss 0.38224634528160095 Val Loss 1.0404024124145508
Trainable Parameters : 264452
Epoch 89 Train Acc 66.91304779052734% Val Acc 56.25% Train Loss 0.3818131685256958 Val Loss 0.9589024782180786
Trainable Parameters : 264452
Epoch 90 Train Acc 67.63043975830078% Val Acc 48.0% Train Loss 0.3813164532184601 Val Loss 1.10177481174469
Trainable Parameters : 264452
Epoch 91 Train Acc 66.28260803222656% Val Acc 53.375% Train Loss 0.3779681622982025 Val Loss 1.0258152484893799
Trainable Parameters : 264452
Epoch 92 Train Acc 67.08695983886719% Val Acc 50.25% Train Loss 0.38475534319877625 Val Loss 1.064633846282959
Trainable Parameters : 264452
Epoch 93 Train Acc 68.52174377441406% Val Acc 62.375% Train Loss 0.37456539273262024 Val Loss 0.8834455013275146
Trainable Parameters : 264452
Epoch 94 Train Acc 66.78260803222656% Val Acc 54.25% Train Loss 0.378512978553772 Val Loss 0.9499419927597046
Trainable Parameters : 264452
Epoch 95 Train Acc 67.52174377441406% Val Acc 45.375% Train Loss 0.3856978416442871 Val Loss 1.1704331636428833
Trainable Parameters : 264452
Epoch 96 Train Acc 67.60869598388672% Val Acc 59.125% Train Loss 0.3781822919845581 Val Loss 0.941545844078064
Trainable Parameters : 264452
Epoch 97 Train Acc 67.23912811279297% Val Acc 56.375% Train Loss 0.3730236291885376 Val Loss 0.9864884614944458
Trainable Parameters : 264452
Epoch 98 Train Acc 66.4565200805664% Val Acc 53.375% Train Loss 0.3772292733192444 Val Loss 1.0545107126235962
Trainable Parameters : 264452
Epoch 99 Train Acc 67.82608795166016% Val Acc 50.25% Train Loss 0.37583649158477783 Val Loss 1.0613446235656738

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:60.125% Loss:0.9229449033737183
CONFUSION MATRIX
[[43 45 12]
 [ 5 86  9]
 [ 6 44 48]]
CONFUSION MATRIX NORMALISED
[[0.1442953  0.15100671 0.04026846]
 [0.01677852 0.2885906  0.03020134]
 [0.02013423 0.14765101 0.16107383]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.80      0.43      0.56       100
           1       0.49      0.86      0.63       100
           2       0.70      0.49      0.57        98

    accuracy                           0.59       298
   macro avg       0.66      0.59      0.59       298
weighted avg       0.66      0.59      0.59       298


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 04/11/2022 03:18:58
