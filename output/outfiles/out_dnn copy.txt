Tue Nov 15 14:20:50 AEDT 2022
2022-11-15 14:20:52.800596: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-15 14:20:53.185468: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-15 14:20:53.315591: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-15 14:20:55.438707: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-15 14:20:55.440004: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-15 14:20:55.440014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_dnn_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_dnn_downstream.py
Started: 15/11/2022 14:21:09

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-dnn
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-dnn
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-dnn_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.5554, -1.2622, -0.7207,  ...,  1.8377,  1.7675,  1.5790],
        [-0.2994, -0.2905, -0.2768,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.2223,  1.3645,  1.5621,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0439,  0.0610,  0.0753,  ...,  0.7105,  0.8055,  0.9735],
        [-0.0188, -0.0184, -0.0162,  ...,  0.0000,  0.0000,  0.0000],
        [-2.0921, -1.2672, -0.4190,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 2, 3, 3, 2, 3, 1, 0, 2, 2, 0, 0, 2, 1, 2, 0, 3, 2, 0, 0, 2, 0, 2, 0,
        0, 2, 2, 2, 2, 3, 0, 1, 1, 3, 0, 0, 0, 1, 1, 1])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.0117, -0.1111, -0.0672,  ...,  0.0000,  0.0000,  0.0000],
        [-0.7708, -0.6337, -0.5243,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.9688,  2.5601,  3.0275,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.1088, -0.0483, -0.0257,  ...,  0.0933,  0.0813,  0.0946],
        [-0.0839, -0.0557,  0.0224,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1691, -0.2025, -0.1341,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 3, 3, 3, 2, 3, 0, 0, 3, 1, 3, 2, 0, 0, 1, 1, 0, 3, 1, 0, 1, 2, 3, 1,
        2, 1, 3, 0, 2, 0, 2, 0, 3, 0, 2, 3, 0, 3, 2, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.bias', 'projector.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0533, -0.1719, -0.4372,  ..., -1.1598, -0.9418, -0.5928],
        [-1.1880, -1.3231, -1.1240,  ...,  0.0000,  0.0000,  0.0000],
        [-2.6289, -2.5579, -2.0135,  ...,  0.3066, -0.0082, -0.0421],
        ...,
        [ 0.2081,  0.3479,  0.3962,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.8125,  0.5474,  0.4316,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2053, -0.1531, -0.3971,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 1, 0, 2, 2, 0, 2, 1, 3, 3, 3, 1, 1, 2, 0, 2, 3, 2, 3, 1, 1, 2, 0,
        1, 3, 1, 3, 3, 1, 2, 2, 1, 2, 0, 1, 0, 2, 2, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 362628
Epoch 0 Train Acc 21.756654739379883% Val Acc 29.700000762939453% Train Loss 0.692217230796814 Val Loss 1.3704030513763428
Trainable Parameters : 362628
Epoch 1 Train Acc 41.11787033081055% Val Acc 31.0% Train Loss 0.6443300247192383 Val Loss 1.3854236602783203
Trainable Parameters : 362628
Epoch 2 Train Acc 50.338401794433594% Val Acc 40.70000076293945% Train Loss 0.5843348503112793 Val Loss 1.299353003501892
Trainable Parameters : 362628
Epoch 3 Train Acc 58.71482849121094% Val Acc 40.79999923706055% Train Loss 0.5273701548576355 Val Loss 1.2773059606552124
Trainable Parameters : 362628
Epoch 4 Train Acc 61.90113830566406% Val Acc 50.0% Train Loss 0.4817192256450653 Val Loss 1.226178765296936
Trainable Parameters : 362628
Epoch 5 Train Acc 63.980987548828125% Val Acc 50.79999923706055% Train Loss 0.45304074883461 Val Loss 1.1694245338439941
Trainable Parameters : 362628
Epoch 6 Train Acc 64.90494537353516% Val Acc 52.79999923706055% Train Loss 0.4442744851112366 Val Loss 1.1663326025009155
Trainable Parameters : 362628
Epoch 7 Train Acc 64.88593292236328% Val Acc 51.0% Train Loss 0.44066402316093445 Val Loss 1.1947144269943237
Trainable Parameters : 362628
Epoch 8 Train Acc 65.19771575927734% Val Acc 54.400001525878906% Train Loss 0.4340488016605377 Val Loss 1.111258864402771
Trainable Parameters : 362628
Epoch 9 Train Acc 64.62357330322266% Val Acc 52.29999923706055% Train Loss 0.43966880440711975 Val Loss 1.1591265201568604
Trainable Parameters : 362628
Epoch 10 Train Acc 65.5589370727539% Val Acc 54.5% Train Loss 0.4377913475036621 Val Loss 1.1968963146209717
Trainable Parameters : 362628
Epoch 11 Train Acc 64.615966796875% Val Acc 49.10000228881836% Train Loss 0.4386778175830841 Val Loss 1.3043763637542725
Trainable Parameters : 362628
Epoch 12 Train Acc 65.19391632080078% Val Acc 49.0% Train Loss 0.4348160922527313 Val Loss 1.2840017080307007
Trainable Parameters : 362628
Epoch 13 Train Acc 65.23954010009766% Val Acc 54.400001525878906% Train Loss 0.43557021021842957 Val Loss 1.1161086559295654
Trainable Parameters : 362628
Epoch 14 Train Acc 65.38783264160156% Val Acc 55.60000228881836% Train Loss 0.43237069249153137 Val Loss 1.0987237691879272
Trainable Parameters : 362628
Epoch 15 Train Acc 65.60836791992188% Val Acc 46.10000228881836% Train Loss 0.432387113571167 Val Loss 1.2771934270858765
Trainable Parameters : 362628
Epoch 16 Train Acc 65.5589370727539% Val Acc 49.10000228881836% Train Loss 0.4308710992336273 Val Loss 1.369938850402832
Trainable Parameters : 362628
Epoch 17 Train Acc 65.0% Val Acc 47.5% Train Loss 0.43290504813194275 Val Loss 1.3726636171340942
Trainable Parameters : 362628
Epoch 18 Train Acc 65.9543685913086% Val Acc 54.70000076293945% Train Loss 0.43226075172424316 Val Loss 1.0952606201171875
Trainable Parameters : 362628
Epoch 19 Train Acc 64.6882095336914% Val Acc 53.0% Train Loss 0.4357994496822357 Val Loss 1.1587668657302856
Trainable Parameters : 362628
Epoch 20 Train Acc 65.01520538330078% Val Acc 54.900001525878906% Train Loss 0.4321422576904297 Val Loss 1.0850497484207153
Trainable Parameters : 362628
Epoch 21 Train Acc 65.37261962890625% Val Acc 51.60000228881836% Train Loss 0.4301868677139282 Val Loss 1.1510814428329468
Trainable Parameters : 362628
Epoch 22 Train Acc 64.41825103759766% Val Acc 56.79999923706055% Train Loss 0.43418726325035095 Val Loss 1.1097272634506226
Trainable Parameters : 362628
Epoch 23 Train Acc 65.46768188476562% Val Acc 55.5% Train Loss 0.430728554725647 Val Loss 1.0789117813110352
Trainable Parameters : 362628
Epoch 24 Train Acc 65.27376556396484% Val Acc 51.400001525878906% Train Loss 0.4315243661403656 Val Loss 1.1878414154052734
Trainable Parameters : 362628
Epoch 25 Train Acc 65.4866943359375% Val Acc 56.70000076293945% Train Loss 0.4302389621734619 Val Loss 1.0504359006881714
Trainable Parameters : 362628
Epoch 26 Train Acc 65.0304183959961% Val Acc 51.400001525878906% Train Loss 0.43126609921455383 Val Loss 1.2132693529129028
Trainable Parameters : 362628
Epoch 27 Train Acc 65.39163208007812% Val Acc 49.900001525878906% Train Loss 0.4316229820251465 Val Loss 1.270856261253357
Trainable Parameters : 362628
Epoch 28 Train Acc 65.49049377441406% Val Acc 52.79999923706055% Train Loss 0.4285222589969635 Val Loss 1.2528369426727295
Trainable Parameters : 362628
Epoch 29 Train Acc 64.76805877685547% Val Acc 46.70000076293945% Train Loss 0.4311714172363281 Val Loss 1.310538649559021
Trainable Parameters : 362628
Epoch 30 Train Acc 65.0722427368164% Val Acc 48.10000228881836% Train Loss 0.4302026331424713 Val Loss 1.2153476476669312
Trainable Parameters : 362628
Epoch 31 Train Acc 65.78707122802734% Val Acc 50.29999923706055% Train Loss 0.42521345615386963 Val Loss 1.2138632535934448
Trainable Parameters : 362628
Epoch 32 Train Acc 65.34600830078125% Val Acc 50.10000228881836% Train Loss 0.42863935232162476 Val Loss 1.252737283706665
Trainable Parameters : 362628
Epoch 33 Train Acc 65.44866943359375% Val Acc 50.70000076293945% Train Loss 0.4327085316181183 Val Loss 1.2541165351867676
Trainable Parameters : 362628
Epoch 34 Train Acc 65.42205047607422% Val Acc 50.60000228881836% Train Loss 0.4275023937225342 Val Loss 1.1842272281646729
Trainable Parameters : 362628
Epoch 35 Train Acc 64.80988311767578% Val Acc 43.79999923706055% Train Loss 0.4302947521209717 Val Loss 1.4193859100341797
Trainable Parameters : 362628
Epoch 36 Train Acc 65.7490463256836% Val Acc 52.5% Train Loss 0.4239870011806488 Val Loss 1.0886763334274292
Trainable Parameters : 362628
Epoch 37 Train Acc 65.67680358886719% Val Acc 55.70000076293945% Train Loss 0.4293968677520752 Val Loss 1.1709779500961304
Trainable Parameters : 362628
Epoch 38 Train Acc 65.19011688232422% Val Acc 51.20000076293945% Train Loss 0.42858514189720154 Val Loss 1.2732845544815063
Trainable Parameters : 362628
Epoch 39 Train Acc 66.18251037597656% Val Acc 35.0% Train Loss 0.421192467212677 Val Loss 1.5873006582260132
Trainable Parameters : 362628
Epoch 40 Train Acc 66.5437240600586% Val Acc 57.10000228881836% Train Loss 0.4160412549972534 Val Loss 1.1280196905136108
Trainable Parameters : 362628
Epoch 41 Train Acc 66.41825103759766% Val Acc 50.10000228881836% Train Loss 0.4131765365600586 Val Loss 1.1966897249221802
Trainable Parameters : 362628
Epoch 42 Train Acc 66.8212890625% Val Acc 50.400001525878906% Train Loss 0.4140084683895111 Val Loss 1.286617398262024
Trainable Parameters : 362628
Epoch 43 Train Acc 67.84790802001953% Val Acc 56.29999923706055% Train Loss 0.4028981029987335 Val Loss 1.159225344657898
Trainable Parameters : 362628
Epoch 44 Train Acc 67.81369018554688% Val Acc 54.400001525878906% Train Loss 0.40252330899238586 Val Loss 1.123481035232544
Trainable Parameters : 362628
Epoch 45 Train Acc 68.26995849609375% Val Acc 56.60000228881836% Train Loss 0.4000140130519867 Val Loss 1.0415548086166382
Trainable Parameters : 362628
Epoch 46 Train Acc 68.03802490234375% Val Acc 46.29999923706055% Train Loss 0.3981173634529114 Val Loss 1.3104220628738403
Trainable Parameters : 362628
Epoch 47 Train Acc 68.46007537841797% Val Acc 58.5% Train Loss 0.3983481824398041 Val Loss 1.0335521697998047
Trainable Parameters : 362628
Epoch 48 Train Acc 68.39923858642578% Val Acc 42.70000076293945% Train Loss 0.38992446660995483 Val Loss 1.3444874286651611
Trainable Parameters : 362628
Epoch 49 Train Acc 68.55133056640625% Val Acc 50.0% Train Loss 0.3900703191757202 Val Loss 1.1550263166427612
Trainable Parameters : 362628
Epoch 50 Train Acc 69.08744812011719% Val Acc 54.29999923706055% Train Loss 0.38709020614624023 Val Loss 1.1070575714111328
Trainable Parameters : 362628
Epoch 51 Train Acc 69.81748962402344% Val Acc 57.29999923706055% Train Loss 0.3778711259365082 Val Loss 1.0818226337432861
Trainable Parameters : 362628
Epoch 52 Train Acc 70.22433471679688% Val Acc 55.60000228881836% Train Loss 0.3773091435432434 Val Loss 1.1191526651382446
Trainable Parameters : 362628
Epoch 53 Train Acc 69.8212890625% Val Acc 56.20000076293945% Train Loss 0.38013598322868347 Val Loss 1.0665937662124634
Trainable Parameters : 362628
Epoch 54 Train Acc 70.84030151367188% Val Acc 45.900001525878906% Train Loss 0.3689349591732025 Val Loss 1.3003485202789307
Trainable Parameters : 362628
Epoch 55 Train Acc 70.8669204711914% Val Acc 53.20000076293945% Train Loss 0.36780768632888794 Val Loss 1.1469606161117554
Trainable Parameters : 362628
Epoch 56 Train Acc 70.66920471191406% Val Acc 48.20000076293945% Train Loss 0.3679274916648865 Val Loss 1.4294302463531494
Trainable Parameters : 362628
Epoch 57 Train Acc 70.87071990966797% Val Acc 55.400001525878906% Train Loss 0.3632202744483948 Val Loss 1.1000220775604248
Trainable Parameters : 362628
Epoch 58 Train Acc 71.205322265625% Val Acc 55.900001525878906% Train Loss 0.358867883682251 Val Loss 1.0714796781539917
Trainable Parameters : 362628
Epoch 59 Train Acc 70.90113830566406% Val Acc 49.400001525878906% Train Loss 0.3613433539867401 Val Loss 1.3208322525024414
Trainable Parameters : 362628
Epoch 60 Train Acc 72.03421783447266% Val Acc 53.60000228881836% Train Loss 0.3535957932472229 Val Loss 1.259226679801941
Trainable Parameters : 362628
Epoch 61 Train Acc 72.36121368408203% Val Acc 53.70000076293945% Train Loss 0.349938303232193 Val Loss 1.296718716621399
Trainable Parameters : 362628
Epoch 62 Train Acc 72.7490463256836% Val Acc 53.60000228881836% Train Loss 0.34734731912612915 Val Loss 1.1362117528915405
Trainable Parameters : 362628
Epoch 63 Train Acc 72.9277572631836% Val Acc 50.0% Train Loss 0.34363216161727905 Val Loss 1.2337092161178589
Trainable Parameters : 362628
Epoch 64 Train Acc 72.85931396484375% Val Acc 57.29999923706055% Train Loss 0.34050673246383667 Val Loss 1.18292236328125
Trainable Parameters : 362628
Epoch 65 Train Acc 73.39163208007812% Val Acc 45.79999923706055% Train Loss 0.3369344174861908 Val Loss 1.446970820426941
Trainable Parameters : 362628
Epoch 66 Train Acc 73.69581604003906% Val Acc 53.20000076293945% Train Loss 0.3291504681110382 Val Loss 1.2631374597549438
Trainable Parameters : 362628
Epoch 67 Train Acc 74.10646057128906% Val Acc 46.900001525878906% Train Loss 0.32831382751464844 Val Loss 1.479753851890564
Trainable Parameters : 362628
Epoch 68 Train Acc 74.43726348876953% Val Acc 53.60000228881836% Train Loss 0.32842540740966797 Val Loss 1.176445484161377
Trainable Parameters : 362628
Epoch 69 Train Acc 74.97718811035156% Val Acc 53.400001525878906% Train Loss 0.3238907754421234 Val Loss 1.2028703689575195
Trainable Parameters : 362628
Epoch 70 Train Acc 74.27376556396484% Val Acc 50.79999923706055% Train Loss 0.321735680103302 Val Loss 1.2980018854141235
Trainable Parameters : 362628
Epoch 71 Train Acc 75.15969848632812% Val Acc 55.400001525878906% Train Loss 0.31540778279304504 Val Loss 1.1545524597167969
Trainable Parameters : 362628
Epoch 72 Train Acc 75.28136444091797% Val Acc 51.400001525878906% Train Loss 0.31354740262031555 Val Loss 1.3050111532211304
Trainable Parameters : 362628
Epoch 73 Train Acc 75.5855484008789% Val Acc 49.79999923706055% Train Loss 0.30948129296302795 Val Loss 1.445371389389038
Trainable Parameters : 362628
Epoch 74 Train Acc 75.63497924804688% Val Acc 44.29999923706055% Train Loss 0.3107607364654541 Val Loss 1.4788964986801147
Trainable Parameters : 362628
Epoch 75 Train Acc 75.95817565917969% Val Acc 57.10000228881836% Train Loss 0.3076815605163574 Val Loss 1.2135223150253296
Trainable Parameters : 362628
Epoch 76 Train Acc 75.99239349365234% Val Acc 56.5% Train Loss 0.3038334250450134 Val Loss 1.1803797483444214
Trainable Parameters : 362628
Epoch 77 Train Acc 76.84790802001953% Val Acc 49.400001525878906% Train Loss 0.29603588581085205 Val Loss 1.2716807126998901
Trainable Parameters : 362628
Epoch 78 Train Acc 77.01520538330078% Val Acc 50.0% Train Loss 0.2954992949962616 Val Loss 1.3694425821304321
Trainable Parameters : 362628
Epoch 79 Train Acc 77.7908706665039% Val Acc 48.10000228881836% Train Loss 0.2863442599773407 Val Loss 1.5871498584747314
Trainable Parameters : 362628
Epoch 80 Train Acc 77.24714660644531% Val Acc 45.10000228881836% Train Loss 0.28872478008270264 Val Loss 1.5797680616378784
Trainable Parameters : 362628
Epoch 81 Train Acc 77.84030151367188% Val Acc 52.29999923706055% Train Loss 0.28489670157432556 Val Loss 1.2810863256454468
Trainable Parameters : 362628
Epoch 82 Train Acc 78.12928009033203% Val Acc 50.20000076293945% Train Loss 0.27894583344459534 Val Loss 1.525107741355896
Trainable Parameters : 362628
Epoch 83 Train Acc 77.58174896240234% Val Acc 55.0% Train Loss 0.28304556012153625 Val Loss 1.2611850500106812
Trainable Parameters : 362628
Epoch 84 Train Acc 78.31558990478516% Val Acc 55.20000076293945% Train Loss 0.27531126141548157 Val Loss 1.2204283475875854
Trainable Parameters : 362628
Epoch 85 Train Acc 78.8212890625% Val Acc 56.900001525878906% Train Loss 0.27365925908088684 Val Loss 1.1830157041549683
Trainable Parameters : 362628
Epoch 86 Train Acc 79.23194122314453% Val Acc 47.79999923706055% Train Loss 0.26775243878364563 Val Loss 1.4881587028503418
Trainable Parameters : 362628
Epoch 87 Train Acc 78.88973236083984% Val Acc 50.79999923706055% Train Loss 0.27000099420547485 Val Loss 1.3849252462387085
Trainable Parameters : 362628
Epoch 88 Train Acc 79.17110443115234% Val Acc 55.70000076293945% Train Loss 0.2651928961277008 Val Loss 1.2521003484725952
Trainable Parameters : 362628
Epoch 89 Train Acc 80.01140594482422% Val Acc 46.10000228881836% Train Loss 0.25664934515953064 Val Loss 1.4750933647155762
Trainable Parameters : 362628
Epoch 90 Train Acc 80.08744812011719% Val Acc 49.900001525878906% Train Loss 0.2561763823032379 Val Loss 1.4141534566879272
Trainable Parameters : 362628
Epoch 91 Train Acc 80.10646057128906% Val Acc 44.70000076293945% Train Loss 0.25636959075927734 Val Loss 1.6991046667099
Trainable Parameters : 362628
Epoch 92 Train Acc 80.71482849121094% Val Acc 49.29999923706055% Train Loss 0.25012364983558655 Val Loss 1.4063054323196411
Trainable Parameters : 362628
Epoch 93 Train Acc 80.59315490722656% Val Acc 46.79999923706055% Train Loss 0.24741330742835999 Val Loss 1.5377477407455444
Trainable Parameters : 362628
Epoch 94 Train Acc 81.1330795288086% Val Acc 47.10000228881836% Train Loss 0.2470245063304901 Val Loss 1.543618083000183
Trainable Parameters : 362628
Epoch 95 Train Acc 81.3384017944336% Val Acc 52.5% Train Loss 0.24169428646564484 Val Loss 1.4822351932525635
Trainable Parameters : 362628
Epoch 96 Train Acc 81.6882095336914% Val Acc 50.20000076293945% Train Loss 0.23732666671276093 Val Loss 1.4357730150222778
Trainable Parameters : 362628
Epoch 97 Train Acc 81.41064453125% Val Acc 53.70000076293945% Train Loss 0.23917672038078308 Val Loss 1.3938339948654175
Trainable Parameters : 362628
Epoch 98 Train Acc 81.70342254638672% Val Acc 47.10000228881836% Train Loss 0.23781374096870422 Val Loss 1.6254339218139648
Trainable Parameters : 362628
Epoch 99 Train Acc 82.2357406616211% Val Acc 50.900001525878906% Train Loss 0.22999680042266846 Val Loss 1.6018145084381104

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:54.0% Loss:1.5804390907287598
CONFUSION MATRIX
[[70  5 23  2]
 [ 7 27 54 12]
 [12  4 80  2]
 [ 7  3 53 37]]
CONFUSION MATRIX NORMALISED
[[0.1758794  0.01256281 0.05778894 0.00502513]
 [0.01758794 0.0678392  0.13567839 0.03015075]
 [0.03015075 0.01005025 0.20100503 0.00502513]
 [0.01758794 0.00753769 0.13316583 0.09296482]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.73      0.70      0.71       100
           1       0.69      0.27      0.39       100
           2       0.38      0.82      0.52        98
           3       0.70      0.37      0.48       100

    accuracy                           0.54       398
   macro avg       0.63      0.54      0.53       398
weighted avg       0.63      0.54      0.53       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 15/11/2022 19:06:44
