Wed Nov 16 20:01:26 AEDT 2022
2022-11-16 20:01:29.824558: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 20:01:30.369733: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-16 20:01:30.542904: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 20:01:32.652338: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 20:01:32.654400: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 20:01:32.654411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_100f.py
Started: 16/11/2022 20:01:46

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-100f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_100f
train_filename: u_train_100f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_100f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_100f_local/ADI17-xlsr-araic-100f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_100f_local/ADI17-xlsr-araic-100f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Traceback (most recent call last):
  File "run_xlsr_100f.py", line 378, in <module>
    traincustomdata = CustomDataset(
  File "/home/z5208494/thesis/customData.py", line 35, in __init__
    self.data_frame = pd.read_csv(csv_fp, delimiter=',')
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File data/u_train_100f.csv does not exist: 'data/u_train_100f.csv'
Wed Nov 16 20:07:11 AEDT 2022
2022-11-16 20:07:12.664090: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 20:07:12.879950: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-16 20:07:12.917136: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 20:07:14.605395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 20:07:14.605511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 20:07:14.605521: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_100f.py
Started: 16/11/2022 20:07:27

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-100f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_100f
train_filename: u_train_100f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_100f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_100f_local/ADI17-xlsr-araic-100f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_100f_local/ADI17-xlsr-araic-100f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.4561, -0.3706, -0.2938,  ...,  0.0197,  0.0174,  0.0151],
        [ 0.2559,  0.2141,  0.2813,  ...,  0.0000,  0.0000,  0.0000],
        [-4.6847, -4.3403, -3.6533,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0689, -0.1829, -0.2441,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1401, -0.2697, -0.0242,  ..., -0.2190,  1.4007,  2.9339],
        [-0.4100, -0.5598, -0.8174,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 2, 0, 3, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 3, 3, 0, 3, 3, 2, 2, 1, 2,
        1, 0, 0, 2, 0, 1, 3, 3, 3, 1, 0, 1, 2, 1, 2, 2])}
Training DataCustom Files: 359
Training Data Files: 9
Val Data Sample
{'input_values': tensor([[-0.0681, -0.0729, -0.0729,  ...,  0.0000,  0.0000,  0.0000],
        [-1.5179, -1.5640, -1.5801,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.5517,  1.7796,  1.6579,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 2.8429,  2.1728,  2.5481,  ..., -0.0690,  0.0302,  0.0134],
        [ 0.6176,  0.6483,  0.7632,  ..., -1.3106, -2.2296, -1.9005],
        [-1.9484,  1.2036, -2.5983,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 3, 3, 0, 2, 3, 1, 1, 2, 1, 3, 2, 2, 0, 2, 3, 3, 2, 1, 2, 1, 2, 3, 1,
        3, 0, 2, 3, 2, 3, 1, 3, 2, 0, 2, 1, 2, 2, 2, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'classifier.bias', 'projector.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-8.1710e-01, -1.2460e+00, -1.7753e+00,  ...,  2.9472e-01,
          4.9951e-01,  7.3134e-01],
        [ 4.5896e-02,  4.8088e-02,  4.8911e-02,  ..., -3.6846e-01,
         -4.0957e-01, -4.4794e-01],
        [-4.0290e-03,  3.8870e-02,  8.3138e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.0231e-03, -2.4461e-02, -3.0118e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.6378e-02,  7.5526e-02,  9.3909e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5680e+00,  1.5692e+00,  1.4990e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 3, 1, 1, 1, 2, 2, 2, 3, 2, 0, 3, 3, 3, 2, 3, 3, 3, 2, 1, 1, 2, 2,
        0, 2, 2, 1, 3, 2, 1, 1, 1, 1, 1, 0, 3, 2, 2, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 0 Train Acc 18.77777862548828% Val Acc 21.5% Train Loss 0.7010706067085266 Val Loss 1.3992342948913574
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 1 Train Acc 20.77777862548828% Val Acc 22.5% Train Loss 0.7004418969154358 Val Loss 1.400101661682129
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 2 Train Acc 21.55555534362793% Val Acc 22.399999618530273% Train Loss 0.7004197239875793 Val Loss 1.3910239934921265
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 3 Train Acc 18.88888931274414% Val Acc 22.5% Train Loss 0.7010270357131958 Val Loss 1.3935655355453491
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 4 Train Acc 20.44444465637207% Val Acc 23.5% Train Loss 0.6995410919189453 Val Loss 1.3880062103271484
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 5 Train Acc 22.44444465637207% Val Acc 24.0% Train Loss 0.6986804604530334 Val Loss 1.3938740491867065
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 6 Train Acc 22.66666603088379% Val Acc 21.700000762939453% Train Loss 0.6994612216949463 Val Loss 1.391821265220642
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 7 Train Acc 22.33333396911621% Val Acc 21.200000762939453% Train Loss 0.69776850938797 Val Loss 1.3904972076416016
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 8 Train Acc 23.22222328186035% Val Acc 24.100000381469727% Train Loss 0.6966963410377502 Val Loss 1.3870759010314941
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 9 Train Acc 24.0% Val Acc 22.700000762939453% Train Loss 0.6962094306945801 Val Loss 1.3898848295211792
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 10 Train Acc 25.44444465637207% Val Acc 25.0% Train Loss 0.6957215070724487 Val Loss 1.3884460926055908
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 11 Train Acc 26.66666603088379% Val Acc 26.700000762939453% Train Loss 0.6941527128219604 Val Loss 1.3891594409942627
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 12 Train Acc 27.11111068725586% Val Acc 22.100000381469727% Train Loss 0.6939066648483276 Val Loss 1.3941141366958618
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 13 Train Acc 27.88888931274414% Val Acc 23.30000114440918% Train Loss 0.6923829317092896 Val Loss 1.390637755393982
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 14 Train Acc 26.66666603088379% Val Acc 24.700000762939453% Train Loss 0.692369282245636 Val Loss 1.3903224468231201
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 15 Train Acc 27.88888931274414% Val Acc 25.899999618530273% Train Loss 0.6911653280258179 Val Loss 1.383592128753662
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 16 Train Acc 28.33333396911621% Val Acc 25.5% Train Loss 0.6904548406600952 Val Loss 1.38616943359375
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 17 Train Acc 28.33333396911621% Val Acc 27.600000381469727% Train Loss 0.6890312433242798 Val Loss 1.3816019296646118
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 18 Train Acc 28.0% Val Acc 28.0% Train Loss 0.6879301071166992 Val Loss 1.3817085027694702
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 19 Train Acc 28.11111068725586% Val Acc 26.80000114440918% Train Loss 0.6863018870353699 Val Loss 1.3833900690078735
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 20 Train Acc 28.77777862548828% Val Acc 25.899999618530273% Train Loss 0.6853640079498291 Val Loss 1.3868753910064697
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 21 Train Acc 29.33333396911621% Val Acc 25.0% Train Loss 0.6844379305839539 Val Loss 1.3903003931045532
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 22 Train Acc 30.0% Val Acc 26.100000381469727% Train Loss 0.6833414435386658 Val Loss 1.385832667350769
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 23 Train Acc 30.55555534362793% Val Acc 26.700000762939453% Train Loss 0.6828524470329285 Val Loss 1.3822628259658813
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 24 Train Acc 31.88888931274414% Val Acc 23.600000381469727% Train Loss 0.6811977028846741 Val Loss 1.3957957029342651
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 25 Train Acc 30.88888931274414% Val Acc 25.100000381469727% Train Loss 0.6803936958312988 Val Loss 1.3833612203598022
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 26 Train Acc 34.88888931274414% Val Acc 27.30000114440918% Train Loss 0.677689790725708 Val Loss 1.386128306388855
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 27 Train Acc 35.66666793823242% Val Acc 27.200000762939453% Train Loss 0.6762089729309082 Val Loss 1.3863496780395508
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 28 Train Acc 37.88888931274414% Val Acc 26.30000114440918% Train Loss 0.675072968006134 Val Loss 1.3889964818954468
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 29 Train Acc 38.88888931274414% Val Acc 29.399999618530273% Train Loss 0.6745507121086121 Val Loss 1.3839001655578613
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 30 Train Acc 38.4444465637207% Val Acc 29.0% Train Loss 0.6732662320137024 Val Loss 1.3883552551269531
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 31 Train Acc 39.88888931274414% Val Acc 29.899999618530273% Train Loss 0.6692343354225159 Val Loss 1.3916529417037964
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 32 Train Acc 42.88888931274414% Val Acc 34.0% Train Loss 0.666185200214386 Val Loss 1.3811016082763672
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 33 Train Acc 41.22222137451172% Val Acc 32.60000228881836% Train Loss 0.6658012866973877 Val Loss 1.3703142404556274
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 34 Train Acc 44.88888931274414% Val Acc 31.100000381469727% Train Loss 0.6594580411911011 Val Loss 1.3857191801071167
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 35 Train Acc 48.33333206176758% Val Acc 29.700000762939453% Train Loss 0.6540704965591431 Val Loss 1.3835185766220093
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 36 Train Acc 49.22222137451172% Val Acc 32.900001525878906% Train Loss 0.6488754749298096 Val Loss 1.3762097358703613
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 37 Train Acc 52.55555725097656% Val Acc 29.700000762939453% Train Loss 0.6400258541107178 Val Loss 1.3708597421646118
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 38 Train Acc 54.55555725097656% Val Acc 34.79999923706055% Train Loss 0.6298133134841919 Val Loss 1.3615241050720215
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 39 Train Acc 52.88888931274414% Val Acc 34.79999923706055% Train Loss 0.6166011095046997 Val Loss 1.344649314880371
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 40 Train Acc 55.88888931274414% Val Acc 36.0% Train Loss 0.601828396320343 Val Loss 1.3621947765350342
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 41 Train Acc 59.88888931274414% Val Acc 37.60000228881836% Train Loss 0.5938065648078918 Val Loss 1.368273138999939
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 42 Train Acc 59.4444465637207% Val Acc 38.79999923706055% Train Loss 0.5622620582580566 Val Loss 1.3453969955444336
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 43 Train Acc 62.0% Val Acc 40.70000076293945% Train Loss 0.5339487195014954 Val Loss 1.3612221479415894
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 44 Train Acc 64.8888931274414% Val Acc 39.10000228881836% Train Loss 0.516242504119873 Val Loss 1.378278136253357
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 45 Train Acc 65.8888931274414% Val Acc 40.400001525878906% Train Loss 0.4934955835342407 Val Loss 1.4065150022506714
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 46 Train Acc 66.22222137451172% Val Acc 40.70000076293945% Train Loss 0.4749049246311188 Val Loss 1.4201879501342773
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 47 Train Acc 66.8888931274414% Val Acc 41.20000076293945% Train Loss 0.46404823660850525 Val Loss 1.4076029062271118
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 48 Train Acc 72.77777862548828% Val Acc 42.70000076293945% Train Loss 0.42146411538124084 Val Loss 1.4217227697372437
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Epoch 49 Train Acc 74.66666412353516% Val Acc 35.70000076293945% Train Loss 0.3960956931114197 Val Loss 1.654496192932129
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 50 Train Acc 74.22222137451172% Val Acc 41.70000076293945% Train Loss 0.3818160891532898 Val Loss 1.4774531126022339
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 51 Train Acc 78.0% Val Acc 39.10000228881836% Train Loss 0.3468106985092163 Val Loss 1.6263607740402222
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 52 Train Acc 79.33333587646484% Val Acc 39.5% Train Loss 0.34146735072135925 Val Loss 1.6789684295654297
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 53 Train Acc 82.8888931274414% Val Acc 40.900001525878906% Train Loss 0.2918721139431 Val Loss 1.5472930669784546
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 54 Train Acc 83.55555725097656% Val Acc 40.20000076293945% Train Loss 0.2700580060482025 Val Loss 1.6410764455795288
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 55 Train Acc 81.55555725097656% Val Acc 42.10000228881836% Train Loss 0.2710915803909302 Val Loss 1.5139806270599365
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 56 Train Acc 87.44444274902344% Val Acc 38.20000076293945% Train Loss 0.2386253923177719 Val Loss 1.8054399490356445
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 57 Train Acc 87.77777862548828% Val Acc 40.400001525878906% Train Loss 0.21544955670833588 Val Loss 1.843072533607483
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 58 Train Acc 89.8888931274414% Val Acc 44.5% Train Loss 0.191142275929451 Val Loss 1.661341667175293
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 59 Train Acc 90.77777862548828% Val Acc 40.0% Train Loss 0.17129257321357727 Val Loss 1.7419700622558594
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 60 Train Acc 92.66666412353516% Val Acc 40.400001525878906% Train Loss 0.15461638569831848 Val Loss 1.7669686079025269
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 61 Train Acc 90.8888931274414% Val Acc 40.70000076293945% Train Loss 0.14961667358875275 Val Loss 1.8808462619781494
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 62 Train Acc 92.22222137451172% Val Acc 39.400001525878906% Train Loss 0.14637742936611176 Val Loss 2.25123929977417
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 63 Train Acc 95.77777862548828% Val Acc 42.10000228881836% Train Loss 0.09921514987945557 Val Loss 2.0461580753326416
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 64 Train Acc 97.8888931274414% Val Acc 45.10000228881836% Train Loss 0.08126930147409439 Val Loss 1.9810243844985962
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 65 Train Acc 96.44444274902344% Val Acc 43.10000228881836% Train Loss 0.08769736438989639 Val Loss 2.1826674938201904
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 66 Train Acc 95.66666412353516% Val Acc 43.10000228881836% Train Loss 0.08050525188446045 Val Loss 2.167398691177368
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 67 Train Acc 95.33333587646484% Val Acc 41.29999923706055% Train Loss 0.06686091423034668 Val Loss 2.229074001312256
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 68 Train Acc 97.33333587646484% Val Acc 44.60000228881836% Train Loss 0.0701269805431366 Val Loss 2.1467108726501465
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 69 Train Acc 98.8888931274414% Val Acc 39.10000228881836% Train Loss 0.0516178198158741 Val Loss 2.4791107177734375
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 70 Train Acc 97.55555725097656% Val Acc 47.10000228881836% Train Loss 0.052826251834630966 Val Loss 2.1661441326141357
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 71 Train Acc 99.77777862548828% Val Acc 39.70000076293945% Train Loss 0.028567250818014145 Val Loss 2.595548391342163
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 72 Train Acc 100.0% Val Acc 40.0% Train Loss 0.022527320310473442 Val Loss 2.6968932151794434
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 73 Train Acc 98.33333587646484% Val Acc 47.0% Train Loss 0.04468001797795296 Val Loss 2.416156530380249
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 74 Train Acc 99.77777862548828% Val Acc 42.20000076293945% Train Loss 0.01790764182806015 Val Loss 2.6957509517669678
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 75 Train Acc 99.55555725097656% Val Acc 42.70000076293945% Train Loss 0.02373158559203148 Val Loss 2.5950145721435547
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 76 Train Acc 98.8888931274414% Val Acc 38.70000076293945% Train Loss 0.03631279617547989 Val Loss 2.786759614944458
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 77 Train Acc 98.0% Val Acc 41.60000228881836% Train Loss 0.03840002790093422 Val Loss 2.750702381134033
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 78 Train Acc 98.8888931274414% Val Acc 45.900001525878906% Train Loss 0.03440912440419197 Val Loss 2.4598042964935303
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 79 Train Acc 99.44444274902344% Val Acc 42.70000076293945% Train Loss 0.023084107786417007 Val Loss 2.8243050575256348
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 80 Train Acc 99.0% Val Acc 43.10000228881836% Train Loss 0.025904493406414986 Val Loss 2.8643031120300293
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 81 Train Acc 99.44444274902344% Val Acc 46.0% Train Loss 0.01904391311109066 Val Loss 2.6927621364593506
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 82 Train Acc 99.44444274902344% Val Acc 45.400001525878906% Train Loss 0.026082012802362442 Val Loss 2.7035083770751953
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 83 Train Acc 99.0% Val Acc 43.79999923706055% Train Loss 0.02716858685016632 Val Loss 2.8271777629852295
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 84 Train Acc 94.11111450195312% Val Acc 46.5% Train Loss 0.08010170608758926 Val Loss 2.7383995056152344
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 85 Train Acc 98.55555725097656% Val Acc 42.29999923706055% Train Loss 0.03092770464718342 Val Loss 3.2078731060028076
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 86 Train Acc 99.22222137451172% Val Acc 41.60000228881836% Train Loss 0.01872493140399456 Val Loss 3.0232222080230713
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 87 Train Acc 99.55555725097656% Val Acc 46.60000228881836% Train Loss 0.015077793039381504 Val Loss 2.834075927734375
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 88 Train Acc 98.22222137451172% Val Acc 43.60000228881836% Train Loss 0.02935035713016987 Val Loss 2.873014450073242
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 89 Train Acc 99.0% Val Acc 47.400001525878906% Train Loss 0.018926842138171196 Val Loss 2.761500120162964
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 90 Train Acc 98.77777862548828% Val Acc 45.79999923706055% Train Loss 0.025289176031947136 Val Loss 2.841181755065918
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 91 Train Acc 100.0% Val Acc 43.29999923706055% Train Loss 0.005316161084920168 Val Loss 3.1720354557037354
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 92 Train Acc 100.0% Val Acc 48.79999923706055% Train Loss 0.007974875159561634 Val Loss 2.8488104343414307
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 93 Train Acc 99.77777862548828% Val Acc 42.70000076293945% Train Loss 0.012126009911298752 Val Loss 3.075364828109741
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 94 Train Acc 98.44444274902344% Val Acc 40.900001525878906% Train Loss 0.026091819629073143 Val Loss 3.2003815174102783
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 95 Train Acc 99.55555725097656% Val Acc 45.900001525878906% Train Loss 0.012060427106916904 Val Loss 3.1102283000946045
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 96 Train Acc 99.33333587646484% Val Acc 43.5% Train Loss 0.01934860087931156 Val Loss 3.15590238571167
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 97 Train Acc 100.0% Val Acc 42.70000076293945% Train Loss 0.005236884579062462 Val Loss 3.3026459217071533
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 98 Train Acc 98.66666412353516% Val Acc 40.70000076293945% Train Loss 0.022709954530000687 Val Loss 3.495387315750122
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Configuration saved in /srv/scratch/z5208494/output/u_train_100f_local/ADI17-xlsr-araic-100f/config.json
Model weights saved in /srv/scratch/z5208494/output/u_train_100f_local/ADI17-xlsr-araic-100f/pytorch_model.bin
Epoch 99 Train Acc 99.77777862548828% Val Acc 50.400001525878906% Train Loss 0.009864628314971924 Val Loss 2.938767194747925

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:45.10000228881836% Loss:2.8374993801116943
CONFUSION MATRIX
[[52 20 16 12]
 [ 9 18 19 54]
 [15  9 57 17]
 [ 6 11 30 53]]
CONFUSION MATRIX NORMALISED
[[0.13065327 0.05025126 0.04020101 0.03015075]
 [0.02261307 0.04522613 0.04773869 0.13567839]
 [0.03768844 0.02261307 0.14321608 0.04271357]
 [0.01507538 0.02763819 0.07537688 0.13316583]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.63      0.52      0.57       100
           1       0.31      0.18      0.23       100
           2       0.47      0.58      0.52        98
           3       0.39      0.53      0.45       100

    accuracy                           0.45       398
   macro avg       0.45      0.45      0.44       398
weighted avg       0.45      0.45      0.44       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 16/11/2022 20:46:06
