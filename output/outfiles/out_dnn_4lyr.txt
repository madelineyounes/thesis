Fri Nov 4 01:20:03 AEDT 2022
python3: can't open file 'run_dnn_downstream_4layer.py': [Errno 2] No such file or directory
Fri Nov 4 23:33:29 AEDT 2022
2022-11-04 23:33:30.338622: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-04 23:33:30.550273: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-04 23:33:30.584280: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-04 23:33:31.924343: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 23:33:31.926462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 23:33:31.926472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Traceback (most recent call last):
  File "run_dnn_downstream_4layers.py", line 24, in <module>
    import customTransform as T
ModuleNotFoundError: No module named 'customTransform'
Fri Nov 4 23:37:39 AEDT 2022
2022-11-04 23:37:40.418134: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-04 23:37:40.645036: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-04 23:37:40.679402: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-04 23:37:42.020983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 23:37:42.022664: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 23:37:42.022674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_dnn_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_dnn_downstream_4layers.py
Started: 04/11/2022 23:37:53

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-dnn-4layer
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-dnn-4layer
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-dnn-4layer_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 1.8118,  1.7818,  1.3628,  ...,  0.5335,  0.8016,  0.9645],
        [-0.0637, -0.0192, -0.0743,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0179,  0.1180,  0.1107,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.8853,  0.6901,  0.6587,  ...,  0.1799, -0.0733,  0.7166],
        [-0.9872, -0.8397, -0.6578,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.7980,  0.7886,  0.7281,  ...,  0.1792,  0.0822,  0.0300]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 2, 2, 3, 0, 0, 2, 3, 0, 2, 2, 0, 2, 0, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3,
        0, 1, 1, 3, 0, 3, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.0745,  0.0996,  0.1281,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2370,  0.2915,  0.3367,  ...,  0.0000,  0.0000,  0.0000],
        [ 3.0469,  2.9617,  2.6460,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 2.2133,  2.0151,  1.9453,  ...,  0.0000,  0.0000,  0.0000],
        [-1.1756, -1.1705, -1.1413,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.6716,  1.0834,  0.6854,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 0, 3, 0, 1, 1, 2, 2, 2, 1, 3, 3, 1, 1,
        2, 1, 3, 2, 0, 3, 1, 0, 3, 0, 3, 1, 0, 1, 0, 3])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'classifier.bias', 'projector.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0101, -0.0128, -0.0156,  ...,  0.0000,  0.0000,  0.0000],
        [-0.6345, -0.5269, -0.8659,  ...,  0.5175,  0.4372,  0.4278],
        [-0.0621, -0.0449, -0.0356,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.2213, -0.2763, -0.3021,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2262,  0.1218,  0.1374,  ...,  0.6414,  0.9592,  1.0641],
        [-0.4978, -0.6588, -0.4798,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 0, 1, 3, 3, 0, 2, 2, 0, 2, 2, 2, 0, 1, 0, 0, 0, 1, 1, 2, 1, 0, 0,
        3, 0, 2, 2, 3, 3, 3, 0, 2, 1, 0, 3, 2, 1, 3, 3])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 366372
Epoch 0 Train Acc 26.68060874938965% Val Acc 26.30000114440918% Train Loss 0.6830058097839355 Val Loss 1.3823970556259155
Trainable Parameters : 366372
Epoch 1 Train Acc 41.55893325805664% Val Acc 26.200000762939453% Train Loss 0.6626214385032654 Val Loss 1.375527262687683
Trainable Parameters : 366372
Epoch 2 Train Acc 43.13688278198242% Val Acc 39.0% Train Loss 0.6095916628837585 Val Loss 1.3250778913497925
Trainable Parameters : 366372
Epoch 3 Train Acc 56.8441047668457% Val Acc 37.60000228881836% Train Loss 0.5429465174674988 Val Loss 1.3049027919769287
Trainable Parameters : 366372
Epoch 4 Train Acc 61.615970611572266% Val Acc 48.900001525878906% Train Loss 0.488472580909729 Val Loss 1.2564669847488403
Trainable Parameters : 366372
Epoch 5 Train Acc 63.92015075683594% Val Acc 51.60000228881836% Train Loss 0.46076539158821106 Val Loss 1.180510401725769
Trainable Parameters : 366372
Epoch 6 Train Acc 63.992393493652344% Val Acc 53.5% Train Loss 0.44902554154396057 Val Loss 1.172646403312683
Trainable Parameters : 366372
Epoch 7 Train Acc 64.91254425048828% Val Acc 46.29999923706055% Train Loss 0.4403046667575836 Val Loss 1.2190039157867432
Trainable Parameters : 366372
Epoch 8 Train Acc 65.15209197998047% Val Acc 54.29999923706055% Train Loss 0.43800798058509827 Val Loss 1.097633719444275
Trainable Parameters : 366372
Epoch 9 Train Acc 65.3079833984375% Val Acc 52.29999923706055% Train Loss 0.43694376945495605 Val Loss 1.1648597717285156
Trainable Parameters : 366372
Epoch 10 Train Acc 65.32319641113281% Val Acc 52.29999923706055% Train Loss 0.4363521635532379 Val Loss 1.1907161474227905
Trainable Parameters : 366372
Epoch 11 Train Acc 64.31938934326172% Val Acc 50.5% Train Loss 0.4397351145744324 Val Loss 1.2992920875549316
Trainable Parameters : 366372
Epoch 12 Train Acc 64.96197509765625% Val Acc 46.400001525878906% Train Loss 0.4392149746417999 Val Loss 1.318424105644226
Trainable Parameters : 366372
Epoch 13 Train Acc 65.2775650024414% Val Acc 55.29999923706055% Train Loss 0.43709075450897217 Val Loss 1.0715928077697754
Trainable Parameters : 366372
Epoch 14 Train Acc 65.23194122314453% Val Acc 55.5% Train Loss 0.43559959530830383 Val Loss 1.1206220388412476
Trainable Parameters : 366372
Epoch 15 Train Acc 65.2509536743164% Val Acc 51.29999923706055% Train Loss 0.43409180641174316 Val Loss 1.2078139781951904
Trainable Parameters : 366372
Epoch 16 Train Acc 65.05703735351562% Val Acc 52.400001525878906% Train Loss 0.4374243915081024 Val Loss 1.28908109664917
Trainable Parameters : 366372
Epoch 17 Train Acc 64.615966796875% Val Acc 52.70000076293945% Train Loss 0.4380227327346802 Val Loss 1.1751031875610352
Trainable Parameters : 366372
Epoch 18 Train Acc 65.3802261352539% Val Acc 55.29999923706055% Train Loss 0.43517687916755676 Val Loss 1.1487843990325928
Trainable Parameters : 366372
Epoch 19 Train Acc 64.63878631591797% Val Acc 49.60000228881836% Train Loss 0.4391389787197113 Val Loss 1.2115654945373535
Trainable Parameters : 366372
Epoch 20 Train Acc 64.8935317993164% Val Acc 56.79999923706055% Train Loss 0.4365803301334381 Val Loss 1.08181893825531
Trainable Parameters : 366372
Epoch 21 Train Acc 64.77566528320312% Val Acc 48.900001525878906% Train Loss 0.43609175086021423 Val Loss 1.194358229637146
Trainable Parameters : 366372
Epoch 22 Train Acc 64.95817565917969% Val Acc 55.10000228881836% Train Loss 0.4367626905441284 Val Loss 1.1417115926742554
Trainable Parameters : 366372
Epoch 23 Train Acc 64.63497924804688% Val Acc 56.400001525878906% Train Loss 0.43946707248687744 Val Loss 1.0689997673034668
Trainable Parameters : 366372
Epoch 24 Train Acc 65.14068603515625% Val Acc 51.900001525878906% Train Loss 0.43432700634002686 Val Loss 1.1259273290634155
Trainable Parameters : 366372
Epoch 25 Train Acc 64.8669204711914% Val Acc 57.10000228881836% Train Loss 0.4349871575832367 Val Loss 1.0499409437179565
Trainable Parameters : 366372
Epoch 26 Train Acc 64.94296264648438% Val Acc 52.900001525878906% Train Loss 0.43452391028404236 Val Loss 1.1521762609481812
Trainable Parameters : 366372
Epoch 27 Train Acc 65.47148132324219% Val Acc 51.29999923706055% Train Loss 0.4312654137611389 Val Loss 1.2099316120147705
Trainable Parameters : 366372
Epoch 28 Train Acc 65.32699584960938% Val Acc 55.900001525878906% Train Loss 0.43418943881988525 Val Loss 1.1801528930664062
Trainable Parameters : 366372
Epoch 29 Train Acc 65.31938934326172% Val Acc 48.900001525878906% Train Loss 0.43053117394447327 Val Loss 1.2995232343673706
Trainable Parameters : 366372
Epoch 30 Train Acc 65.19771575927734% Val Acc 51.20000076293945% Train Loss 0.4314110577106476 Val Loss 1.150314211845398
Trainable Parameters : 366372
Epoch 31 Train Acc 65.62357330322266% Val Acc 54.900001525878906% Train Loss 0.427203506231308 Val Loss 1.0888108015060425
Trainable Parameters : 366372
Epoch 32 Train Acc 65.28897094726562% Val Acc 49.29999923706055% Train Loss 0.4292590320110321 Val Loss 1.3234498500823975
Trainable Parameters : 366372
Epoch 33 Train Acc 65.58935546875% Val Acc 49.60000228881836% Train Loss 0.43024182319641113 Val Loss 1.3398245573043823
Trainable Parameters : 366372
Epoch 34 Train Acc 65.66920471191406% Val Acc 54.29999923706055% Train Loss 0.42615726590156555 Val Loss 1.1004114151000977
Trainable Parameters : 366372
Epoch 35 Train Acc 65.3536148071289% Val Acc 47.20000076293945% Train Loss 0.4283774197101593 Val Loss 1.3352998495101929
Trainable Parameters : 366372
Epoch 36 Train Acc 66.08745574951172% Val Acc 54.900001525878906% Train Loss 0.4239763021469116 Val Loss 1.159224033355713
Trainable Parameters : 366372
Epoch 37 Train Acc 65.62737274169922% Val Acc 56.400001525878906% Train Loss 0.42640063166618347 Val Loss 1.074337363243103
Trainable Parameters : 366372
Epoch 38 Train Acc 65.9277572631836% Val Acc 53.70000076293945% Train Loss 0.42454296350479126 Val Loss 1.186038851737976
Trainable Parameters : 366372
Epoch 39 Train Acc 65.4144515991211% Val Acc 44.5% Train Loss 0.42290088534355164 Val Loss 1.2982312440872192
Trainable Parameters : 366372
Epoch 40 Train Acc 65.64258575439453% Val Acc 53.20000076293945% Train Loss 0.42250558733940125 Val Loss 1.0985963344573975
Trainable Parameters : 366372
Epoch 41 Train Acc 66.47908782958984% Val Acc 50.900001525878906% Train Loss 0.4168536067008972 Val Loss 1.303449273109436
Trainable Parameters : 366372
Epoch 42 Train Acc 66.69581604003906% Val Acc 55.0% Train Loss 0.41276267170906067 Val Loss 1.142931342124939
Trainable Parameters : 366372
Epoch 43 Train Acc 66.84410858154297% Val Acc 45.400001525878906% Train Loss 0.4100581407546997 Val Loss 1.2966474294662476
Trainable Parameters : 366372
Epoch 44 Train Acc 67.59315490722656% Val Acc 47.20000076293945% Train Loss 0.4069700837135315 Val Loss 1.3135721683502197
Trainable Parameters : 366372
Epoch 45 Train Acc 67.95056915283203% Val Acc 58.10000228881836% Train Loss 0.3990299701690674 Val Loss 0.9949862360954285
Trainable Parameters : 366372
Epoch 46 Train Acc 67.45246887207031% Val Acc 44.60000228881836% Train Loss 0.3999404013156891 Val Loss 1.416256070137024
Trainable Parameters : 366372
Epoch 47 Train Acc 68.26235961914062% Val Acc 55.79999923706055% Train Loss 0.3949861228466034 Val Loss 1.0270545482635498
Trainable Parameters : 366372
Epoch 48 Train Acc 68.93155670166016% Val Acc 42.900001525878906% Train Loss 0.3899156153202057 Val Loss 1.4067515134811401
Trainable Parameters : 366372
Epoch 49 Train Acc 69.13687896728516% Val Acc 48.29999923706055% Train Loss 0.3873676657676697 Val Loss 1.2326743602752686
Trainable Parameters : 366372
Epoch 50 Train Acc 68.88593292236328% Val Acc 52.900001525878906% Train Loss 0.38622474670410156 Val Loss 1.164754867553711
Trainable Parameters : 366372
Epoch 51 Train Acc 69.45246887207031% Val Acc 50.20000076293945% Train Loss 0.3817020356655121 Val Loss 1.3056707382202148
Trainable Parameters : 366372
Epoch 52 Train Acc 69.01901245117188% Val Acc 49.400001525878906% Train Loss 0.38153913617134094 Val Loss 1.2802633047103882
Trainable Parameters : 366372
Epoch 53 Train Acc 70.43345642089844% Val Acc 54.29999923706055% Train Loss 0.3757403492927551 Val Loss 1.0952262878417969
Trainable Parameters : 366372
Epoch 54 Train Acc 70.205322265625% Val Acc 44.70000076293945% Train Loss 0.37492823600769043 Val Loss 1.539414405822754
Trainable Parameters : 366372
Epoch 55 Train Acc 70.54753112792969% Val Acc 53.20000076293945% Train Loss 0.37250816822052 Val Loss 1.272059679031372
Trainable Parameters : 366372
Epoch 56 Train Acc 71.03802490234375% Val Acc 48.900001525878906% Train Loss 0.3656732141971588 Val Loss 1.4093265533447266
Trainable Parameters : 366372
Epoch 57 Train Acc 71.06463623046875% Val Acc 48.400001525878906% Train Loss 0.36176344752311707 Val Loss 1.171017050743103
Trainable Parameters : 366372
Epoch 58 Train Acc 70.88593292236328% Val Acc 52.0% Train Loss 0.3642081916332245 Val Loss 1.1446284055709839
Trainable Parameters : 366372
Epoch 59 Train Acc 71.36502075195312% Val Acc 52.70000076293945% Train Loss 0.3592779040336609 Val Loss 1.1672180891036987
Trainable Parameters : 366372
Epoch 60 Train Acc 72.15969848632812% Val Acc 53.29999923706055% Train Loss 0.3560097813606262 Val Loss 1.1989314556121826
Trainable Parameters : 366372
Epoch 61 Train Acc 72.36502075195312% Val Acc 50.70000076293945% Train Loss 0.34752288460731506 Val Loss 1.2031904458999634
Trainable Parameters : 366372
Epoch 62 Train Acc 72.64258575439453% Val Acc 55.70000076293945% Train Loss 0.3432040810585022 Val Loss 1.1326534748077393
Trainable Parameters : 366372
Epoch 63 Train Acc 73.06083679199219% Val Acc 50.0% Train Loss 0.3439064025878906 Val Loss 1.2238181829452515
Trainable Parameters : 366372
Epoch 64 Train Acc 73.07604217529297% Val Acc 56.5% Train Loss 0.34231069684028625 Val Loss 1.2334870100021362
Trainable Parameters : 366372
Epoch 65 Train Acc 73.66539764404297% Val Acc 47.400001525878906% Train Loss 0.3356170058250427 Val Loss 1.4054603576660156
Trainable Parameters : 366372
Epoch 66 Train Acc 73.70722198486328% Val Acc 50.29999923706055% Train Loss 0.3328876793384552 Val Loss 1.2459986209869385
Trainable Parameters : 366372
Epoch 67 Train Acc 74.00760650634766% Val Acc 48.79999923706055% Train Loss 0.3339276611804962 Val Loss 1.416245698928833
Trainable Parameters : 366372
Epoch 68 Train Acc 74.68441009521484% Val Acc 53.79999923706055% Train Loss 0.3254399299621582 Val Loss 1.1528314352035522
Trainable Parameters : 366372
Epoch 69 Train Acc 74.06083679199219% Val Acc 52.5% Train Loss 0.32765981554985046 Val Loss 1.150583267211914
Trainable Parameters : 366372
Epoch 70 Train Acc 75.3536148071289% Val Acc 51.29999923706055% Train Loss 0.32220885157585144 Val Loss 1.2597101926803589
Trainable Parameters : 366372
Epoch 71 Train Acc 75.63117980957031% Val Acc 57.20000076293945% Train Loss 0.3119550943374634 Val Loss 1.1085113286972046
Trainable Parameters : 366372
Epoch 72 Train Acc 74.77946472167969% Val Acc 51.79999923706055% Train Loss 0.3159426748752594 Val Loss 1.2879737615585327
Trainable Parameters : 366372
Epoch 73 Train Acc 75.49429321289062% Val Acc 54.20000076293945% Train Loss 0.3159678280353546 Val Loss 1.2954267263412476
Trainable Parameters : 366372
Epoch 74 Train Acc 75.54753112792969% Val Acc 44.400001525878906% Train Loss 0.31120604276657104 Val Loss 1.415290117263794
Trainable Parameters : 366372
Epoch 75 Train Acc 75.90494537353516% Val Acc 53.900001525878906% Train Loss 0.30454906821250916 Val Loss 1.1580009460449219
Trainable Parameters : 366372
Epoch 76 Train Acc 76.14448547363281% Val Acc 55.0% Train Loss 0.3014867603778839 Val Loss 1.275086760520935
Trainable Parameters : 366372
Epoch 77 Train Acc 76.8973388671875% Val Acc 48.400001525878906% Train Loss 0.294026255607605 Val Loss 1.3388938903808594
Trainable Parameters : 366372
Epoch 78 Train Acc 77.07604217529297% Val Acc 53.79999923706055% Train Loss 0.29333072900772095 Val Loss 1.2716655731201172
Trainable Parameters : 366372
Epoch 79 Train Acc 77.34220123291016% Val Acc 52.5% Train Loss 0.28922075033187866 Val Loss 1.4439743757247925
Trainable Parameters : 366372
Epoch 80 Train Acc 77.63117980957031% Val Acc 48.29999923706055% Train Loss 0.28891652822494507 Val Loss 1.3954050540924072
Trainable Parameters : 366372
Epoch 81 Train Acc 77.57414245605469% Val Acc 48.79999923706055% Train Loss 0.28755566477775574 Val Loss 1.5979832410812378
Trainable Parameters : 366372
Epoch 82 Train Acc 78.6920166015625% Val Acc 56.5% Train Loss 0.27778899669647217 Val Loss 1.358014464378357
Trainable Parameters : 366372
Epoch 83 Train Acc 77.21672821044922% Val Acc 48.20000076293945% Train Loss 0.2824283242225647 Val Loss 1.374481201171875
Trainable Parameters : 366372
Epoch 84 Train Acc 78.8517074584961% Val Acc 50.29999923706055% Train Loss 0.27473169565200806 Val Loss 1.4740278720855713
Trainable Parameters : 366372
Epoch 85 Train Acc 78.91254425048828% Val Acc 56.0% Train Loss 0.27092307806015015 Val Loss 1.3571995496749878
Trainable Parameters : 366372
Epoch 86 Train Acc 78.8973388671875% Val Acc 51.60000228881836% Train Loss 0.2716910243034363 Val Loss 1.3994934558868408
Trainable Parameters : 366372
Epoch 87 Train Acc 79.28136444091797% Val Acc 43.400001525878906% Train Loss 0.27063634991645813 Val Loss 1.6738439798355103
Trainable Parameters : 366372
Epoch 88 Train Acc 79.93916320800781% Val Acc 47.10000228881836% Train Loss 0.2609947621822357 Val Loss 1.5315532684326172
Trainable Parameters : 366372
Epoch 89 Train Acc 79.28897094726562% Val Acc 40.60000228881836% Train Loss 0.2611466348171234 Val Loss 1.9992914199829102
Trainable Parameters : 366372
Epoch 90 Train Acc 80.5855484008789% Val Acc 49.0% Train Loss 0.25280672311782837 Val Loss 1.770429253578186
Trainable Parameters : 366372
Epoch 91 Train Acc 80.31938934326172% Val Acc 48.5% Train Loss 0.2539694011211395 Val Loss 1.6396795511245728
Trainable Parameters : 366372
Epoch 92 Train Acc 79.93916320800781% Val Acc 52.10000228881836% Train Loss 0.25412610173225403 Val Loss 1.4019123315811157
Trainable Parameters : 366372
Epoch 93 Train Acc 80.70342254638672% Val Acc 46.10000228881836% Train Loss 0.24931292235851288 Val Loss 1.827561378479004
Trainable Parameters : 366372
Epoch 94 Train Acc 81.11786651611328% Val Acc 51.400001525878906% Train Loss 0.2459055632352829 Val Loss 1.3498204946517944
Trainable Parameters : 366372
Epoch 95 Train Acc 81.50189971923828% Val Acc 54.10000228881836% Train Loss 0.24181914329528809 Val Loss 1.6360715627670288
Trainable Parameters : 366372
Epoch 96 Train Acc 81.65779113769531% Val Acc 49.10000228881836% Train Loss 0.239091157913208 Val Loss 1.6165412664413452
Trainable Parameters : 366372
Epoch 97 Train Acc 81.46007537841797% Val Acc 52.20000076293945% Train Loss 0.23850524425506592 Val Loss 1.4751626253128052
Trainable Parameters : 366372
Epoch 98 Train Acc 80.9695816040039% Val Acc 49.60000228881836% Train Loss 0.24317529797554016 Val Loss 1.47395920753479
Trainable Parameters : 366372
Epoch 99 Train Acc 81.85551452636719% Val Acc 51.29999923706055% Train Loss 0.23377329111099243 Val Loss 1.4260331392288208

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:53.79999923706055% Loss:1.3924593925476074
CONFUSION MATRIX
[[62  5 28  5]
 [ 4 21 51 24]
 [ 8  2 84  4]
 [14  2 37 47]]
CONFUSION MATRIX NORMALISED
[[0.15577889 0.01256281 0.07035176 0.01256281]
 [0.01005025 0.05276382 0.1281407  0.06030151]
 [0.0201005  0.00502513 0.21105528 0.01005025]
 [0.03517588 0.00502513 0.09296482 0.11809045]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.70      0.62      0.66       100
           1       0.70      0.21      0.32       100
           2       0.42      0.86      0.56        98
           3       0.59      0.47      0.52       100

    accuracy                           0.54       398
   macro avg       0.60      0.54      0.52       398
weighted avg       0.60      0.54      0.52       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 05/11/2022 04:23:22
