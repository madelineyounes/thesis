Tue Nov 8 01:42:34 AEDT 2022
2022-11-08 01:42:36.691399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-08 01:42:36.975706: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-08 01:42:39.183854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-08 01:42:39.184002: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-08 01:42:39.184017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_unfreezex2EGY.py
Started: 08/11/2022 01:42:52

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-unfreeze-x2EGY
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_x2EGY
train_filename: u_train_x2EGY
validation_filename: dev_u_x2EGY
evaluation_filename: test_u_x2EGY
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_x2EGY.csv
--> data_test_fp: data/dev_u_x2EGY.csv
--> data_test_fp: data/test_u_x2EGY.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_x2EGY_local/ADI17-xlsr-araic-unfreeze-x2EGY
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_x2EGY_local/ADI17-xlsr-araic-unfreeze-x2EGY_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Traceback (most recent call last):
  File "run_xlsr_unfreezex2EGY.py", line 378, in <module>
    traincustomdata = CustomDataset(
  File "/home/z5208494/thesis/customData.py", line 35, in __init__
    self.data_frame = pd.read_csv(csv_fp, delimiter=',')
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File data/u_train_x2EGY.csv does not exist: 'data/u_train_x2EGY.csv'
Tue Nov 8 16:09:11 AEDT 2022
2022-11-08 16:09:13.711882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-08 16:09:14.113786: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-08 16:09:14.248113: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-08 16:09:16.163731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-08 16:09:16.165690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-08 16:09:16.165703: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_unfreezex2EGY.py
Started: 08/11/2022 16:09:30

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-unfreeze-x2EGY
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: train_u_x2EGY
train_filename: train_u_x2EGY
validation_filename: dev_u_x2EGY
evaluation_filename: test_u_x2EGY
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/train_u_x2EGY.csv
--> data_test_fp: data/dev_u_x2EGY.csv
--> data_test_fp: data/test_u_x2EGY.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/train_u_x2EGY_local/ADI17-xlsr-araic-unfreeze-x2EGY
--> finetuned_results_fp: /srv/scratch/z5208494/output/train_u_x2EGY_local/ADI17-xlsr-araic-unfreeze-x2EGY_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.0381, -0.1900, -0.4016,  ..., -0.1712, -0.2360, -0.1989],
        [ 0.0633,  0.0499,  0.0499,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2452, -0.3305, -0.3297,  ..., -1.8061, -1.8210, -1.7768],
        ...,
        [ 0.7714,  0.6863,  0.5462,  ..., -1.7615, -2.2998, -2.4077],
        [ 0.0550,  0.0327, -0.0106,  ...,  0.2948,  0.3029,  0.2930],
        [ 1.5479,  1.1124,  0.9767,  ..., -0.4324, -0.4449, -0.4614]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([2, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 0, 1, 0,
        0, 2, 1, 1, 2, 1, 1, 3, 3, 1, 3, 3, 1, 1, 2, 2])}
Training DataCustom Files: 3225
Training Data Files: 81
Val Data Sample
{'input_values': tensor([[-0.2721, -0.3029, -0.5326,  ...,  0.0000,  0.0000,  0.0000],
        [-1.0155, -0.6306,  0.2885,  ..., -0.6799, -0.5109, -0.3512],
        [-0.3041, -0.2508, -0.3184,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0524,  0.0046, -0.1674,  ..., -0.3514, -0.3691, -0.3372],
        [-0.2400, -1.5097, -2.0742,  ...,  1.1386,  1.1433,  1.1516],
        [ 0.5364,  0.5469,  0.3394,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 3, 1, 2, 2, 1, 3, 1, 1, 0, 1, 1, 1,
        0, 1, 3, 0, 2, 2, 1, 1, 2, 1, 1, 0, 2, 0, 2, 1])}
Test CustomData Files: 698
Test Data Files: 18
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'classifier.bias', 'projector.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-2.0487e-03,  2.0163e-02,  5.1259e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.9709e-01, -4.3145e-01, -1.0923e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.4970e-02,  5.5538e-02,  5.2133e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 4.1578e-01,  4.1646e-01,  4.2599e-01,  ..., -7.8096e-01,
         -7.9321e-01, -8.3065e-01],
        [ 4.3204e+00,  3.3976e+00,  2.1972e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.2618e-01,  1.2179e-01,  1.3744e-01,  ...,  6.4140e-01,
          9.5924e-01,  1.0641e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 2, 1, 0, 1, 1, 1, 3, 3, 2, 2, 1, 2, 2, 1, 3, 3, 0, 1, 1, 3, 2, 3, 1,
        3, 0, 3, 1, 1, 1, 2, 1, 2, 1, 1, 3, 2, 0, 1, 3])}
Test CustomData Files: 498
Test Data Files: 13
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 0 Train Acc 19.962963104248047% Val Acc 46.38461685180664% Train Loss 0.6946250200271606 Val Loss 1.3619511127471924
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 1 Train Acc 42.345680236816406% Val Acc 58.30769348144531% Train Loss 0.6741116642951965 Val Loss 1.2549974918365479
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 2 Train Acc 43.4444465637207% Val Acc 57.230770111083984% Train Loss 0.6522347331047058 Val Loss 1.180037498474121
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 3 Train Acc 43.55555725097656% Val Acc 60.076927185058594% Train Loss 0.6392467021942139 Val Loss 1.1240347623825073
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 4 Train Acc 45.7283935546875% Val Acc 61.38461685180664% Train Loss 0.6092687845230103 Val Loss 1.0493444204330444
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 5 Train Acc 52.71604919433594% Val Acc 62.46154022216797% Train Loss 0.5632701516151428 Val Loss 0.9065427184104919
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 6 Train Acc 56.71604919433594% Val Acc 62.46154022216797% Train Loss 0.5179829597473145 Val Loss 0.9109762907028198
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 7 Train Acc 61.38271713256836% Val Acc 64.46154022216797% Train Loss 0.46822866797447205 Val Loss 0.941135048866272
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 8 Train Acc 67.81481170654297% Val Acc 68.53846740722656% Train Loss 0.41804251074790955 Val Loss 0.8132815957069397
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 9 Train Acc 71.81481170654297% Val Acc 68.84615325927734% Train Loss 0.37338337302207947 Val Loss 0.7778517007827759
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 10 Train Acc 77.74073791503906% Val Acc 71.92308044433594% Train Loss 0.3059810400009155 Val Loss 0.8183569312095642
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 11 Train Acc 81.13580322265625% Val Acc 77.46154022216797% Train Loss 0.259035587310791 Val Loss 0.7267875671386719
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 12 Train Acc 85.93827056884766% Val Acc 77.69231414794922% Train Loss 0.20943666994571686 Val Loss 0.7363347411155701
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 13 Train Acc 89.53086853027344% Val Acc 80.92308044433594% Train Loss 0.16027887165546417 Val Loss 0.6722515821456909
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 14 Train Acc 90.35802459716797% Val Acc 75.46154022216797% Train Loss 0.14418759942054749 Val Loss 0.7691594362258911
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 15 Train Acc 92.87654113769531% Val Acc 74.69231414794922% Train Loss 0.11321016401052475 Val Loss 0.9176902174949646
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 16 Train Acc 94.18518829345703% Val Acc 79.76923370361328% Train Loss 0.08962936699390411 Val Loss 0.7190597653388977
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 17 Train Acc 95.48148345947266% Val Acc 75.69231414794922% Train Loss 0.07382214814424515 Val Loss 0.975368320941925
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 18 Train Acc 95.29629516601562% Val Acc 76.76923370361328% Train Loss 0.06937505304813385 Val Loss 1.0693303346633911
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 19 Train Acc 95.9259262084961% Val Acc 78.15384674072266% Train Loss 0.060692016035318375 Val Loss 0.9242302179336548
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 20 Train Acc 95.79012298583984% Val Acc 78.0% Train Loss 0.057963188737630844 Val Loss 0.8926655054092407
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 21 Train Acc 97.35802459716797% Val Acc 76.61538696289062% Train Loss 0.0413818396627903 Val Loss 1.1025844812393188
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 22 Train Acc 95.66666412353516% Val Acc 76.0% Train Loss 0.06252434104681015 Val Loss 1.1218128204345703
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 23 Train Acc 96.29629516601562% Val Acc 77.30769348144531% Train Loss 0.05550110712647438 Val Loss 0.9720242023468018
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 24 Train Acc 97.0740737915039% Val Acc 80.15384674072266% Train Loss 0.043625302612781525 Val Loss 0.9440387487411499
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 25 Train Acc 97.44444274902344% Val Acc 75.15384674072266% Train Loss 0.04066414758563042 Val Loss 1.2795425653457642
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 26 Train Acc 97.543212890625% Val Acc 76.61538696289062% Train Loss 0.038273364305496216 Val Loss 1.166448712348938
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 27 Train Acc 97.6172866821289% Val Acc 78.0769271850586% Train Loss 0.035953398793935776 Val Loss 1.0079063177108765
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 28 Train Acc 98.4691390991211% Val Acc 77.53846740722656% Train Loss 0.027297312393784523 Val Loss 1.0580346584320068
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 29 Train Acc 97.76543426513672% Val Acc 78.84615325927734% Train Loss 0.03638414293527603 Val Loss 1.0853623151779175
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 30 Train Acc 97.8148193359375% Val Acc 80.69231414794922% Train Loss 0.034201059490442276 Val Loss 0.9085510969161987
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 31 Train Acc 98.28395080566406% Val Acc 78.15384674072266% Train Loss 0.02838575653731823 Val Loss 1.1735085248947144
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 32 Train Acc 97.76543426513672% Val Acc 74.23077392578125% Train Loss 0.03698640689253807 Val Loss 1.5662529468536377
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 33 Train Acc 97.56790161132812% Val Acc 79.69231414794922% Train Loss 0.0410856157541275 Val Loss 1.0533950328826904
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 34 Train Acc 97.95061492919922% Val Acc 77.0769271850586% Train Loss 0.036957718431949615 Val Loss 0.9915324449539185
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 35 Train Acc 97.96296691894531% Val Acc 71.3846206665039% Train Loss 0.03692455217242241 Val Loss 1.4481090307235718
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 36 Train Acc 97.30863952636719% Val Acc 78.53846740722656% Train Loss 0.039212316274642944 Val Loss 1.2321990728378296
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 37 Train Acc 98.8148193359375% Val Acc 76.69231414794922% Train Loss 0.019863193854689598 Val Loss 1.2273858785629272
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 38 Train Acc 99.1975326538086% Val Acc 81.53846740722656% Train Loss 0.013306455686688423 Val Loss 1.0195239782333374
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 39 Train Acc 97.93827056884766% Val Acc 76.76923370361328% Train Loss 0.02965623140335083 Val Loss 1.3607734441757202
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 40 Train Acc 98.66666412353516% Val Acc 80.69231414794922% Train Loss 0.02332334779202938 Val Loss 1.1472980976104736
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 41 Train Acc 98.25926208496094% Val Acc 79.23077392578125% Train Loss 0.025829918682575226 Val Loss 1.095202088356018
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 42 Train Acc 98.6172866821289% Val Acc 75.0% Train Loss 0.025264156982302666 Val Loss 1.394244909286499
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 43 Train Acc 97.82716369628906% Val Acc 77.61538696289062% Train Loss 0.03560859337449074 Val Loss 1.0646874904632568
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 44 Train Acc 98.13580322265625% Val Acc 78.23077392578125% Train Loss 0.03099016845226288 Val Loss 1.0963786840438843
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 45 Train Acc 98.93827056884766% Val Acc 80.3846206665039% Train Loss 0.019391579553484917 Val Loss 1.1826436519622803
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 46 Train Acc 98.64197540283203% Val Acc 76.23077392578125% Train Loss 0.02609606832265854 Val Loss 1.318718433380127
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 47 Train Acc 99.11111450195312% Val Acc 73.61538696289062% Train Loss 0.0170279573649168 Val Loss 1.6574819087982178
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 48 Train Acc 98.1975326538086% Val Acc 78.0% Train Loss 0.0312239658087492 Val Loss 1.158151388168335
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Epoch 49 Train Acc 97.33333587646484% Val Acc 71.46154022216797% Train Loss 0.03895990550518036 Val Loss 1.428230881690979
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 50 Train Acc 98.30863952636719% Val Acc 77.84615325927734% Train Loss 0.025695668533444405 Val Loss 1.34966242313385
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 51 Train Acc 97.44444274902344% Val Acc 72.84615325927734% Train Loss 0.040811434388160706 Val Loss 1.5541207790374756
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 52 Train Acc 98.30863952636719% Val Acc 77.61538696289062% Train Loss 0.027294857427477837 Val Loss 1.163394808769226
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 53 Train Acc 98.86419677734375% Val Acc 74.15384674072266% Train Loss 0.019245080649852753 Val Loss 1.367668628692627
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 54 Train Acc 98.02468872070312% Val Acc 76.61538696289062% Train Loss 0.034723203629255295 Val Loss 1.3632150888442993
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 55 Train Acc 98.67901611328125% Val Acc 78.30769348144531% Train Loss 0.022116975858807564 Val Loss 1.173832893371582
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 56 Train Acc 97.90123748779297% Val Acc 77.0769271850586% Train Loss 0.033123571425676346 Val Loss 1.212021827697754
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 57 Train Acc 97.55555725097656% Val Acc 79.0769271850586% Train Loss 0.03731669485569 Val Loss 0.9364316463470459
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 58 Train Acc 97.28395080566406% Val Acc 77.3846206665039% Train Loss 0.039594314992427826 Val Loss 1.264548659324646
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 59 Train Acc 98.62963104248047% Val Acc 81.15384674072266% Train Loss 0.022251609712839127 Val Loss 0.9359676241874695
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 60 Train Acc 98.8148193359375% Val Acc 77.3846206665039% Train Loss 0.019460881128907204 Val Loss 1.1216480731964111
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 61 Train Acc 98.67901611328125% Val Acc 78.84615325927734% Train Loss 0.023229120299220085 Val Loss 1.1956695318222046
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 62 Train Acc 98.8888931274414% Val Acc 76.69231414794922% Train Loss 0.02255202829837799 Val Loss 1.2738779783248901
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 63 Train Acc 98.75308990478516% Val Acc 75.61538696289062% Train Loss 0.02256382443010807 Val Loss 1.289838433265686
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 64 Train Acc 97.55555725097656% Val Acc 79.0769271850586% Train Loss 0.03838889300823212 Val Loss 1.1339945793151855
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 65 Train Acc 98.3827133178711% Val Acc 77.76923370361328% Train Loss 0.028170092031359673 Val Loss 1.4292536973953247
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 66 Train Acc 97.49382781982422% Val Acc 80.69231414794922% Train Loss 0.037255145609378815 Val Loss 1.0250324010849
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 67 Train Acc 98.49382781982422% Val Acc 77.30769348144531% Train Loss 0.022859584540128708 Val Loss 1.4754667282104492
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 68 Train Acc 98.40740966796875% Val Acc 76.15384674072266% Train Loss 0.024680906906723976 Val Loss 1.4361940622329712
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 69 Train Acc 98.56790161132812% Val Acc 78.30769348144531% Train Loss 0.019917113706469536 Val Loss 1.3422023057937622
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 70 Train Acc 98.58024597167969% Val Acc 74.84615325927734% Train Loss 0.02446686290204525 Val Loss 1.6417988538742065
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 71 Train Acc 98.75308990478516% Val Acc 69.84615325927734% Train Loss 0.023273926228284836 Val Loss 1.5781539678573608
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 72 Train Acc 97.09876251220703% Val Acc 79.92308044433594% Train Loss 0.0427103228867054 Val Loss 0.9614034295082092
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 73 Train Acc 98.75308990478516% Val Acc 78.69231414794922% Train Loss 0.02374609187245369 Val Loss 1.0145050287246704
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 74 Train Acc 98.86419677734375% Val Acc 77.23077392578125% Train Loss 0.018715059384703636 Val Loss 1.2606096267700195
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 75 Train Acc 98.14814758300781% Val Acc 69.92308044433594% Train Loss 0.03442445769906044 Val Loss 1.730130910873413
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 76 Train Acc 97.9259262084961% Val Acc 76.61538696289062% Train Loss 0.030694017186760902 Val Loss 1.0199142694473267
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 77 Train Acc 98.91358184814453% Val Acc 69.3846206665039% Train Loss 0.020447174087166786 Val Loss 1.6684728860855103
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 78 Train Acc 99.03704071044922% Val Acc 76.3846206665039% Train Loss 0.017305446788668633 Val Loss 1.1231608390808105
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 79 Train Acc 98.69136047363281% Val Acc 72.53846740722656% Train Loss 0.027426322922110558 Val Loss 1.499640941619873
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 80 Train Acc 98.11111450195312% Val Acc 77.23077392578125% Train Loss 0.03042190708220005 Val Loss 1.237648606300354
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 81 Train Acc 97.90123748779297% Val Acc 73.23077392578125% Train Loss 0.031478580087423325 Val Loss 1.4905648231506348
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 82 Train Acc 96.8148193359375% Val Acc 77.53846740722656% Train Loss 0.05116647481918335 Val Loss 1.1517598628997803
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 83 Train Acc 98.6543197631836% Val Acc 75.46154022216797% Train Loss 0.021378446370363235 Val Loss 1.199533462524414
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 84 Train Acc 98.456787109375% Val Acc 74.23077392578125% Train Loss 0.02269655093550682 Val Loss 1.9929018020629883
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 85 Train Acc 97.55555725097656% Val Acc 74.46154022216797% Train Loss 0.04147530719637871 Val Loss 1.2438300848007202
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 86 Train Acc 98.6172866821289% Val Acc 76.84615325927734% Train Loss 0.024105343967676163 Val Loss 1.3834123611450195
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 87 Train Acc 97.62963104248047% Val Acc 79.3846206665039% Train Loss 0.03953234851360321 Val Loss 1.0060561895370483
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 88 Train Acc 98.17284393310547% Val Acc 78.84615325927734% Train Loss 0.025863921269774437 Val Loss 1.0207481384277344
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 89 Train Acc 97.87654113769531% Val Acc 75.69231414794922% Train Loss 0.03337812051177025 Val Loss 1.1937999725341797
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 90 Train Acc 98.4691390991211% Val Acc 78.3846206665039% Train Loss 0.025873031467199326 Val Loss 1.031728982925415
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 91 Train Acc 98.91358184814453% Val Acc 75.46154022216797% Train Loss 0.019291995093226433 Val Loss 1.3702678680419922
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 92 Train Acc 98.67901611328125% Val Acc 70.61538696289062% Train Loss 0.023274626582860947 Val Loss 1.7573435306549072
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 93 Train Acc 97.16049194335938% Val Acc 74.61538696289062% Train Loss 0.04571586102247238 Val Loss 1.1531729698181152
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 94 Train Acc 98.3827133178711% Val Acc 79.69231414794922% Train Loss 0.024614907801151276 Val Loss 1.1318553686141968
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 95 Train Acc 98.16049194335938% Val Acc 68.76923370361328% Train Loss 0.02762068435549736 Val Loss 1.8687379360198975
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 96 Train Acc 97.79012298583984% Val Acc 74.23077392578125% Train Loss 0.032846640795469284 Val Loss 1.620194911956787
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 97 Train Acc 99.79012298583984% Val Acc 76.53846740722656% Train Loss 0.005098642315715551 Val Loss 1.618512749671936
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 98 Train Acc 98.08641815185547% Val Acc 76.15384674072266% Train Loss 0.02993152104318142 Val Loss 1.1723250150680542
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Configuration saved in /srv/scratch/z5208494/output/train_u_x2EGY_local/ADI17-xlsr-araic-unfreeze-x2EGY/config.json
Model weights saved in /srv/scratch/z5208494/output/train_u_x2EGY_local/ADI17-xlsr-araic-unfreeze-x2EGY/pytorch_model.bin
Epoch 99 Train Acc 98.0740737915039% Val Acc 79.15384674072266% Train Loss 0.03572344779968262 Val Loss 1.0516544580459595

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:70.3846206665039% Loss:1.3579813241958618
CONFUSION MATRIX
[[ 61  13  20   6]
 [  4 145  23  28]
 [ 12   4  71  11]
 [  1   9  19  71]]
CONFUSION MATRIX NORMALISED
[[0.12248996 0.02610442 0.04016064 0.01204819]
 [0.00803213 0.29116466 0.04618474 0.0562249 ]
 [0.02409639 0.00803213 0.14257028 0.02208835]
 [0.00200803 0.01807229 0.03815261 0.14257028]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.78      0.61      0.69       100
           1       0.85      0.72      0.78       200
           2       0.53      0.72      0.61        98
           3       0.61      0.71      0.66       100

    accuracy                           0.70       498
   macro avg       0.69      0.69      0.68       498
weighted avg       0.73      0.70      0.70       498


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 08/11/2022 19:58:51
