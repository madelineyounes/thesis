Sat Oct 29 10:18:50 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 10:19:04

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.2246, -0.0031, -0.1920,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1225, -0.2788, -0.1839,  ...,  0.4644, -0.1015, -0.5092],
        [ 0.3899,  0.6884,  0.4334,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.4374,  0.8121,  1.2092,  ...,  0.1827, -0.1444, -0.3226],
        [-0.5623, -0.4542, -0.6336,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1456, -0.1480, -0.1280,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 3, 2, 1, 2, 2, 2, 3, 2, 3, 1, 2, 2, 3, 0, 3, 0, 2, 2, 2, 3, 2, 1,
        2, 0, 2, 2, 3, 2, 0, 2, 3, 3, 2, 0, 2, 1, 1, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.1210,  0.1561,  0.1701,  ..., -1.2012, -1.3359, -1.4308],
        [-0.0879, -0.1471, -0.1475,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2107,  0.1798,  0.1503,  ...,  0.0141, -0.0196, -0.0468],
        ...,
        [-0.3438, -0.2937, -0.2367,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0691, -0.0121, -0.0800,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0064, -0.0030, -0.0439,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 3, 0, 3, 0, 0, 1, 3, 1, 2, 2, 1, 3, 1, 1, 1, 1, 2, 1, 3, 3, 3, 3, 3,
        0, 1, 1, 0, 2, 1, 2, 2, 3, 1, 0, 0, 3, 0, 3, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0885, -0.0729, -0.0600,  ...,  0.2091,  0.3746,  0.4556],
        [-0.8915, -0.7216,  0.5245,  ...,  0.2021,  0.1842,  0.1600],
        [-0.1185, -0.1481, -0.1487,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.4790, -0.0252,  0.3645,  ...,  0.0000,  0.0000,  0.0000],
        [-0.7581, -0.7194, -1.0606,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1198, -0.1511,  0.0263,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 1, 1, 0, 3, 3, 1, 0, 0, 3, 1, 0, 1, 2, 3, 1, 1, 3, 0, 2, 0, 0, 3,
        3, 0, 0, 3, 3, 1, 1, 2, 0, 0, 3, 2, 3, 1, 3, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 289108
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 734, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 551, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 577, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 619, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
TypeError: Caught TypeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
TypeError: linear(): argument 'input' (position 1) must be Tensor, not tuple

Sat Oct 29 10:27:28 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 10:27:42

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 2.8124,  4.1014,  3.5403,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0433, -0.0331, -0.0324,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4910, -0.5682, -0.4621,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.3635, -0.3416, -0.2272,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4198, -0.0139,  0.0742,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4509,  0.4612,  0.4781,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 0, 1, 3, 3, 1, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 1, 2,
        1, 1, 1, 0, 3, 2, 3, 3, 0, 3, 3, 2, 2, 2, 3, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.6348,  0.2865, -0.0239,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0660, -0.0687, -0.0644,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1467, -0.1402, -0.0991,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.1722,  0.1740,  0.1728,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4383,  0.4300,  0.4082,  ...,  0.3136,  0.3431,  0.3237],
        [-0.0729, -0.0565, -0.0443,  ..., -1.8699, -1.8500, -1.7305]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 3, 1, 0, 2, 1, 2, 3, 0, 1, 1, 3, 3, 3, 2, 0, 0, 1, 1, 2, 0, 1, 2, 0,
        2, 3, 0, 0, 1, 1, 0, 0, 2, 0, 1, 1, 2, 3, 1, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.weight', 'projector.bias', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.0859,  0.0620,  0.0274,  ...,  0.0000,  0.0000,  0.0000],
        [ 2.5927,  2.6432,  2.6870,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1714,  0.2282,  0.1530,  ...,  3.0038,  3.4317,  3.0729],
        ...,
        [ 0.1632,  0.1285,  0.1211,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.7373,  1.5380,  1.3582,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1393, -0.1396, -0.1493,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 2, 1, 0, 1, 2, 2, 3, 2, 0, 1, 2, 0, 3, 2, 3, 3, 1, 1, 3, 2, 2, 1,
        2, 1, 2, 1, 3, 1, 2, 3, 0, 1, 1, 3, 3, 1, 1, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 354900
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 735, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 552, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 578, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 620, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
TypeError: Caught TypeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
TypeError: linear(): argument 'input' (position 1) must be Tensor, not tuple

Sat Oct 29 10:42:34 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 10:42:49

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.3715,  0.1551,  0.5073,  ...,  0.0000,  0.0000,  0.0000],
        [-1.9889, -2.3575, -2.8732,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0275, -0.0223, -0.0190,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.2508, -0.3681, -0.4020,  ..., -1.1997, -1.0813, -1.0372],
        [ 0.1215, -0.1033,  0.0603,  ...,  3.9817,  3.9817,  3.4965],
        [-0.1100,  0.0725,  0.1221,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 2, 0, 1, 2, 3, 0, 3, 3, 2, 1, 3, 0, 3, 0, 0, 3, 1, 3, 2, 2, 2, 2,
        3, 2, 2, 1, 2, 2, 0, 2, 3, 3, 3, 0, 3, 3, 2, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.0021,  0.0083, -0.0025,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0212, -0.0645, -0.0885,  ..., -0.3442, -0.3365, -0.3628],
        [-1.1756, -1.1705, -1.1413,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0478,  0.1835,  0.3545,  ..., -0.5811, -0.9328, -1.2595],
        [ 0.0888,  0.0105,  0.0833,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.5569,  0.8791,  0.1861,  ..., -0.1111,  0.0242,  0.0095]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 3, 0, 1, 2, 2, 1, 3, 1, 2, 0, 3, 0, 2, 1, 3, 2, 2, 3, 2, 2, 3, 0, 3,
        0, 0, 3, 2, 0, 1, 0, 3, 2, 1, 0, 2, 2, 0, 2, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.bias', 'classifier.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-2.0487e-03,  2.0163e-02,  5.1259e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.6204e-03, -3.2540e-03,  2.5449e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9076e+00, -1.8135e+00, -1.7409e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.4433e-02,  1.0952e-01,  9.7480e-02,  ...,  7.1586e-01,
         -8.2141e-01, -1.8090e-01],
        [ 1.4947e+00,  5.3999e-01, -3.4544e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3122e-02,  3.5537e-02,  2.5192e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 3, 0, 1, 2, 3, 2, 0, 1, 1, 1, 3, 0, 1, 2, 1, 2, 3, 1, 3, 1, 2, 2,
        1, 1, 1, 0, 0, 2, 2, 1, 1, 3, 1, 1, 0, 1, 1, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 383700
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 736, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 553, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 579, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 621, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
TypeError: Caught TypeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
TypeError: dropout(): argument 'input' (position 1) must be Tensor, not tuple

Sat Oct 29 10:52:27 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 10:52:42

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0636, -0.1375, -0.1999,  ...,  1.5084, -0.0923, -0.8970],
        [ 0.3722,  0.4467,  0.4216,  ...,  0.3569,  0.5114,  0.4216],
        [-0.0265, -0.0719,  0.0937,  ...,  1.1341,  1.3127,  1.1371],
        ...,
        [-0.3106, -0.3650, -0.0519,  ...,  0.1817,  0.2607,  0.2030],
        [ 1.0203,  0.6096,  0.1190,  ...,  0.3064,  0.4433,  0.5910],
        [ 0.0217, -0.0044, -0.0391,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 1, 2, 2, 1, 0, 2, 1, 0,
        3, 2, 2, 3, 3, 1, 2, 2, 2, 2, 0, 0, 3, 1, 0, 1])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.3658,  0.7448,  1.1126,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3390, -0.3374, -0.3267,  ...,  0.0000,  0.0000,  0.0000],
        [-0.5095, -0.2174, -0.4198,  ..., -5.0045, -4.9304, -4.6827],
        ...,
        [ 0.3543,  0.3481,  0.2343,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0165, -0.0232, -0.0126,  ...,  0.0573,  0.0793,  0.0990],
        [ 3.6483,  3.7510,  3.3296,  ...,  0.2101,  0.3438,  0.4403]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([2, 3, 2, 3, 0, 0, 0, 2, 2, 0, 0, 2, 2, 1, 0, 0, 0, 0, 3, 2, 3, 3, 2, 2,
        2, 3, 0, 2, 0, 1, 3, 0, 0, 3, 3, 2, 1, 0, 1, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'input_values': tensor([[ 0.5327,  0.5681,  0.5219,  ...,  0.2447,  0.1297, -0.0158],
        [ 0.4840, -1.0241,  0.5227,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3675, -0.5567, -1.0380,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 2.5632,  2.3553,  2.2741,  ...,  1.3446,  1.1136,  0.8093],
        [ 1.5680,  1.5692,  1.4990,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0293, -0.0162, -0.0195,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 1, 0, 0, 2, 3, 3, 0, 2, 0, 3, 3, 1, 0, 2, 2, 0, 3, 0, 2, 3, 2, 0,
        3, 2, 2, 0, 0, 2, 3, 2, 0, 2, 3, 0, 2, 0, 1, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 460, in <module>
    nn.GetLSTMOutput(),
AttributeError: module 'torch.nn' has no attribute 'GetLSTMOutput'
Sat Oct 29 10:57:23 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 10:57:36

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.5212, -0.5141, -0.4801,  ..., -0.2026, -0.1790, -0.2753],
        [ 0.8971,  1.0764,  1.1304,  ...,  0.0743,  0.0188,  0.0226],
        [-0.5361, -0.4693, -0.0465,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.5659, -0.5397, -2.5055,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0629,  0.1996,  0.3766,  ..., -0.9909, -0.8448, -0.3403],
        [-0.7226, -0.6200, -0.5426,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 1, 2, 3, 1, 2, 1, 0, 0, 2, 0, 2, 0, 2, 2, 2, 2, 3, 3, 0, 2, 3, 3,
        0, 2, 1, 3, 3, 2, 2, 3, 0, 1, 3, 2, 2, 3, 1, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-2.1937, -2.2820, -2.2517,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4112, -0.4821, -0.1493,  ...,  0.0000,  0.0000,  0.0000],
        [-1.2361, -1.4040, -1.2044,  ..., -0.1842, -0.2800, -0.2748],
        ...,
        [ 3.6483,  3.7510,  3.3296,  ...,  0.2101,  0.3438,  0.4403],
        [-0.0210,  0.0525,  0.1422,  ...,  4.1841,  4.2378,  3.5857],
        [ 0.2363,  0.1882,  0.0785,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 0, 0, 0, 1, 3, 0, 3, 1, 2, 3, 1, 2, 2, 2, 2, 3, 0, 3, 3, 0, 2, 3,
        3, 3, 1, 2, 2, 1, 1, 2, 2, 1, 1, 3, 2, 0, 3, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.bias', 'classifier.weight', 'classifier.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.3915, -0.1187,  0.1507,  ...,  0.1375,  0.0724,  0.0578],
        [-0.0750, -0.0271,  0.1032,  ..., -1.0620, -0.9375, -0.7213],
        [ 0.9599,  0.9302,  1.1542,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.4575, -0.3944, -0.3409,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0670, -0.2170, -0.1622,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1005, -0.1147, -0.1313,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 2, 2, 2, 0, 3, 0, 3, 3, 1, 2, 0, 3, 0, 1, 2, 1, 1, 0, 3, 2, 1, 2, 3,
        2, 3, 2, 0, 0, 0, 1, 3, 1, 0, 2, 3, 2, 1, 3, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 383700
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 741, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 558, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 584, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 626, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (14x40 and 20x4)

Sat Oct 29 11:05:22 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 11:05:34

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-1.1333, -0.6835, -0.4012,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.3606,  0.9319,  0.5792,  ..., -0.2091, -0.1983, -0.1978],
        [ 0.4527,  0.1905,  0.0337,  ...,  1.0268,  0.6125,  0.3245],
        ...,
        [ 0.1038,  0.0511,  0.1461,  ..., -0.9118, -0.8024, -0.7479],
        [ 0.0185,  0.0245,  0.0366,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4234,  0.2015, -0.0417,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 3, 0, 3, 3, 0, 3, 1, 0, 1, 2, 2, 1, 2, 3, 0, 1, 0, 2, 1, 3, 3, 2,
        3, 2, 0, 2, 1, 3, 2, 1, 3, 3, 3, 1, 1, 1, 2, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.0111, -0.0098, -0.0266,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1413,  0.0057, -0.0289,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1711,  0.3643,  0.4009,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0063,  0.0138,  0.0143,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.7054,  0.6045,  0.6146,  ...,  0.0000,  0.0000,  0.0000],
        [ 3.0469,  2.9617,  2.6460,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 2, 0, 2, 2, 0, 1, 2, 2, 2, 1, 0, 0, 0, 3, 3, 0, 2, 0, 2, 0, 3, 1,
        2, 1, 3, 0, 2, 3, 0, 3, 2, 1, 3, 3, 0, 2, 2, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0389, -0.1059, -0.1063,  ...,  0.0000,  0.0000,  0.0000],
        [ 2.1443,  1.0528, -0.1578,  ...,  0.0000,  0.0000,  0.0000],
        [-1.2582, -1.0331, -1.3803,  ..., -1.6003, -1.3458, -1.8567],
        ...,
        [-1.6076, -0.9129, -0.6243,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1013, -0.0475, -0.0355,  ...,  0.0000,  0.0000,  0.0000],
        [-1.7172, -1.8952, -2.3181,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 3, 3, 0, 1, 0, 1, 3, 2, 2, 0, 3, 2, 2, 1, 3, 2, 1, 3, 0, 1, 1, 2,
        0, 1, 2, 0, 1, 2, 0, 3, 1, 1, 1, 1, 0, 0, 2, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 383676
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 741, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 558, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 584, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 626, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (14x40 and 14x4)

Sat Oct 29 11:12:00 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 11:12:12

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 1.5328e-01,  1.7810e-01,  1.9615e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1273e-02,  2.8813e-02,  2.8813e-02,  ...,  2.4246e-01,
          4.1489e-01,  4.4505e-01],
        [-4.3496e-03,  7.2896e-03,  5.3497e-03,  ..., -9.1643e-02,
         -9.3583e-02, -6.7072e-02],
        ...,
        [-5.0716e-02,  2.5493e-02, -1.5270e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.0612e-01,  3.7642e-01,  4.9005e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.4705e-03,  3.5572e-04,  6.4705e-03,  ..., -7.5089e-01,
         -1.0029e+00, -1.1283e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 1, 3, 2, 2, 3, 2, 3, 2, 3, 0, 2, 3, 2, 2, 1, 2, 2, 0, 3, 3, 0, 0, 0,
        2, 2, 1, 2, 3, 3, 3, 2, 1, 2, 0, 2, 0, 2, 1, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-4.0335, -4.0114, -3.6133,  ...,  0.2187,  0.1707,  0.0659],
        [ 1.6224,  2.1345,  2.2826,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.5517,  1.7796,  1.6579,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.1578,  0.1401,  0.1212,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8807, -1.0383, -1.0760,  ..., -0.7737,  2.4600,  0.0100],
        [-0.0564, -0.0848, -0.1133,  ..., -0.0689, -0.0390, -0.0446]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 3, 2, 2, 2, 0, 1, 2, 3, 1, 1, 1, 1, 2, 0, 3, 1, 3, 2, 1, 0, 2, 3,
        1, 2, 1, 0, 0, 1, 1, 1, 3, 3, 2, 1, 1, 3, 1, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 4.6696e-04,  7.6588e-04,  4.6696e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6866e-01, -3.0104e-03,  1.8249e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.2213e-01,  4.0438e-01,  3.6323e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.2434e-01, -7.6359e-01, -7.5223e-01,  ...,  4.6460e-02,
          2.1706e-01,  5.9545e-02],
        [ 6.3408e-02,  1.1205e-01,  2.2408e-02,  ...,  4.8120e-02,
          4.7425e-02,  8.7034e-02],
        [-4.0955e-02, -1.3796e-01, -4.0821e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 0, 1, 0, 1, 1, 0, 0, 3, 2, 3, 0, 2, 1, 0, 3, 1, 2, 2, 1, 0, 2, 1,
        3, 1, 0, 3, 3, 0, 3, 0, 2, 1, 1, 0, 1, 0, 0, 3])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 333564
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 742, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 559, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 585, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 627, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (14x4 and 14x4)

Sat Oct 29 12:19:52 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 12:20:06

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.2007, -0.2620, -0.2899,  ...,  2.3082,  3.3565,  3.8390],
        [-2.9655, -2.9961, -2.1499,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0447,  0.0409,  0.0438,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.1039, -0.1011, -0.1125,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1216, -0.2787, -0.6242,  ..., -2.7057, -2.7650, -2.6999],
        [ 0.1239, -0.1259, -0.2588,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 2, 3, 0, 2, 2, 0, 0, 3, 2, 1, 0, 0, 1, 3, 0, 3, 2, 2, 2, 2, 2, 2, 3,
        3, 3, 0, 3, 0, 0, 0, 0, 0, 3, 0, 3, 2, 1, 1, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.4582,  0.8991,  0.8132,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1326,  0.0588, -0.0089,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1148,  0.2213,  0.3966,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0337,  0.0651, -0.0119,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.6966,  1.2498,  1.5425,  ..., -0.0666,  0.2842,  0.1918],
        [-1.5179, -1.5640, -1.5801,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 1, 1, 2, 0, 0, 3, 1, 1, 0, 3, 3, 2, 2, 3, 3, 0, 1, 2, 1, 1, 1, 2, 0,
        1, 3, 0, 0, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1, 1, 3])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.bias', 'classifier.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.0290,  0.0415,  0.0597,  ...,  1.0335,  1.4112,  0.9908],
        [-0.0293, -0.0162, -0.0195,  ...,  0.0000,  0.0000,  0.0000],
        [-1.5990, -1.7487, -1.9265,  ...,  0.3870,  0.4586,  0.5316],
        ...,
        [-1.1798, -1.1132, -0.7150,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1714,  0.2282,  0.1530,  ...,  3.0038,  3.4317,  3.0729],
        [-0.2072, -0.2131, -0.2381,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 1, 2, 0, 3, 3, 1, 1, 0, 0, 2, 0, 2, 1, 2, 3, 3, 1, 1, 1, 3, 0, 3,
        3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 3, 2, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 263900
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 741, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 558, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 584, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 626, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 759, in forward
    self.check_forward_args(input, hx, batch_sizes)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 684, in check_forward_args
    self.check_input(input, batch_sizes)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 205, in check_input
    raise RuntimeError(
RuntimeError: input.size(-1) must be equal to input_size. Expected 14, got 256

Sat Oct 29 12:40:51 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 12:41:05

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0095, -0.0101, -0.0070,  ...,  0.0000,  0.0000,  0.0000],
        [-1.5018, -1.4892, -0.2593,  ...,  0.3187,  0.2117,  0.1801],
        [-1.7489, -2.1751, -2.0898,  ...,  1.6679,  1.8374,  1.8420],
        ...,
        [-0.1889, -0.2751, -0.2862,  ...,  0.0000,  0.0000,  0.0000],
        [ 3.1931,  2.4788,  2.0993,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0845, -0.0057,  0.0674,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 0, 3, 3, 3, 0, 0, 3, 0, 1, 2, 0, 0, 2, 2, 3, 2, 3, 1, 3, 2, 3, 0, 2,
        1, 0, 1, 2, 0, 2, 3, 2, 3, 2, 0, 0, 2, 0, 0, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 1.7630e-01,  2.3282e-01,  2.4752e-02,  ...,  4.3947e-01,
          5.0871e-01,  5.8819e-01],
        [-6.6615e-01, -6.3769e-01, -7.1849e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.9141e-01, -2.7488e-03, -2.4667e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.2040e-02, -3.0253e-02, -4.4006e-02,  ..., -1.9681e+00,
         -2.1637e+00, -2.3795e+00],
        [-2.2207e-02,  1.3199e-01,  1.0080e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.2640e-03,  2.9522e-03,  3.6974e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 2, 1, 3, 0, 2, 2, 0, 2, 2, 0, 0, 3, 0, 0, 1, 3, 1, 1, 2, 2, 3, 0, 3,
        2, 2, 1, 1, 3, 1, 3, 2, 1, 1, 3, 1, 0, 1, 0, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.bias', 'classifier.bias', 'classifier.weight', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-2.4462, -2.7787, -2.3450,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4482, -0.5700, -0.6391,  ...,  0.1453, -0.3222, -0.3616],
        [ 0.4055,  0.2585,  0.6810,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.3174, -0.2837, -0.2379,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3681, -0.1342,  0.3953,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0529, -0.0506, -0.3518,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 1, 2, 0, 3, 0, 2, 1, 3, 3, 0, 1, 0, 3, 1, 0, 2, 2, 2, 0, 1, 2, 0, 1,
        2, 1, 1, 0, 1, 3, 3, 3, 0, 1, 2, 0, 0, 1, 2, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 298876
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 741, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 558, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 584, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 626, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (14x28 and 14x4)

Sat Oct 29 13:31:09 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 29/10/2022 13:31:23

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.1725, -0.0413, -0.1354,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.9327,  2.0012,  1.9490,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0274,  0.0246,  0.0246,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.7993, -0.7888, -0.7627,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.5425,  0.6474,  0.5425,  ...,  0.0000,  0.0000,  0.0000],
        [-1.0871, -1.8060, -2.4846,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 0, 2, 3, 3, 3, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 3, 2, 3, 3, 1, 0, 3,
        0, 2, 1, 0, 2, 2, 1, 3, 0, 1, 2, 3, 3, 1, 2, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.0063, -0.1413, -0.0708,  ...,  0.9254,  0.8978,  0.8409],
        [-0.0083,  0.1104,  0.1410,  ...,  0.0000,  0.0000,  0.0000],
        [-0.6493, -0.5560, -0.3146,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 1.7058,  1.8118,  1.5666,  ..., -0.6853, -0.6341, -0.5698],
        [-0.0189, -0.0268, -0.0186,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0160, -0.0183,  0.0022,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 2, 2, 0, 2, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 3, 2, 2, 2, 1, 2, 0, 1, 2,
        3, 2, 3, 0, 3, 0, 3, 0, 0, 1, 2, 2, 1, 0, 0, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.bias', 'projector.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 1.2793,  1.2673,  1.3023,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0612,  0.0535,  0.0547,  ..., -0.2541, -0.1216,  0.4730],
        [-0.4482, -0.5700, -0.6391,  ...,  0.1453, -0.3222, -0.3616],
        ...,
        [ 0.4600, -0.0958, -0.1159,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8380, -0.8204, -0.7205,  ...,  0.0000,  0.0000,  0.0000],
        [-1.4756, -1.5934, -1.7841,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 3, 1, 3, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 3, 0, 2, 2, 2, 2,
        0, 3, 3, 0, 2, 3, 3, 0, 0, 1, 1, 2, 2, 2, 0, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 358948
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 741, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 558, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 584, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 626, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (14x80 and 40x4)

Mon Oct 31 14:01:18 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 31/10/2022 14:01:33

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.2069, -0.2053, -0.1762,  ..., -0.5782, -0.5894, -0.6537],
        [-0.0205,  0.0878, -0.0300,  ..., -0.1424, -0.2059, -0.2816],
        [ 0.3955,  0.3166,  0.3166,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0853,  0.0489, -0.1900,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1419,  0.1440,  0.0943,  ..., -1.1467, -2.1287, -2.8642],
        [-0.2657, -0.4003, -0.3563,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 3, 0, 1, 1, 2, 3, 3, 2, 0, 1, 3, 3, 1, 2, 1, 2, 0, 0, 0, 2, 2, 0, 2,
        1, 3, 2, 0, 2, 2, 1, 0, 2, 2, 0, 2, 2, 1, 0, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 1.2231,  1.2585,  1.3976,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0730, -0.0853, -0.0038,  ..., -1.7750,  0.5246, -0.8325],
        [-0.1498, -0.1333, -0.0543,  ...,  0.1821,  0.0901,  0.0821],
        ...,
        [-0.6808, -0.2606, -0.1863,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2366, -0.2593,  0.4906,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2107,  0.1798,  0.1503,  ...,  0.0141, -0.0196, -0.0468]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 0, 2, 1, 0, 1, 2, 3, 3, 1, 0, 0, 2, 0, 1, 2, 2, 0, 2, 0, 2, 3, 0, 0,
        1, 3, 1, 2, 0, 3, 2, 2, 0, 3, 3, 0, 0, 2, 0, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-5.0773e-02, -3.8584e-02, -1.7074e-02,  ..., -7.7532e-03,
          8.0206e-03,  3.3832e-02],
        [ 7.6737e-01,  3.6240e-01,  1.4657e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.4019e-01,  3.0198e-01,  1.9138e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.3884e+00,  1.0760e+00,  3.0073e-02,  ...,  4.0532e-02,
          1.0817e-01,  1.0398e-01],
        [ 1.7309e-02,  1.0059e-03, -2.0829e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0853e-01, -1.3632e-01, -2.4989e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 0, 1, 2, 2, 2, 1, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        2, 0, 0, 0, 3, 1, 2, 3, 0, 3, 0, 1, 2, 0, 0, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  2 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 358948
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 741, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 558, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 584, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 626, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (20x80 and 40x4)

Mon Oct 31 16:18:16 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 31/10/2022 16:18:33

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.8708, -1.0387, -1.4957,  ..., -0.1100, -0.2482, -0.0506],
        [ 0.6313,  1.0023,  1.1001,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0689, -0.1829, -0.2441,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.6651, -0.7104, -0.6578,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1031, -0.1600, -0.0737,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0218, -0.0123,  0.0932,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 1, 1, 2, 0, 3, 2, 3, 0, 2, 2, 1, 2, 3, 2, 3, 3, 3, 0, 0, 2, 2, 2, 0,
        1, 1, 3, 2, 2, 0, 3, 3, 3, 2, 3, 0, 2, 2, 2, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.5778,  0.8069,  0.6408,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0616,  0.1316,  0.0123,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0986, -0.0797, -0.0581,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0165, -0.0232, -0.0126,  ...,  0.0573,  0.0793,  0.0990],
        [ 0.0745,  0.0996,  0.1281,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0574,  0.0137,  0.0562,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 2, 0, 3, 0, 0, 3, 2, 0, 1, 3, 1, 3, 1, 0, 0, 0, 1, 3, 1, 2, 3, 3,
        3, 1, 3, 1, 2, 2, 1, 0, 0, 1, 0, 3, 0, 1, 2, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'projector.weight', 'classifier.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 3.8240e+00,  3.0034e+00,  2.3658e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.6254e-02, -6.3653e-02, -7.6658e-02,  ..., -2.3129e+00,
         -2.0314e+00, -1.5892e+00],
        [ 6.5216e-02,  1.1652e-01,  8.5957e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 8.0495e-01,  8.5473e-01,  9.0408e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3708e-03,  6.3708e-03,  8.5800e-04,  ..., -1.0194e-01,
         -1.4750e-01, -1.5253e-01],
        [-6.3448e-01, -5.2688e-01, -8.6589e-01,  ...,  5.1747e-01,
          4.3720e-01,  4.2781e-01]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 2, 0, 2, 0, 0, 3, 3, 0, 3, 2, 2, 1, 0, 2, 1, 3, 0, 0, 1, 3, 2, 2, 0,
        3, 2, 0, 3, 2, 3, 2, 1, 3, 0, 3, 2, 1, 1, 0, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  2 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 486100
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 741, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 558, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 584, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 626, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1793, in forward
    outputs = self.wav2vec2(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1304, in forward
    encoder_outputs = self.encoder(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 876, in forward
    layer_outputs = layer(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 715, in forward
    hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 651, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/activations.py", line 56, in forward
    return self.act(input)
RuntimeError: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 0; 31.75 GiB total capacity; 30.08 GiB already allocated; 102.00 MiB free; 30.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Mon Oct 31 16:51:48 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 31/10/2022 16:52:05

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.1541, -0.3019,  0.2839,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3247,  0.3154,  0.3118,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2947, -0.2920, -0.2803,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.1167, -0.3287,  0.0088,  ...,  0.0000,  0.0000,  0.0000],
        [ 7.1712,  6.9891,  5.4432,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0512,  0.0322,  0.2253,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 2, 0, 0, 0, 3, 2, 3, 0, 2, 3, 0, 1, 2, 3, 2, 1, 2, 2, 2, 1, 3, 0,
        2, 3, 0, 0, 1, 0, 1, 2, 1, 3, 0, 0, 2, 2, 3, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.3587,  0.5046,  0.6434,  ...,  0.2170,  0.2271,  0.1709],
        [-0.0165, -0.0232, -0.0126,  ...,  0.0573,  0.0793,  0.0990],
        [ 0.1932,  0.2056,  0.2154,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-2.1937, -2.2820, -2.2517,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2613,  0.3422,  0.3204,  ..., -0.1876, -0.6730, -1.0055],
        [-1.3849, -1.4961, -1.5095,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 3, 3, 3, 1, 2, 2, 3, 1, 0, 1, 1, 0, 2, 3, 1, 0, 2, 2, 0, 3, 3, 1,
        0, 0, 2, 2, 0, 3, 1, 0, 1, 3, 0, 0, 2, 1, 2, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-1.1076, -1.1066, -1.5257,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4305,  0.6189,  0.6607,  ...,  3.1094,  2.9794,  2.7483],
        [-0.1086,  1.0498,  0.9433,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0261,  0.0750,  0.1125,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0173,  0.0519,  0.0599,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0655,  0.2089,  0.3651,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 2, 3, 1, 0, 0, 3, 0, 1, 2, 3, 2, 3, 3, 2, 0, 0, 2, 0, 1, 3, 3, 3, 2,
        1, 1, 0, 0, 3, 2, 2, 0, 0, 1, 2, 3, 0, 0, 0, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 486100
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 741, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_lstm_downstream.py", line 558, in fit
    train_loss, train_acc = self._train(
  File "run_lstm_downstream.py", line 584, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_lstm_downstream.py", line 626, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1817, in forward
    logits = self.classifier(pooled_output)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (14x28 and 4x4)

Mon Oct 31 18:13:40 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 31/10/2022 18:13:55

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0054, -0.0074, -0.0089,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0270, -0.0194,  0.0132,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.9754,  1.8253,  1.7071,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.1327,  0.0072, -0.2006,  ...,  0.1196,  0.1405,  0.1875],
        [-0.0535, -1.2690, -1.0911,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1402,  0.2536,  0.2163,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 2, 1, 3, 0, 2, 3, 2, 2, 2, 0, 2, 0, 3, 0, 0, 1, 2, 0, 3, 2, 0, 0,
        3, 3, 3, 0, 3, 3, 2, 0, 2, 3, 1, 3, 2, 3, 3, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.0070, -0.0218, -0.0254,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1733,  0.0973,  0.0610,  ...,  0.1360,  0.5360, -0.8190],
        [ 0.2580,  0.2951,  0.3360,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.9978,  0.9285,  1.1128,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4439, -0.4538, -0.3998,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4190, -0.3650, -0.2714,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 3, 0, 3, 0, 0, 3, 1, 1, 0, 3, 2, 1, 0, 1, 2, 0, 2, 3, 3, 3, 0, 2, 3,
        0, 1, 3, 0, 1, 2, 3, 3, 2, 3, 0, 0, 0, 3, 2, 3])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.1157,  1.0266,  0.8192,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3358,  1.1371,  1.8788,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0210, -0.0335, -0.0089,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-1.6076, -0.9129, -0.6243,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2309,  0.2496,  0.2316,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1609, -0.1850, -0.2012,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 0, 1, 0, 0, 0, 2, 0, 3, 0, 3, 3, 1, 1, 0, 3, 2, 3, 0, 0, 1, 2, 2,
        1, 2, 0, 1, 3, 3, 1, 2, 0, 2, 1, 2, 3, 0, 0, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 272596
Epoch 0 Train Acc 13.323193550109863% Val Acc 23.600000381469727% Train Loss 0.7198541164398193 Val Loss 1.4017390012741089
Trainable Parameters : 272596
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Epoch 1 Train Acc 13.326995849609375% Val Acc 23.0% Train Loss 0.7129936814308167 Val Loss 1.3978791236877441
Trainable Parameters : 272596
Epoch 2 Train Acc 15.216730117797852% Val Acc 24.700000762939453% Train Loss 0.7010857462882996 Val Loss 1.3903177976608276
Trainable Parameters : 272596
Epoch 3 Train Acc 38.775665283203125% Val Acc 23.899999618530273% Train Loss 0.6865049004554749 Val Loss 1.3955965042114258
Trainable Parameters : 272596
Epoch 4 Train Acc 40.030418395996094% Val Acc 25.100000381469727% Train Loss 0.6733935475349426 Val Loss 1.404322862625122
Trainable Parameters : 272596
Epoch 5 Train Acc 40.076045989990234% Val Acc 26.399999618530273% Train Loss 0.6640284657478333 Val Loss 1.4121431112289429
Trainable Parameters : 272596
Epoch 6 Train Acc 40.026615142822266% Val Acc 26.30000114440918% Train Loss 0.6590638160705566 Val Loss 1.423511028289795
Trainable Parameters : 272596
Epoch 7 Train Acc 40.030418395996094% Val Acc 27.5% Train Loss 0.6564187407493591 Val Loss 1.4425057172775269
Trainable Parameters : 272596
Epoch 8 Train Acc 39.98859405517578% Val Acc 25.600000381469727% Train Loss 0.6557431817054749 Val Loss 1.4658182859420776
Trainable Parameters : 272596
Epoch 9 Train Acc 40.03422164916992% Val Acc 24.899999618530273% Train Loss 0.6554574370384216 Val Loss 1.4660381078720093
Trainable Parameters : 272596
Epoch 10 Train Acc 40.06843948364258% Val Acc 26.100000381469727% Train Loss 0.655097484588623 Val Loss 1.4654697179794312
Trainable Parameters : 272596
Epoch 11 Train Acc 40.083648681640625% Val Acc 24.899999618530273% Train Loss 0.6549196839332581 Val Loss 1.4717960357666016
Trainable Parameters : 272596
Epoch 12 Train Acc 40.01520919799805% Val Acc 25.0% Train Loss 0.654621422290802 Val Loss 1.456130862236023
Trainable Parameters : 272596
Epoch 13 Train Acc 40.00380325317383% Val Acc 26.399999618530273% Train Loss 0.6550803184509277 Val Loss 1.4516140222549438
Trainable Parameters : 272596
Epoch 14 Train Acc 40.038021087646484% Val Acc 23.899999618530273% Train Loss 0.6549211740493774 Val Loss 1.4648933410644531
Trainable Parameters : 272596
Epoch 15 Train Acc 40.06843948364258% Val Acc 23.899999618530273% Train Loss 0.6547539234161377 Val Loss 1.4780162572860718
Trainable Parameters : 272596
Epoch 16 Train Acc 40.038021087646484% Val Acc 27.5% Train Loss 0.6548789143562317 Val Loss 1.4556788206100464
Trainable Parameters : 272596
Tue Nov 1 01:51:49 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 01/11/2022 01:52:11

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0879, -0.4139, -0.5009,  ..., -0.8577, -1.0074, -1.2012],
        [-0.0709, -1.5227, -1.3535,  ..., -3.1830, -3.0731, -1.8891],
        [-0.8630, -1.0976, -1.1935,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0600, -0.5285, -0.4759,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1029,  0.0922,  0.1205,  ...,  0.0000,  0.0000,  0.0000],
        [-2.0335, -2.2553, -2.4810,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 2, 2, 3, 2, 3, 3, 3, 0, 1, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 3,
        3, 3, 2, 2, 2, 2, 3, 3, 0, 0, 3, 3, 1, 3, 2, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 2.1067e-01,  1.7978e-01,  1.5029e-01,  ...,  1.4087e-02,
         -1.9613e-02, -4.6760e-02],
        [ 8.8821e-02,  1.0482e-02,  8.3295e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3626e-01,  1.8825e-01,  7.8514e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.9716e-01, -3.9312e-01, -4.1112e-01,  ..., -1.3550e+00,
         -1.5523e+00, -1.7332e+00],
        [-1.4741e-02, -1.1776e-03,  2.8635e-02,  ..., -2.1447e-01,
         -2.4608e-01,  1.4034e-01],
        [ 5.5700e-02,  2.5793e-02,  4.4697e-02,  ...,  7.5738e-01,
          3.2402e-01,  4.2220e-01]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 2, 1, 0, 3, 0, 3, 0, 1, 1, 2, 1, 0, 3, 3, 3, 3, 2, 2, 1, 3, 3, 1, 2,
        0, 0, 0, 1, 1, 3, 2, 2, 1, 3, 0, 2, 1, 1, 3, 2])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.bias', 'classifier.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.1916, -0.2201, -0.1819,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.4739,  0.4929, -0.4964,  ...,  0.0000,  0.0000,  0.0000],
        [-1.1639, -1.0068, -0.8807,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.4221,  0.4044,  0.3632,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3711,  0.1469, -0.1017,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0339,  0.0242,  0.0249,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 3, 0, 1, 3, 0, 0, 2, 1, 2, 2, 1, 1, 1, 1, 0, 0, 3, 2, 3, 0, 3, 2,
        2, 2, 2, 0, 1, 2, 2, 0, 3, 1, 3, 0, 3, 0, 1, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 272596
Epoch 0 Train Acc 14.779467582702637% Val Acc 23.600000381469727% Train Loss 0.7466060519218445 Val Loss 1.4112966060638428
Trainable Parameters : 272596
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Epoch 1 Train Acc 16.429656982421875% Val Acc 24.80000114440918% Train Loss 0.7355238795280457 Val Loss 1.4021445512771606
Trainable Parameters : 272596
Epoch 2 Train Acc 19.52851676940918% Val Acc 25.700000762939453% Train Loss 0.7169185876846313 Val Loss 1.3879417181015015
Trainable Parameters : 272596
Epoch 3 Train Acc 22.714828491210938% Val Acc 22.700000762939453% Train Loss 0.695926308631897 Val Loss 1.3880125284194946
Trainable Parameters : 272596
Epoch 4 Train Acc 35.29657745361328% Val Acc 25.100000381469727% Train Loss 0.6788384318351746 Val Loss 1.4014049768447876
Trainable Parameters : 272596
Epoch 5 Train Acc 39.45247268676758% Val Acc 26.399999618530273% Train Loss 0.6665055751800537 Val Loss 1.4105314016342163
Trainable Parameters : 272596
Epoch 6 Train Acc 40.026615142822266% Val Acc 26.30000114440918% Train Loss 0.6593831777572632 Val Loss 1.4220603704452515
Trainable Parameters : 272596
Epoch 7 Train Acc 40.030418395996094% Val Acc 27.5% Train Loss 0.6571364402770996 Val Loss 1.445993185043335
Trainable Parameters : 272596
Epoch 8 Train Acc 39.98859405517578% Val Acc 25.600000381469727% Train Loss 0.6557830572128296 Val Loss 1.4702210426330566
Trainable Parameters : 272596
Epoch 9 Train Acc 40.03422164916992% Val Acc 24.899999618530273% Train Loss 0.6552972197532654 Val Loss 1.4699183702468872
Trainable Parameters : 272596
Epoch 10 Train Acc 40.06843948364258% Val Acc 26.100000381469727% Train Loss 0.6553294658660889 Val Loss 1.4686049222946167
Trainable Parameters : 272596
Epoch 11 Train Acc 40.083648681640625% Val Acc 24.899999618530273% Train Loss 0.6550477743148804 Val Loss 1.4727445840835571
Trainable Parameters : 272596
Epoch 12 Train Acc 40.01520919799805% Val Acc 25.0% Train Loss 0.6548715233802795 Val Loss 1.4580193758010864
Trainable Parameters : 272596
Epoch 13 Train Acc 40.00380325317383% Val Acc 26.399999618530273% Train Loss 0.6549823880195618 Val Loss 1.452681541442871
Trainable Parameters : 272596
Epoch 14 Train Acc 40.038021087646484% Val Acc 23.899999618530273% Train Loss 0.6549054980278015 Val Loss 1.4650709629058838
Trainable Parameters : 272596
Epoch 15 Train Acc 40.06843948364258% Val Acc 23.899999618530273% Train Loss 0.6548660397529602 Val Loss 1.479191780090332
Trainable Parameters : 272596
Epoch 16 Train Acc 40.038021087646484% Val Acc 27.5% Train Loss 0.6548136472702026 Val Loss 1.4559153318405151
Trainable Parameters : 272596
Epoch 17 Train Acc 40.06083679199219% Val Acc 23.5% Train Loss 0.6548291444778442 Val Loss 1.4881032705307007
Trainable Parameters : 272596
Epoch 18 Train Acc 40.019012451171875% Val Acc 27.0% Train Loss 0.6550148725509644 Val Loss 1.4777096509933472
Trainable Parameters : 272596
Epoch 19 Train Acc 40.02281188964844% Val Acc 24.200000762939453% Train Loss 0.6551235914230347 Val Loss 1.4835305213928223
Trainable Parameters : 272596
Epoch 20 Train Acc 40.026615142822266% Val Acc 22.600000381469727% Train Loss 0.6549666523933411 Val Loss 1.4872437715530396
Trainable Parameters : 272596
Epoch 21 Train Acc 40.02281188964844% Val Acc 25.0% Train Loss 0.6550135612487793 Val Loss 1.4658688306808472
Trainable Parameters : 272596
Epoch 22 Train Acc 40.02281188964844% Val Acc 25.5% Train Loss 0.6550800204277039 Val Loss 1.4672905206680298
Trainable Parameters : 272596
Epoch 23 Train Acc 40.0% Val Acc 24.700000762939453% Train Loss 0.6549667119979858 Val Loss 1.4723423719406128
Trainable Parameters : 272596
Epoch 24 Train Acc 40.026615142822266% Val Acc 25.100000381469727% Train Loss 0.6549412608146667 Val Loss 1.4641169309616089
Trainable Parameters : 272596
Epoch 25 Train Acc 40.00760269165039% Val Acc 23.899999618530273% Train Loss 0.6549721360206604 Val Loss 1.4657312631607056
Trainable Parameters : 272596
Epoch 26 Train Acc 40.02281188964844% Val Acc 23.200000762939453% Train Loss 0.654896080493927 Val Loss 1.503360629081726
Trainable Parameters : 272596
Epoch 27 Train Acc 40.00380325317383% Val Acc 25.700000762939453% Train Loss 0.6549341678619385 Val Loss 1.4760533571243286
Trainable Parameters : 272596
Epoch 28 Train Acc 40.05703353881836% Val Acc 26.899999618530273% Train Loss 0.6549286842346191 Val Loss 1.4551677703857422
Trainable Parameters : 272596
Epoch 29 Train Acc 40.05703353881836% Val Acc 24.899999618530273% Train Loss 0.654813289642334 Val Loss 1.4649800062179565
Trainable Parameters : 272596
Epoch 30 Train Acc 39.98479080200195% Val Acc 28.0% Train Loss 0.6553048491477966 Val Loss 1.428829550743103
Trainable Parameters : 272596
Epoch 31 Train Acc 40.038021087646484% Val Acc 22.899999618530273% Train Loss 0.6550379991531372 Val Loss 1.4743542671203613
Trainable Parameters : 272596
Epoch 32 Train Acc 40.05323028564453% Val Acc 24.100000381469727% Train Loss 0.6549084782600403 Val Loss 1.4692600965499878
Trainable Parameters : 272596
Epoch 33 Train Acc 40.026615142822266% Val Acc 24.0% Train Loss 0.6548303365707397 Val Loss 1.479438304901123
Trainable Parameters : 272596
Epoch 34 Train Acc 40.05703353881836% Val Acc 27.30000114440918% Train Loss 0.6548871994018555 Val Loss 1.4618902206420898
Trainable Parameters : 272596
Epoch 35 Train Acc 40.030418395996094% Val Acc 27.399999618530273% Train Loss 0.655001163482666 Val Loss 1.4672520160675049
Trainable Parameters : 272596
Epoch 36 Train Acc 40.05323028564453% Val Acc 26.100000381469727% Train Loss 0.6549379825592041 Val Loss 1.4569971561431885
Trainable Parameters : 272596
Epoch 37 Train Acc 40.01520919799805% Val Acc 29.200000762939453% Train Loss 0.6551625728607178 Val Loss 1.4301329851150513
Trainable Parameters : 272596
Epoch 38 Train Acc 40.03422164916992% Val Acc 24.30000114440918% Train Loss 0.6549636125564575 Val Loss 1.4786903858184814
Trainable Parameters : 272596
Epoch 39 Train Acc 39.992393493652344% Val Acc 26.5% Train Loss 0.6549515724182129 Val Loss 1.4607768058776855
Trainable Parameters : 272596
Epoch 40 Train Acc 39.992393493652344% Val Acc 27.5% Train Loss 0.6550060510635376 Val Loss 1.4372514486312866
Trainable Parameters : 272596
Epoch 41 Train Acc 39.9771842956543% Val Acc 27.399999618530273% Train Loss 0.655131459236145 Val Loss 1.4729681015014648
Trainable Parameters : 272596
Epoch 42 Train Acc 40.026615142822266% Val Acc 25.700000762939453% Train Loss 0.6552134156227112 Val Loss 1.4669454097747803
Trainable Parameters : 272596
Epoch 43 Train Acc 40.026615142822266% Val Acc 27.200000762939453% Train Loss 0.6549694538116455 Val Loss 1.4329279661178589
Trainable Parameters : 272596
Epoch 44 Train Acc 40.0% Val Acc 25.30000114440918% Train Loss 0.654988706111908 Val Loss 1.4579695463180542
Trainable Parameters : 272596
Epoch 45 Train Acc 40.03422164916992% Val Acc 25.80000114440918% Train Loss 0.6550731658935547 Val Loss 1.4599417448043823
Trainable Parameters : 272596
Epoch 46 Train Acc 40.019012451171875% Val Acc 23.5% Train Loss 0.655013918876648 Val Loss 1.469769835472107
Trainable Parameters : 272596
Epoch 47 Train Acc 40.03422164916992% Val Acc 23.200000762939453% Train Loss 0.6549242734909058 Val Loss 1.4778594970703125
Trainable Parameters : 272596
Epoch 48 Train Acc 40.064640045166016% Val Acc 25.600000381469727% Train Loss 0.654911458492279 Val Loss 1.4544451236724854
Trainable Parameters : 272596
Epoch 49 Train Acc 40.03422164916992% Val Acc 26.899999618530273% Train Loss 0.6548314094543457 Val Loss 1.4474705457687378
Trainable Parameters : 272596
Epoch 50 Train Acc 40.04182434082031% Val Acc 25.5% Train Loss 0.6550493240356445 Val Loss 1.4665822982788086
Trainable Parameters : 272596
Epoch 51 Train Acc 40.030418395996094% Val Acc 25.80000114440918% Train Loss 0.6551384329795837 Val Loss 1.4620836973190308
Trainable Parameters : 272596
Epoch 52 Train Acc 40.01520919799805% Val Acc 25.30000114440918% Train Loss 0.6548558473587036 Val Loss 1.4701157808303833
Trainable Parameters : 272596
Epoch 53 Train Acc 40.026615142822266% Val Acc 23.80000114440918% Train Loss 0.6548293828964233 Val Loss 1.4743856191635132
Trainable Parameters : 272596
Epoch 54 Train Acc 40.026615142822266% Val Acc 25.30000114440918% Train Loss 0.6549987196922302 Val Loss 1.4647960662841797
Trainable Parameters : 272596
Epoch 55 Train Acc 40.04562759399414% Val Acc 22.899999618530273% Train Loss 0.6548161506652832 Val Loss 1.5056498050689697
Trainable Parameters : 272596
Epoch 56 Train Acc 40.019012451171875% Val Acc 25.899999618530273% Train Loss 0.6547611355781555 Val Loss 1.465523600578308
Trainable Parameters : 272596
Epoch 57 Train Acc 40.01140594482422% Val Acc 25.0% Train Loss 0.6550487279891968 Val Loss 1.4610352516174316
Trainable Parameters : 272596
Epoch 58 Train Acc 40.04182434082031% Val Acc 25.100000381469727% Train Loss 0.6549516916275024 Val Loss 1.459218144416809
Trainable Parameters : 272596
Epoch 59 Train Acc 40.05703353881836% Val Acc 26.899999618530273% Train Loss 0.6548821926116943 Val Loss 1.4610341787338257
Trainable Parameters : 272596
Epoch 60 Train Acc 40.03422164916992% Val Acc 25.5% Train Loss 0.6549460887908936 Val Loss 1.4599303007125854
Trainable Parameters : 272596
Epoch 61 Train Acc 40.10646438598633% Val Acc 23.600000381469727% Train Loss 0.6547601222991943 Val Loss 1.476767897605896
Trainable Parameters : 272596
Epoch 62 Train Acc 39.99619674682617% Val Acc 23.5% Train Loss 0.6552575826644897 Val Loss 1.483673095703125
Trainable Parameters : 272596
Epoch 63 Train Acc 40.02281188964844% Val Acc 23.899999618530273% Train Loss 0.6549513339996338 Val Loss 1.4654620885849
Trainable Parameters : 272596
Epoch 64 Train Acc 40.0% Val Acc 24.399999618530273% Train Loss 0.6549633145332336 Val Loss 1.4833399057388306
Trainable Parameters : 272596
Epoch 65 Train Acc 40.03422164916992% Val Acc 25.899999618530273% Train Loss 0.6550726294517517 Val Loss 1.455875039100647
Trainable Parameters : 272596
Epoch 66 Train Acc 40.00380325317383% Val Acc 26.100000381469727% Train Loss 0.6550848484039307 Val Loss 1.4572557210922241
Trainable Parameters : 272596
Epoch 67 Train Acc 40.019012451171875% Val Acc 25.30000114440918% Train Loss 0.6548571586608887 Val Loss 1.4684081077575684
Trainable Parameters : 272596
Epoch 68 Train Acc 39.96577835083008% Val Acc 25.200000762939453% Train Loss 0.6549901962280273 Val Loss 1.4618500471115112
Trainable Parameters : 272596
Epoch 69 Train Acc 40.019012451171875% Val Acc 24.80000114440918% Train Loss 0.6549738049507141 Val Loss 1.4751131534576416
Trainable Parameters : 272596
Epoch 70 Train Acc 40.03422164916992% Val Acc 23.899999618530273% Train Loss 0.6548904180526733 Val Loss 1.4821271896362305
Trainable Parameters : 272596
Epoch 71 Train Acc 40.05323028564453% Val Acc 28.200000762939453% Train Loss 0.6549537181854248 Val Loss 1.4536837339401245
Trainable Parameters : 272596
Epoch 72 Train Acc 40.02281188964844% Val Acc 24.899999618530273% Train Loss 0.6549574136734009 Val Loss 1.4651402235031128
Trainable Parameters : 272596
Epoch 73 Train Acc 40.00760269165039% Val Acc 22.5% Train Loss 0.6550455093383789 Val Loss 1.4877492189407349
Trainable Parameters : 272596
Epoch 74 Train Acc 40.030418395996094% Val Acc 21.899999618530273% Train Loss 0.654966413974762 Val Loss 1.4740537405014038
Trainable Parameters : 272596
Epoch 75 Train Acc 40.00760269165039% Val Acc 26.30000114440918% Train Loss 0.6549848914146423 Val Loss 1.4401999711990356
Trainable Parameters : 272596
Epoch 76 Train Acc 40.030418395996094% Val Acc 27.600000381469727% Train Loss 0.654841423034668 Val Loss 1.4534392356872559
Trainable Parameters : 272596
Epoch 77 Train Acc 39.980987548828125% Val Acc 25.100000381469727% Train Loss 0.6549825668334961 Val Loss 1.476779580116272
Trainable Parameters : 272596
Epoch 78 Train Acc 40.01520919799805% Val Acc 23.5% Train Loss 0.6550846099853516 Val Loss 1.4571453332901
Trainable Parameters : 272596
Epoch 79 Train Acc 40.038021087646484% Val Acc 25.80000114440918% Train Loss 0.6548717617988586 Val Loss 1.4674886465072632
Trainable Parameters : 272596
Epoch 80 Train Acc 40.038021087646484% Val Acc 25.700000762939453% Train Loss 0.6549789905548096 Val Loss 1.4538886547088623
Trainable Parameters : 272596
Epoch 81 Train Acc 39.98479080200195% Val Acc 25.30000114440918% Train Loss 0.6549757719039917 Val Loss 1.4574977159500122
Trainable Parameters : 272596
Epoch 82 Train Acc 40.04562759399414% Val Acc 22.100000381469727% Train Loss 0.6547816395759583 Val Loss 1.496099591255188
Trainable Parameters : 272596
Epoch 83 Train Acc 40.04562759399414% Val Acc 25.80000114440918% Train Loss 0.6549793481826782 Val Loss 1.448554515838623
Trainable Parameters : 272596
Epoch 84 Train Acc 40.0% Val Acc 24.200000762939453% Train Loss 0.6549505591392517 Val Loss 1.4747049808502197
Trainable Parameters : 272596
Epoch 85 Train Acc 40.01520919799805% Val Acc 26.200000762939453% Train Loss 0.6549229621887207 Val Loss 1.46739661693573
Trainable Parameters : 272596
Epoch 86 Train Acc 40.03422164916992% Val Acc 25.399999618530273% Train Loss 0.6549763083457947 Val Loss 1.475840449333191
Trainable Parameters : 272596
Epoch 87 Train Acc 40.038021087646484% Val Acc 25.5% Train Loss 0.6548850536346436 Val Loss 1.4507488012313843
Trainable Parameters : 272596
Epoch 88 Train Acc 40.06083679199219% Val Acc 23.399999618530273% Train Loss 0.6549311280250549 Val Loss 1.4775127172470093
Trainable Parameters : 272596
Epoch 89 Train Acc 39.98859405517578% Val Acc 24.0% Train Loss 0.6550463438034058 Val Loss 1.4732075929641724
Trainable Parameters : 272596
Epoch 90 Train Acc 40.038021087646484% Val Acc 22.700000762939453% Train Loss 0.6549121141433716 Val Loss 1.487905502319336
Trainable Parameters : 272596
Epoch 91 Train Acc 39.992393493652344% Val Acc 27.0% Train Loss 0.6549674272537231 Val Loss 1.442392349243164
Trainable Parameters : 272596
Epoch 92 Train Acc 40.00760269165039% Val Acc 23.0% Train Loss 0.654934287071228 Val Loss 1.4921598434448242
Trainable Parameters : 272596
Epoch 93 Train Acc 40.04562759399414% Val Acc 21.80000114440918% Train Loss 0.6548876166343689 Val Loss 1.485980749130249
Trainable Parameters : 272596
Epoch 94 Train Acc 40.026615142822266% Val Acc 25.5% Train Loss 0.6549448370933533 Val Loss 1.4761543273925781
Trainable Parameters : 272596
Epoch 95 Train Acc 40.09885787963867% Val Acc 23.30000114440918% Train Loss 0.6548510789871216 Val Loss 1.496284008026123
Trainable Parameters : 272596
Epoch 96 Train Acc 40.019012451171875% Val Acc 24.200000762939453% Train Loss 0.6551535725593567 Val Loss 1.4731767177581787
Trainable Parameters : 272596
Epoch 97 Train Acc 40.09505844116211% Val Acc 25.100000381469727% Train Loss 0.6549162268638611 Val Loss 1.4641711711883545
Trainable Parameters : 272596
Epoch 98 Train Acc 40.07984924316406% Val Acc 25.0% Train Loss 0.6548957228660583 Val Loss 1.4579787254333496
Trainable Parameters : 272596
Configuration saved in ../output/u_train_700f_local/ADI17-xlsr-lstm/config.json
Epoch 99 Train Acc 40.04182434082031% Val Acc 25.100000381469727% Train Loss 0.6549833416938782 Val Loss 1.453677773475647
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 744, in <module>
    model.module.save_pretrained(model_fp)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/modeling_utils.py", line 1657, in save_pretrained
    save_function(shard, os.path.join(save_directory, shard_file))
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/serialization.py", line 381, in save
    return
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/serialization.py", line 215, in __exit__
    self.file_like.close()
OSError: [Errno 122] Disk quota exceeded
Tue Nov 1 10:30:33 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 01/11/2022 10:30:49

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-1.3459e-03,  4.5244e-03, -1.5602e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6968e+00,  1.5187e+00,  1.0648e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.3925e-02,  6.1000e-02,  7.5342e-02,  ...,  7.1052e-01,
          8.0545e-01,  9.7347e-01],
        ...,
        [ 2.9176e+00,  3.8077e+00,  3.9245e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.4763e-02,  1.5129e-01,  1.1326e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9741e-02,  9.6808e-02,  1.1638e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 3, 1, 2, 0, 2, 2, 0, 1, 2, 0, 2, 2, 3, 3, 0, 2, 2, 2, 2, 1, 1, 3, 0,
        1, 3, 2, 2, 0, 3, 0, 3, 1, 3, 1, 2, 1, 2, 0, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-1.0706e+00, -1.1199e+00, -1.0521e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.1118e+00,  7.4866e-01,  3.1155e-01,  ...,  7.0185e-01,
          6.4681e-01,  5.3105e-01],
        [ 1.2863e+00,  9.5101e-01,  2.6689e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.4690e-03, -1.8663e-02, -3.0325e-04,  ..., -1.6729e-01,
         -1.9235e-01, -2.1887e-01],
        [-3.3774e-02, -4.3346e-02, -5.6599e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.5189e-01, -5.0832e-01, -4.5053e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 3, 1, 3, 2, 3, 3, 0, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 0, 0, 3, 3, 0,
        3, 3, 3, 2, 3, 3, 0, 0, 1, 2, 1, 0, 2, 1, 2, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.bias', 'classifier.bias', 'classifier.weight', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'input_values': tensor([[ 0.0094,  0.0116,  0.0174,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.5925,  0.3576,  1.0196,  ..., -0.1081, -0.1409, -0.2969],
        [ 3.3494,  3.9602,  4.3515,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0563,  0.0702,  0.1428,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.5167,  0.5071,  0.4931,  ..., -1.2942, -1.2100, -1.2311],
        [ 0.5472,  0.7734,  0.8065,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 2, 1, 3, 2, 3, 0, 0, 0, 0, 3, 1, 3, 1, 0, 3, 0, 3, 3, 3, 1, 3, 1, 1,
        3, 2, 1, 3, 1, 3, 2, 0, 1, 0, 1, 2, 2, 3, 3, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
Traceback (most recent call last):
  File "run_lstm_downstream.py", line 481, in <module>
    for param in model.wav2vec2.classifer.parameters():
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'Wav2Vec2Model' object has no attribute 'classifer'
Tue Nov 1 10:50:27 AEDT 2022
------------------------------------------------------------------------
                         run_lstm_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_lstm_downstream.py
Started: 01/11/2022 10:50:41

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-lstm
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-lstm
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-lstm_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0212, -0.0080, -0.0202,  ..., -0.1137, -0.0902,  0.0320],
        [ 0.0821,  0.1142,  0.0776,  ...,  0.3029,  0.3479,  0.3473],
        [ 0.0813,  0.0705,  0.0643,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-1.1996, -1.1109, -1.0061,  ..., -0.2036, -0.2000, -0.2161],
        [-0.3940,  0.2806,  0.4246,  ...,  0.0205,  0.0291,  0.0262],
        [ 1.6196,  1.8756,  1.9982,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 0, 2, 0, 3, 2, 2, 0, 3, 3, 2, 3, 0, 0, 2, 2, 3, 1, 3, 3, 1, 2, 2,
        1, 3, 3, 3, 2, 3, 2, 0, 1, 2, 2, 0, 2, 3, 3, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.5335, -0.0093, -0.2900,  ...,  0.0000,  0.0000,  0.0000],
        [-1.9286, -1.8918, -1.8632,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.6716,  1.0834,  0.6854,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-2.9830, -2.7691, -2.4663,  ...,  0.0000,  0.0000,  0.0000],
        [-0.5663, -0.2162, -0.0935,  ..., -0.1950, -0.2679, -0.2213],
        [ 0.0047,  0.0297, -0.0226,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 3, 3, 3, 0, 1, 3, 0, 2, 2, 3, 0, 1, 2, 0, 1, 3, 3, 3, 0, 1, 1, 3,
        1, 1, 0, 0, 2, 2, 1, 2, 0, 0, 1, 2, 3, 0, 1, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0132, -0.0112,  0.0087,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4840, -1.0241,  0.5227,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0946,  0.2151,  0.2934,  ..., -0.1349, -0.1239, -0.1172],
        ...,
        [ 0.0131,  0.0355,  0.0252,  ...,  0.0000,  0.0000,  0.0000],
        [-3.7384, -3.9535, -2.5906,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.5971, -0.4314, -1.0923,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 0, 1, 1, 3, 1, 1, 1, 3, 1, 2, 1, 3, 1, 3, 3, 2, 3, 2, 1, 3, 0, 0, 1,
        2, 0, 0, 2, 1, 3, 2, 3, 1, 3, 3, 1, 3, 2, 3, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 272596
Epoch 0 Train Acc 21.27376365661621% Val Acc 25.0% Train Loss 0.7201992273330688 Val Loss 1.3963632583618164
Trainable Parameters : 272596
/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Epoch 1 Train Acc 21.627376556396484% Val Acc 24.5% Train Loss 0.713088870048523 Val Loss 1.3925228118896484
Trainable Parameters : 272596
Epoch 2 Train Acc 24.019010543823242% Val Acc 24.80000114440918% Train Loss 0.6998655796051025 Val Loss 1.3898924589157104
Trainable Parameters : 272596
Epoch 3 Train Acc 27.258554458618164% Val Acc 25.0% Train Loss 0.6847915649414062 Val Loss 1.3930009603500366
Trainable Parameters : 272596
Epoch 4 Train Acc 38.14828872680664% Val Acc 25.100000381469727% Train Loss 0.6704914569854736 Val Loss 1.4116039276123047
Trainable Parameters : 272596
Epoch 5 Train Acc 40.076045989990234% Val Acc 26.399999618530273% Train Loss 0.6613543033599854 Val Loss 1.4210275411605835
Trainable Parameters : 272596
Epoch 6 Train Acc 40.026615142822266% Val Acc 26.30000114440918% Train Loss 0.6567606329917908 Val Loss 1.4305638074874878
Trainable Parameters : 272596
Epoch 7 Train Acc 40.030418395996094% Val Acc 27.5% Train Loss 0.6554886102676392 Val Loss 1.454285979270935
Trainable Parameters : 272596
Epoch 8 Train Acc 39.98859405517578% Val Acc 25.600000381469727% Train Loss 0.6550173759460449 Val Loss 1.4744240045547485
Trainable Parameters : 272596
Epoch 9 Train Acc 40.03422164916992% Val Acc 24.899999618530273% Train Loss 0.6551594138145447 Val Loss 1.4705417156219482
Trainable Parameters : 272596
Epoch 10 Train Acc 40.06843948364258% Val Acc 26.100000381469727% Train Loss 0.655045747756958 Val Loss 1.4680942296981812
Trainable Parameters : 272596
Epoch 11 Train Acc 40.083648681640625% Val Acc 24.899999618530273% Train Loss 0.6549749374389648 Val Loss 1.474333643913269
Trainable Parameters : 272596
Epoch 12 Train Acc 40.01520919799805% Val Acc 25.0% Train Loss 0.654974639415741 Val Loss 1.4568755626678467
Trainable Parameters : 272596
Epoch 13 Train Acc 40.00380325317383% Val Acc 26.399999618530273% Train Loss 0.6550474166870117 Val Loss 1.4520372152328491
Trainable Parameters : 272596
Epoch 14 Train Acc 40.038021087646484% Val Acc 23.899999618530273% Train Loss 0.6549133062362671 Val Loss 1.4656769037246704
Trainable Parameters : 272596
Epoch 15 Train Acc 40.06843948364258% Val Acc 23.899999618530273% Train Loss 0.6546875834465027 Val Loss 1.4784473180770874
Trainable Parameters : 272596
Epoch 16 Train Acc 40.038021087646484% Val Acc 27.5% Train Loss 0.6550458073616028 Val Loss 1.4558902978897095
Trainable Parameters : 272596
Epoch 17 Train Acc 40.06083679199219% Val Acc 23.5% Train Loss 0.6549383401870728 Val Loss 1.4857470989227295
Trainable Parameters : 272596
Epoch 18 Train Acc 40.019012451171875% Val Acc 27.0% Train Loss 0.6550720930099487 Val Loss 1.4779081344604492
Trainable Parameters : 272596
Epoch 19 Train Acc 40.02281188964844% Val Acc 24.200000762939453% Train Loss 0.6549211740493774 Val Loss 1.4849673509597778
Trainable Parameters : 272596
Epoch 20 Train Acc 40.026615142822266% Val Acc 22.600000381469727% Train Loss 0.6549240946769714 Val Loss 1.4870777130126953
Trainable Parameters : 272596
Epoch 21 Train Acc 40.02281188964844% Val Acc 25.0% Train Loss 0.655017614364624 Val Loss 1.4666370153427124
Trainable Parameters : 272596
Epoch 22 Train Acc 40.02281188964844% Val Acc 25.5% Train Loss 0.6550382971763611 Val Loss 1.466497778892517
Trainable Parameters : 272596
Epoch 23 Train Acc 40.0% Val Acc 24.700000762939453% Train Loss 0.6549065113067627 Val Loss 1.4718855619430542
Trainable Parameters : 272596
Epoch 24 Train Acc 40.026615142822266% Val Acc 25.100000381469727% Train Loss 0.6549041271209717 Val Loss 1.4651254415512085
Trainable Parameters : 272596
Epoch 25 Train Acc 40.00760269165039% Val Acc 23.899999618530273% Train Loss 0.6550216674804688 Val Loss 1.4660435914993286
Trainable Parameters : 272596
Epoch 26 Train Acc 40.02281188964844% Val Acc 23.200000762939453% Train Loss 0.6549756526947021 Val Loss 1.5038044452667236
Trainable Parameters : 272596
Epoch 27 Train Acc 40.00380325317383% Val Acc 25.700000762939453% Train Loss 0.6549333333969116 Val Loss 1.476565957069397
Trainable Parameters : 272596
Epoch 28 Train Acc 40.05703353881836% Val Acc 26.899999618530273% Train Loss 0.6549517512321472 Val Loss 1.455094575881958
Trainable Parameters : 272596
Epoch 29 Train Acc 40.05703353881836% Val Acc 24.899999618530273% Train Loss 0.654880940914154 Val Loss 1.465899109840393
Trainable Parameters : 272596
Epoch 30 Train Acc 39.98479080200195% Val Acc 28.0% Train Loss 0.655339777469635 Val Loss 1.4284820556640625
Trainable Parameters : 272596
Epoch 31 Train Acc 40.038021087646484% Val Acc 22.899999618530273% Train Loss 0.6549761295318604 Val Loss 1.474423885345459
Trainable Parameters : 272596
Epoch 32 Train Acc 40.05323028564453% Val Acc 24.100000381469727% Train Loss 0.6549030542373657 Val Loss 1.4695405960083008
Trainable Parameters : 272596
Epoch 33 Train Acc 40.026615142822266% Val Acc 24.0% Train Loss 0.6548742651939392 Val Loss 1.479040503501892
Trainable Parameters : 272596
Epoch 34 Train Acc 40.05703353881836% Val Acc 27.30000114440918% Train Loss 0.6549411416053772 Val Loss 1.4619711637496948
Trainable Parameters : 272596
Epoch 35 Train Acc 40.030418395996094% Val Acc 27.399999618530273% Train Loss 0.6549930572509766 Val Loss 1.4671539068222046
Trainable Parameters : 272596
Epoch 36 Train Acc 40.05323028564453% Val Acc 26.100000381469727% Train Loss 0.6549552083015442 Val Loss 1.4564579725265503
Trainable Parameters : 272596
Epoch 37 Train Acc 40.01520919799805% Val Acc 29.200000762939453% Train Loss 0.6551495790481567 Val Loss 1.4301187992095947
Trainable Parameters : 272596
Epoch 38 Train Acc 40.03422164916992% Val Acc 24.30000114440918% Train Loss 0.6550225615501404 Val Loss 1.4793070554733276
Trainable Parameters : 272596
Epoch 39 Train Acc 39.992393493652344% Val Acc 26.5% Train Loss 0.6549683213233948 Val Loss 1.4606629610061646
Trainable Parameters : 272596
Epoch 40 Train Acc 39.992393493652344% Val Acc 27.5% Train Loss 0.6550126671791077 Val Loss 1.4369847774505615
Trainable Parameters : 272596
Epoch 41 Train Acc 39.9771842956543% Val Acc 27.399999618530273% Train Loss 0.655166506767273 Val Loss 1.473148226737976
Trainable Parameters : 272596
Epoch 42 Train Acc 40.026615142822266% Val Acc 25.700000762939453% Train Loss 0.6552720665931702 Val Loss 1.4671653509140015
Trainable Parameters : 272596
Epoch 43 Train Acc 40.026615142822266% Val Acc 27.200000762939453% Train Loss 0.6549909710884094 Val Loss 1.432706356048584
Trainable Parameters : 272596
Epoch 44 Train Acc 40.0% Val Acc 25.30000114440918% Train Loss 0.6549516320228577 Val Loss 1.4578200578689575
Trainable Parameters : 272596
Epoch 45 Train Acc 40.03422164916992% Val Acc 25.80000114440918% Train Loss 0.6551167964935303 Val Loss 1.4597374200820923
Trainable Parameters : 272596
Epoch 46 Train Acc 40.019012451171875% Val Acc 23.5% Train Loss 0.6550229787826538 Val Loss 1.4697163105010986
Trainable Parameters : 272596
Epoch 47 Train Acc 40.03422164916992% Val Acc 23.200000762939453% Train Loss 0.6549701690673828 Val Loss 1.4772030115127563
Trainable Parameters : 272596
Epoch 48 Train Acc 40.064640045166016% Val Acc 25.600000381469727% Train Loss 0.654941737651825 Val Loss 1.4543226957321167
Trainable Parameters : 272596
Epoch 49 Train Acc 40.03422164916992% Val Acc 26.899999618530273% Train Loss 0.654863715171814 Val Loss 1.447418451309204
Trainable Parameters : 272596
Epoch 50 Train Acc 40.04182434082031% Val Acc 25.5% Train Loss 0.6550622582435608 Val Loss 1.4665184020996094
Trainable Parameters : 272596
Epoch 51 Train Acc 40.030418395996094% Val Acc 25.80000114440918% Train Loss 0.6551539301872253 Val Loss 1.46195387840271
Trainable Parameters : 272596
Epoch 52 Train Acc 40.01520919799805% Val Acc 25.30000114440918% Train Loss 0.6548866629600525 Val Loss 1.471022605895996
Trainable Parameters : 272596
Epoch 53 Train Acc 40.026615142822266% Val Acc 23.80000114440918% Train Loss 0.6548346281051636 Val Loss 1.4741486310958862
Trainable Parameters : 272596
Epoch 54 Train Acc 40.026615142822266% Val Acc 25.30000114440918% Train Loss 0.655039370059967 Val Loss 1.46475088596344
Trainable Parameters : 272596
Epoch 55 Train Acc 40.04562759399414% Val Acc 22.899999618530273% Train Loss 0.6549006104469299 Val Loss 1.506351113319397
Trainable Parameters : 272596
Epoch 56 Train Acc 40.019012451171875% Val Acc 25.899999618530273% Train Loss 0.6547459959983826 Val Loss 1.4650992155075073
Trainable Parameters : 272596
Epoch 57 Train Acc 40.01140594482422% Val Acc 25.0% Train Loss 0.6550140380859375 Val Loss 1.4616721868515015
Trainable Parameters : 272596
Epoch 58 Train Acc 40.04182434082031% Val Acc 25.100000381469727% Train Loss 0.6549530029296875 Val Loss 1.4586752653121948
Trainable Parameters : 272596
Epoch 59 Train Acc 40.05703353881836% Val Acc 26.899999618530273% Train Loss 0.6548587679862976 Val Loss 1.4617201089859009
Trainable Parameters : 272596
Epoch 60 Train Acc 40.03422164916992% Val Acc 25.5% Train Loss 0.6549552083015442 Val Loss 1.459634780883789
Trainable Parameters : 272596
Epoch 61 Train Acc 40.10646438598633% Val Acc 23.600000381469727% Train Loss 0.6547641754150391 Val Loss 1.4764280319213867
Trainable Parameters : 272596
Epoch 62 Train Acc 39.99619674682617% Val Acc 23.5% Train Loss 0.6552801728248596 Val Loss 1.4836548566818237
Trainable Parameters : 272596
Epoch 63 Train Acc 40.02281188964844% Val Acc 23.899999618530273% Train Loss 0.6549720764160156 Val Loss 1.465396761894226
Trainable Parameters : 272596
Epoch 64 Train Acc 40.0% Val Acc 24.399999618530273% Train Loss 0.6549604535102844 Val Loss 1.4827536344528198
Trainable Parameters : 272596
Epoch 65 Train Acc 40.03422164916992% Val Acc 25.899999618530273% Train Loss 0.6551068425178528 Val Loss 1.4552580118179321
Trainable Parameters : 272596
Epoch 66 Train Acc 40.00380325317383% Val Acc 26.100000381469727% Train Loss 0.6550846695899963 Val Loss 1.4572073221206665
Trainable Parameters : 272596
Epoch 67 Train Acc 40.019012451171875% Val Acc 25.30000114440918% Train Loss 0.6548730731010437 Val Loss 1.4683712720870972
Trainable Parameters : 272596
Epoch 68 Train Acc 39.96577835083008% Val Acc 25.200000762939453% Train Loss 0.6549951434135437 Val Loss 1.461854100227356
Trainable Parameters : 272596
Epoch 69 Train Acc 40.019012451171875% Val Acc 24.80000114440918% Train Loss 0.6549943089485168 Val Loss 1.4753323793411255
Trainable Parameters : 272596
Epoch 70 Train Acc 40.03422164916992% Val Acc 23.899999618530273% Train Loss 0.6549059748649597 Val Loss 1.482286810874939
Trainable Parameters : 272596
Epoch 71 Train Acc 40.05323028564453% Val Acc 28.200000762939453% Train Loss 0.6549899578094482 Val Loss 1.4536597728729248
Trainable Parameters : 272596
Epoch 72 Train Acc 40.02281188964844% Val Acc 24.899999618530273% Train Loss 0.6549718976020813 Val Loss 1.465768814086914
Trainable Parameters : 272596
Epoch 73 Train Acc 40.00760269165039% Val Acc 22.5% Train Loss 0.6550467610359192 Val Loss 1.4873062372207642
Trainable Parameters : 272596
Epoch 74 Train Acc 40.030418395996094% Val Acc 21.899999618530273% Train Loss 0.6549932360649109 Val Loss 1.4743331670761108
Trainable Parameters : 272596
Epoch 75 Train Acc 40.00760269165039% Val Acc 26.30000114440918% Train Loss 0.6549667716026306 Val Loss 1.439650297164917
Trainable Parameters : 272596
Epoch 76 Train Acc 40.030418395996094% Val Acc 27.600000381469727% Train Loss 0.6548990607261658 Val Loss 1.4538013935089111
Trainable Parameters : 272596
Epoch 77 Train Acc 39.980987548828125% Val Acc 25.100000381469727% Train Loss 0.65501469373703 Val Loss 1.4771326780319214
Trainable Parameters : 272596
Epoch 78 Train Acc 40.01520919799805% Val Acc 23.5% Train Loss 0.6550915241241455 Val Loss 1.4569486379623413
Trainable Parameters : 272596
Epoch 79 Train Acc 40.038021087646484% Val Acc 25.80000114440918% Train Loss 0.6548606157302856 Val Loss 1.4679926633834839
Trainable Parameters : 272596
Epoch 80 Train Acc 40.038021087646484% Val Acc 25.700000762939453% Train Loss 0.6550045609474182 Val Loss 1.453635573387146
Trainable Parameters : 272596
Epoch 81 Train Acc 39.98479080200195% Val Acc 25.30000114440918% Train Loss 0.6550054550170898 Val Loss 1.4573476314544678
Trainable Parameters : 272596
Epoch 82 Train Acc 40.04562759399414% Val Acc 22.100000381469727% Train Loss 0.6547909379005432 Val Loss 1.4956034421920776
Trainable Parameters : 272596
Epoch 83 Train Acc 40.04562759399414% Val Acc 25.80000114440918% Train Loss 0.6549866795539856 Val Loss 1.448515772819519
Trainable Parameters : 272596
Epoch 84 Train Acc 40.0% Val Acc 24.200000762939453% Train Loss 0.6549350619316101 Val Loss 1.4745911359786987
Trainable Parameters : 272596
Epoch 85 Train Acc 40.01520919799805% Val Acc 26.200000762939453% Train Loss 0.6549725532531738 Val Loss 1.4678914546966553
Trainable Parameters : 272596
Epoch 86 Train Acc 40.03422164916992% Val Acc 25.399999618530273% Train Loss 0.6549453139305115 Val Loss 1.4760105609893799
Trainable Parameters : 272596
Epoch 87 Train Acc 40.038021087646484% Val Acc 25.5% Train Loss 0.6548908948898315 Val Loss 1.4506944417953491
Trainable Parameters : 272596
Epoch 88 Train Acc 40.06083679199219% Val Acc 23.399999618530273% Train Loss 0.654936671257019 Val Loss 1.4771604537963867
Trainable Parameters : 272596
Epoch 89 Train Acc 39.98859405517578% Val Acc 24.0% Train Loss 0.6550671458244324 Val Loss 1.4725512266159058
Trainable Parameters : 272596
Epoch 90 Train Acc 40.038021087646484% Val Acc 22.700000762939453% Train Loss 0.6549211740493774 Val Loss 1.4878556728363037
Trainable Parameters : 272596
Epoch 91 Train Acc 39.992393493652344% Val Acc 27.0% Train Loss 0.6549791693687439 Val Loss 1.4420084953308105
Trainable Parameters : 272596
Epoch 92 Train Acc 40.00760269165039% Val Acc 23.0% Train Loss 0.6549524664878845 Val Loss 1.4926791191101074
Trainable Parameters : 272596
Epoch 93 Train Acc 40.04562759399414% Val Acc 21.80000114440918% Train Loss 0.654897928237915 Val Loss 1.4860637187957764
Trainable Parameters : 272596
Epoch 94 Train Acc 40.026615142822266% Val Acc 25.5% Train Loss 0.6549594402313232 Val Loss 1.4766380786895752
Trainable Parameters : 272596
Epoch 95 Train Acc 40.09885787963867% Val Acc 23.30000114440918% Train Loss 0.6548383831977844 Val Loss 1.4970910549163818
Trainable Parameters : 272596
Epoch 96 Train Acc 40.019012451171875% Val Acc 24.200000762939453% Train Loss 0.6551790833473206 Val Loss 1.472852349281311
Trainable Parameters : 272596
Epoch 97 Train Acc 40.09505844116211% Val Acc 25.100000381469727% Train Loss 0.6549162268638611 Val Loss 1.4644993543624878
Trainable Parameters : 272596
Epoch 98 Train Acc 40.07984924316406% Val Acc 25.0% Train Loss 0.6548961400985718 Val Loss 1.4580204486846924
Trainable Parameters : 272596
Epoch 99 Train Acc 40.04182434082031% Val Acc 25.100000381469727% Train Loss 0.6549894213676453 Val Loss 1.453077793121338

------> EVALUATING MODEL... ------------------------------------------ 

/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Final Test Acc:24.700000762939453% Loss:1.464413046836853
CONFUSION MATRIX
[[  0   0 100   0]
 [  0   0 100   0]
 [  0   0  98   0]
 [  0   0 100   0]]
CONFUSION MATRIX NORMALISED
[[0.         0.         0.25125628 0.        ]
 [0.         0.         0.25125628 0.        ]
 [0.         0.         0.24623116 0.        ]
 [0.         0.         0.25125628 0.        ]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       0.00      0.00      0.00       100
           2       0.25      1.00      0.40        98
           3       0.00      0.00      0.00       100

    accuracy                           0.25       398
   macro avg       0.06      0.25      0.10       398
weighted avg       0.06      0.25      0.10       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 01/11/2022 22:01:59
