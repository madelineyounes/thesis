Mon Nov 7 13:30:24 AEDT 2022
2022-11-07 13:30:26.089513: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-07 13:30:26.297772: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-07 13:30:26.331857: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-07 13:30:27.644435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-07 13:30:27.646217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-07 13:30:27.646227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_r_unfreeze.py
Started: 07/11/2022 13:30:30

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-regional-unfreeze
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: train_r_700
train_filename: train_r_700
validation_filename: dev_r_200
evaluation_filename: test_r_100
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 24
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/train_r_700.csv
--> data_test_fp: data/dev_r_200.csv
--> data_test_fp: data/test_r_100.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/train_r_700_local/ADI17-xlsr-regional-unfreeze
--> finetuned_results_fp: /srv/scratch/z5208494/output/train_r_700_local/ADI17-xlsr-regional-unfreeze_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.5038, -1.0828, -1.5211,  ..., -0.2228, -0.2440, -0.1189],
        [-0.2221,  0.3218,  0.8154,  ...,  0.0000,  0.0000,  0.0000],
        [-0.5516, -0.1808,  0.1244,  ..., -0.7342,  0.8759,  2.6001],
        ...,
        [-0.7416, -0.7060, -0.8167,  ..., -1.5944, -1.3894, -0.8658],
        [-0.1940, -0.3123, -0.1846,  ...,  0.0000,  0.0000,  0.0000],
        [-0.5050, -0.1893,  0.3962,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([ 9, 11, 13,  2,  4,  2, 13, 16,  0, 13,  3,  0,  2,  9,  4,  4,  6, 13,
        16,  3,  5,  5,  3,  1])}
Training DataCustom Files: 10500
Training Data Files: 438
Val Data Sample
{'input_values': tensor([[-0.1575,  0.1531,  0.3088,  ...,  0.0000,  0.0000,  0.0000],
        [-1.6354, -1.4630, -1.2795,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3823, -0.3796, -0.2748,  ...,  0.4234,  0.4022,  0.4559],
        ...,
        [-0.0279, -0.0875, -0.1645,  ..., -1.3530, -1.7549, -1.3791],
        [-3.0948, -3.0588, -2.8045,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.1439,  0.9397,  0.6059,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([14, 13, 10, 15, 13,  1,  1,  9,  2,  7, 11,  8, 14, 10, 12,  5,  0,  9,
         5,  8,  4, 10, 12, 10])}
Test CustomData Files: 3400
Test Data Files: 142
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.bias', 'projector.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.3625,  0.3642,  0.3509,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3389, -0.3076, -0.2763,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.2845,  1.2587,  1.1914,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0647, -0.1965, -0.1264,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0129, -0.2191, -0.3336,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0635,  0.0306,  0.0675,  ...,  2.5548,  3.0369,  3.4877]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([ 7,  4, 15,  9, 10,  9,  1,  9,  2,  9, 14,  4,  7,  1, 10, 13, 10, 10,
         6,  7,  2,  4, 14,  8])}
Test CustomData Files: 1700
Test Data Files: 71
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 17
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151422481
Epoch 0 Train Acc 9.116437911987305% Val Acc 13.239436149597168% Train Loss 1.3967982530593872 Val Loss 2.8351829051971436
EPOCH unfeeze : 1
Trainable Parameters : 151422481
Epoch 1 Train Acc 25.595890045166016% Val Acc 30.042253494262695% Train Loss 1.1942538022994995 Val Loss 2.4659109115600586
EPOCH unfeeze : 2
Trainable Parameters : 151422481
Epoch 2 Train Acc 43.46346664428711% Val Acc 37.408451080322266% Train Loss 0.9276579022407532 Val Loss 2.3381216526031494
EPOCH unfeeze : 3
Trainable Parameters : 151422481
Epoch 3 Train Acc 57.54337692260742% Val Acc 46.53520965576172% Train Loss 0.7069090604782104 Val Loss 2.1525237560272217
EPOCH unfeeze : 4
Trainable Parameters : 151422481
Epoch 4 Train Acc 67.28082275390625% Val Acc 57.408451080322266% Train Loss 0.5407760143280029 Val Loss 2.1202051639556885
EPOCH unfeeze : 5
Trainable Parameters : 151422481
Epoch 5 Train Acc 74.60045623779297% Val Acc 57.549293518066406% Train Loss 0.42332154512405396 Val Loss 2.123178720474243
EPOCH unfeeze : 6
Trainable Parameters : 151422481
Epoch 6 Train Acc 79.39269256591797% Val Acc 60.295772552490234% Train Loss 0.3388698995113373 Val Loss 2.071927070617676
EPOCH unfeeze : 7
Trainable Parameters : 151422481
Epoch 7 Train Acc 83.28538513183594% Val Acc 53.90140914916992% Train Loss 0.27554190158843994 Val Loss 2.4781789779663086
EPOCH unfeeze : 8
Trainable Parameters : 151422481
Epoch 8 Train Acc 85.4292221069336% Val Acc 56.95774459838867% Train Loss 0.2375239133834839 Val Loss 2.3786802291870117
EPOCH unfeeze : 9
Trainable Parameters : 151422481
Epoch 9 Train Acc 87.57077026367188% Val Acc 57.549293518066406% Train Loss 0.2094564139842987 Val Loss 2.507396697998047
EPOCH unfeeze : 10
Trainable Parameters : 151422481
Epoch 10 Train Acc 88.07762145996094% Val Acc 54.7746467590332% Train Loss 0.19574694335460663 Val Loss 2.5749282836914062
EPOCH unfeeze : 11
Trainable Parameters : 151422481
Epoch 11 Train Acc 88.55250549316406% Val Acc 57.197181701660156% Train Loss 0.18620458245277405 Val Loss 2.6125059127807617
EPOCH unfeeze : 12
Trainable Parameters : 151422481
Epoch 12 Train Acc 88.6598129272461% Val Acc 57.04225158691406% Train Loss 0.18617647886276245 Val Loss 2.583935022354126
EPOCH unfeeze : 13
Trainable Parameters : 151422481
Epoch 13 Train Acc 88.36072540283203% Val Acc 49.943660736083984% Train Loss 0.1871965527534485 Val Loss 3.2057206630706787
EPOCH unfeeze : 14
Trainable Parameters : 151422481
Epoch 14 Train Acc 89.13926696777344% Val Acc 52.211265563964844% Train Loss 0.17900791764259338 Val Loss 2.8837642669677734
EPOCH unfeeze : 15
Trainable Parameters : 151422481
Epoch 15 Train Acc 89.02510833740234% Val Acc 50.88732147216797% Train Loss 0.1790611296892166 Val Loss 3.1613705158233643
EPOCH unfeeze : 16
Trainable Parameters : 151422481
Epoch 16 Train Acc 89.30821228027344% Val Acc 56.7746467590332% Train Loss 0.1792479008436203 Val Loss 2.860812187194824
EPOCH unfeeze : 17
Trainable Parameters : 151422481
Epoch 17 Train Acc 89.00228118896484% Val Acc 56.380279541015625% Train Loss 0.18321098387241364 Val Loss 2.751810073852539
EPOCH unfeeze : 18
Trainable Parameters : 151422481
Epoch 18 Train Acc 88.7625503540039% Val Acc 56.6338005065918% Train Loss 0.18134267628192902 Val Loss 2.5585527420043945
EPOCH unfeeze : 19
Trainable Parameters : 151422481
Epoch 19 Train Acc 88.77853393554688% Val Acc 55.08450698852539% Train Loss 0.19107037782669067 Val Loss 2.7413291931152344
EPOCH unfeeze : 20
Trainable Parameters : 151422481
Epoch 20 Train Acc 88.83332824707031% Val Acc 55.028167724609375% Train Loss 0.1829497516155243 Val Loss 2.620469093322754
EPOCH unfeeze : 21
Trainable Parameters : 151422481
Epoch 21 Train Acc 87.63241577148438% Val Acc 56.83098602294922% Train Loss 0.20879696309566498 Val Loss 2.411977529525757
EPOCH unfeeze : 22
Trainable Parameters : 151422481
Epoch 22 Train Acc 87.90182495117188% Val Acc 53.028167724609375% Train Loss 0.2021375447511673 Val Loss 3.042266845703125
EPOCH unfeeze : 23
Trainable Parameters : 151422481
Epoch 23 Train Acc 87.69178009033203% Val Acc 52.97182846069336% Train Loss 0.20291173458099365 Val Loss 3.0154147148132324
EPOCH unfeeze : 24
Trainable Parameters : 151422481
Epoch 24 Train Acc 89.41323852539062% Val Acc 54.028167724609375% Train Loss 0.1745966374874115 Val Loss 3.369690418243408
EPOCH unfeeze : 25
Trainable Parameters : 151422481
Epoch 25 Train Acc 90.71460723876953% Val Acc 54.39436340332031% Train Loss 0.15560242533683777 Val Loss 3.3360302448272705
EPOCH unfeeze : 26
Trainable Parameters : 151422481
Epoch 26 Train Acc 90.96803283691406% Val Acc 56.90140914916992% Train Loss 0.14784805476665497 Val Loss 3.088271141052246
EPOCH unfeeze : 27
Trainable Parameters : 151422481
Epoch 27 Train Acc 92.01826477050781% Val Acc 56.380279541015625% Train Loss 0.1316654533147812 Val Loss 2.807307243347168
EPOCH unfeeze : 28
Trainable Parameters : 151422481
Epoch 28 Train Acc 92.60273742675781% Val Acc 53.39436340332031% Train Loss 0.12016741186380386 Val Loss 3.2708029747009277
EPOCH unfeeze : 29
Trainable Parameters : 151422481
Epoch 29 Train Acc 93.5936050415039% Val Acc 54.64788818359375% Train Loss 0.10913465172052383 Val Loss 3.375257730484009
EPOCH unfeeze : 30
Trainable Parameters : 151422481
Epoch 30 Train Acc 93.5753402709961% Val Acc 61.211265563964844% Train Loss 0.11055850982666016 Val Loss 2.9005024433135986
EPOCH unfeeze : 31
Trainable Parameters : 151422481
Epoch 31 Train Acc 94.01141357421875% Val Acc 58.90140914916992% Train Loss 0.1026310920715332 Val Loss 2.8830063343048096
EPOCH unfeeze : 32
Trainable Parameters : 151422481
Epoch 32 Train Acc 95.07762145996094% Val Acc 51.80281448364258% Train Loss 0.08302700519561768 Val Loss 3.7248482704162598
EPOCH unfeeze : 33
Trainable Parameters : 151422481
Epoch 33 Train Acc 94.65296173095703% Val Acc 55.380279541015625% Train Loss 0.09477876871824265 Val Loss 3.081613302230835
EPOCH unfeeze : 34
Trainable Parameters : 151422481
Epoch 34 Train Acc 95.16438293457031% Val Acc 51.04225158691406% Train Loss 0.08215455710887909 Val Loss 3.7870960235595703
EPOCH unfeeze : 35
Trainable Parameters : 151422481
Epoch 35 Train Acc 95.25341796875% Val Acc 55.028167724609375% Train Loss 0.08431313931941986 Val Loss 3.1076817512512207
EPOCH unfeeze : 36
Trainable Parameters : 151422481
Epoch 36 Train Acc 95.48172760009766% Val Acc 55.83098602294922% Train Loss 0.0776173546910286 Val Loss 3.566701889038086
EPOCH unfeeze : 37
Trainable Parameters : 151422481
Epoch 37 Train Acc 95.3858413696289% Val Acc 58.507041931152344% Train Loss 0.07650604844093323 Val Loss 3.1276016235351562
EPOCH unfeeze : 38
Trainable Parameters : 151422481
Epoch 38 Train Acc 95.54566192626953% Val Acc 57.36619567871094% Train Loss 0.07190511375665665 Val Loss 3.47499680519104
EPOCH unfeeze : 39
Trainable Parameters : 151422481
Epoch 39 Train Acc 96.21460723876953% Val Acc 58.90140914916992% Train Loss 0.06279440969228745 Val Loss 3.7041659355163574
EPOCH unfeeze : 40
Trainable Parameters : 151422481
Epoch 40 Train Acc 95.6164321899414% Val Acc 59.0% Train Loss 0.07068943977355957 Val Loss 3.4089748859405518
EPOCH unfeeze : 41
Trainable Parameters : 151422481
Epoch 41 Train Acc 95.9474868774414% Val Acc 57.760562896728516% Train Loss 0.06810929626226425 Val Loss 3.4189295768737793
EPOCH unfeeze : 42
Trainable Parameters : 151422481
Epoch 42 Train Acc 96.38356018066406% Val Acc 56.73239517211914% Train Loss 0.06397129595279694 Val Loss 3.4659485816955566
EPOCH unfeeze : 43
Trainable Parameters : 151422481
Epoch 43 Train Acc 96.81734466552734% Val Acc 58.676055908203125% Train Loss 0.05318259820342064 Val Loss 3.487952709197998
EPOCH unfeeze : 44
Trainable Parameters : 151422481
Epoch 44 Train Acc 96.76483917236328% Val Acc 56.71830749511719% Train Loss 0.05416256934404373 Val Loss 3.431936025619507
EPOCH unfeeze : 45
Trainable Parameters : 151422481
Epoch 45 Train Acc 96.9086685180664% Val Acc 58.87323760986328% Train Loss 0.04998191073536873 Val Loss 3.3302841186523438
EPOCH unfeeze : 46
Trainable Parameters : 151422481
Epoch 46 Train Acc 96.94976806640625% Val Acc 56.39436340332031% Train Loss 0.05381638929247856 Val Loss 3.4751758575439453
EPOCH unfeeze : 47
Trainable Parameters : 151422481
Epoch 47 Train Acc 97.01597595214844% Val Acc 59.0704231262207% Train Loss 0.050125423818826675 Val Loss 3.6968278884887695
EPOCH unfeeze : 48
Trainable Parameters : 151422481
Epoch 48 Train Acc 97.14382934570312% Val Acc 58.98591613769531% Train Loss 0.04873635247349739 Val Loss 3.5094821453094482
EPOCH unfeeze : 49
Trainable Parameters : 151422481
Epoch 49 Train Acc 97.20319366455078% Val Acc 57.6338005065918% Train Loss 0.0477244071662426 Val Loss 3.6457295417785645
EPOCH unfeeze : 0
Trainable Parameters : 151422481
Epoch 50 Train Acc 97.21917724609375% Val Acc 59.60563278198242% Train Loss 0.04687049984931946 Val Loss 3.624225378036499
EPOCH unfeeze : 1
Trainable Parameters : 151422481
Epoch 51 Train Acc 97.2397232055664% Val Acc 61.647884368896484% Train Loss 0.04916108772158623 Val Loss 3.3399569988250732
EPOCH unfeeze : 2
Trainable Parameters : 151422481
Epoch 52 Train Acc 97.67122650146484% Val Acc 60.22534942626953% Train Loss 0.040025778114795685 Val Loss 3.878819704055786
EPOCH unfeeze : 3
Trainable Parameters : 151422481
Epoch 53 Train Acc 97.91323852539062% Val Acc 61.35211181640625% Train Loss 0.03610707446932793 Val Loss 3.5611636638641357
EPOCH unfeeze : 4
Trainable Parameters : 151422481
Epoch 54 Train Acc 97.94976806640625% Val Acc 59.74647903442383% Train Loss 0.03470136225223541 Val Loss 3.90990948677063
EPOCH unfeeze : 5
Trainable Parameters : 151422481
Epoch 55 Train Acc 98.10958862304688% Val Acc 60.90140914916992% Train Loss 0.03259507566690445 Val Loss 3.904628276824951
EPOCH unfeeze : 6
Trainable Parameters : 151422481
Epoch 56 Train Acc 98.0547866821289% Val Acc 58.112674713134766% Train Loss 0.032431986182928085 Val Loss 4.232994079589844
EPOCH unfeeze : 7
Trainable Parameters : 151422481
Epoch 57 Train Acc 98.20547485351562% Val Acc 60.211265563964844% Train Loss 0.030082901939749718 Val Loss 4.002278804779053
EPOCH unfeeze : 8
Trainable Parameters : 151422481
Epoch 58 Train Acc 97.7625503540039% Val Acc 60.154930114746094% Train Loss 0.039310380816459656 Val Loss 3.632882833480835
EPOCH unfeeze : 9
Trainable Parameters : 151422481
Epoch 59 Train Acc 97.86529541015625% Val Acc 59.760562896728516% Train Loss 0.03442021459341049 Val Loss 4.039078235626221
EPOCH unfeeze : 10
Trainable Parameters : 151422481
Epoch 60 Train Acc 98.12556457519531% Val Acc 57.81690216064453% Train Loss 0.032799217849969864 Val Loss 4.37327241897583
EPOCH unfeeze : 11
Trainable Parameters : 151422481
Epoch 61 Train Acc 98.28081512451172% Val Acc 59.549293518066406% Train Loss 0.03003954514861107 Val Loss 4.032870292663574
EPOCH unfeeze : 12
Trainable Parameters : 151422481
Epoch 62 Train Acc 98.04794311523438% Val Acc 60.591548919677734% Train Loss 0.033253882080316544 Val Loss 3.679264545440674
EPOCH unfeeze : 13
Trainable Parameters : 151422481
Epoch 63 Train Acc 98.31278228759766% Val Acc 61.380279541015625% Train Loss 0.028168590739369392 Val Loss 3.9195144176483154
EPOCH unfeeze : 14
Trainable Parameters : 151422481
Epoch 64 Train Acc 98.52738952636719% Val Acc 61.81690216064453% Train Loss 0.025575967505574226 Val Loss 4.329658031463623
EPOCH unfeeze : 15
Trainable Parameters : 151422481
Epoch 65 Train Acc 98.48629760742188% Val Acc 60.87323760986328% Train Loss 0.027138998731970787 Val Loss 4.202094554901123
EPOCH unfeeze : 16
Trainable Parameters : 151422481
Epoch 66 Train Acc 98.48857879638672% Val Acc 63.25352096557617% Train Loss 0.0259403083473444 Val Loss 3.9431629180908203
EPOCH unfeeze : 17
Trainable Parameters : 151422481
Epoch 67 Train Acc 98.44291687011719% Val Acc 62.57746505737305% Train Loss 0.02710934355854988 Val Loss 3.810152769088745
EPOCH unfeeze : 18
Trainable Parameters : 151422481
Epoch 68 Train Acc 98.81050109863281% Val Acc 60.04225158691406% Train Loss 0.02051691897213459 Val Loss 3.954592704772949
EPOCH unfeeze : 19
Trainable Parameters : 151422481
Epoch 69 Train Acc 98.4703140258789% Val Acc 61.39436340332031% Train Loss 0.0275681484490633 Val Loss 4.1040802001953125
EPOCH unfeeze : 20
Trainable Parameters : 151422481
Epoch 70 Train Acc 98.56163787841797% Val Acc 59.323944091796875% Train Loss 0.026078850030899048 Val Loss 4.3998894691467285
EPOCH unfeeze : 21
Trainable Parameters : 151422481
Epoch 71 Train Acc 98.85616302490234% Val Acc 58.12675857543945% Train Loss 0.020882844924926758 Val Loss 4.479112148284912
EPOCH unfeeze : 22
Trainable Parameters : 151422481
Epoch 72 Train Acc 98.37442779541016% Val Acc 58.4788703918457% Train Loss 0.02645326405763626 Val Loss 4.188177108764648
EPOCH unfeeze : 23
Trainable Parameters : 151422481
Epoch 73 Train Acc 98.67351531982422% Val Acc 58.36619567871094% Train Loss 0.022585822269320488 Val Loss 4.156015872955322
EPOCH unfeeze : 24
Trainable Parameters : 151422481
Epoch 74 Train Acc 98.5228271484375% Val Acc 59.83098602294922% Train Loss 0.02342434972524643 Val Loss 4.594109058380127
EPOCH unfeeze : 25
Trainable Parameters : 151422481
Epoch 75 Train Acc 98.75570678710938% Val Acc 59.95774459838867% Train Loss 0.02150009386241436 Val Loss 4.501078128814697
EPOCH unfeeze : 26
Trainable Parameters : 151422481
Epoch 76 Train Acc 98.7214584350586% Val Acc 60.549293518066406% Train Loss 0.02277517504990101 Val Loss 4.135769367218018
EPOCH unfeeze : 27
Trainable Parameters : 151422481
Epoch 77 Train Acc 98.88356018066406% Val Acc 59.52112579345703% Train Loss 0.020075825974345207 Val Loss 4.608147621154785
EPOCH unfeeze : 28
Trainable Parameters : 151422481
Epoch 78 Train Acc 98.90410614013672% Val Acc 60.211265563964844% Train Loss 0.01763966865837574 Val Loss 4.362227439880371
EPOCH unfeeze : 29
Trainable Parameters : 151422481
Epoch 79 Train Acc 98.75341796875% Val Acc 61.197181701660156% Train Loss 0.019666161388158798 Val Loss 4.533109188079834
EPOCH unfeeze : 30
Trainable Parameters : 151422481
Epoch 80 Train Acc 98.89497375488281% Val Acc 60.591548919677734% Train Loss 0.01935718022286892 Val Loss 4.145019054412842
EPOCH unfeeze : 31
Trainable Parameters : 151422481
Epoch 81 Train Acc 98.82191467285156% Val Acc 60.12675857543945% Train Loss 0.01679769717156887 Val Loss 4.325558662414551
EPOCH unfeeze : 32
Trainable Parameters : 151422481
Epoch 82 Train Acc 98.89497375488281% Val Acc 58.43661880493164% Train Loss 0.0191181693226099 Val Loss 4.63446044921875
EPOCH unfeeze : 33
Trainable Parameters : 151422481
Epoch 83 Train Acc 98.96346282958984% Val Acc 60.0704231262207% Train Loss 0.018639221787452698 Val Loss 4.226962566375732
EPOCH unfeeze : 34
Trainable Parameters : 151422481
Epoch 84 Train Acc 98.98857879638672% Val Acc 62.112674713134766% Train Loss 0.018326014280319214 Val Loss 4.536972999572754
EPOCH unfeeze : 35
Trainable Parameters : 151422481
Epoch 85 Train Acc 98.85159301757812% Val Acc 58.88732147216797% Train Loss 0.01842953823506832 Val Loss 4.369014739990234
EPOCH unfeeze : 36
Trainable Parameters : 151422481
Epoch 86 Train Acc 99.2397232055664% Val Acc 62.464786529541016% Train Loss 0.014538182877004147 Val Loss 4.35134744644165
EPOCH unfeeze : 37
Trainable Parameters : 151422481
Epoch 87 Train Acc 99.22373962402344% Val Acc 63.507041931152344% Train Loss 0.012763123959302902 Val Loss 4.587704658508301
EPOCH unfeeze : 38
Trainable Parameters : 151422481
Epoch 88 Train Acc 99.12556457519531% Val Acc 61.30985641479492% Train Loss 0.01651729829609394 Val Loss 4.745511054992676
EPOCH unfeeze : 39
Trainable Parameters : 151422481
Epoch 89 Train Acc 99.11415100097656% Val Acc 61.7042236328125% Train Loss 0.01698361150920391 Val Loss 4.639603614807129
EPOCH unfeeze : 40
Trainable Parameters : 151422481
Epoch 90 Train Acc 99.12785339355469% Val Acc 60.0% Train Loss 0.014468424022197723 Val Loss 4.804969310760498
EPOCH unfeeze : 41
Trainable Parameters : 151422481
Epoch 91 Train Acc 99.06391906738281% Val Acc 61.53520965576172% Train Loss 0.014757683500647545 Val Loss 4.164061069488525
EPOCH unfeeze : 42
Trainable Parameters : 151422481
Epoch 92 Train Acc 99.2397232055664% Val Acc 62.323944091796875% Train Loss 0.013786655850708485 Val Loss 4.184449195861816
EPOCH unfeeze : 43
Trainable Parameters : 151422481
Epoch 93 Train Acc 99.11872100830078% Val Acc 61.112674713134766% Train Loss 0.014734691940248013 Val Loss 4.604396820068359
EPOCH unfeeze : 44
Trainable Parameters : 151422481
Epoch 94 Train Acc 99.13926696777344% Val Acc 61.36619567871094% Train Loss 0.013951730914413929 Val Loss 4.2144904136657715
EPOCH unfeeze : 45
Trainable Parameters : 151422481
Epoch 95 Train Acc 99.40638732910156% Val Acc 62.53520965576172% Train Loss 0.010601661168038845 Val Loss 4.565633296966553
EPOCH unfeeze : 46
Trainable Parameters : 151422481
Epoch 96 Train Acc 99.28538513183594% Val Acc 63.295772552490234% Train Loss 0.011230149306356907 Val Loss 4.6235504150390625
EPOCH unfeeze : 47
Trainable Parameters : 151422481
Epoch 97 Train Acc 99.2305908203125% Val Acc 62.61971664428711% Train Loss 0.012976383790373802 Val Loss 4.687655925750732
EPOCH unfeeze : 48
Trainable Parameters : 151422481
Epoch 98 Train Acc 99.07077026367188% Val Acc 63.28168869018555% Train Loss 0.016370879486203194 Val Loss 4.397747993469238
EPOCH unfeeze : 49
Trainable Parameters : 151422481
Epoch 99 Train Acc 99.47944641113281% Val Acc 62.90140914916992% Train Loss 0.009781922213733196 Val Loss 4.923112869262695

------> EVALUATING MODEL... ------------------------------------------ 

/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Final Test Acc:62.154930114746094% Loss:4.927154541015625
CONFUSION MATRIX
[[41  0  0  2  8  0 18  0  9  3  8  2  1  2  4  0  2]
 [16 32  2  9  2  1  6  0  6  3  0  4  5 13  1  0  0]
 [ 0  1 97  0  0  0  0  0  1  0  0  1  0  0  0  0  0]
 [ 1  0  0 89  0  1  3  0  2  2  1  0  0  0  0  0  1]
 [ 4  0  0  2 73 10  1  0  3  0  5  0  0  0  1  0  1]
 [ 6  0  0  1  0 83  2  0  4  0  0  0  1  3  0  0  0]
 [ 3  1  0  1  7  0 66  0  3  2  2  4  3  0  1  0  7]
 [15  3  1  8 10 11  8  0 21  5  2  0  3  1  3  0  9]
 [ 4  0  0  0  1  0  3  0 86  2  2  1  0  0  0  0  1]
 [ 6  1  0  1  0  5  4  0 12 59  1  4  0  1  0  0  6]
 [ 0  0  0  0  0  3  0  0  0  0 93  2  0  0  0  0  2]
 [ 5  0  0  1  6  1  7  0  0  2 13 59  1  0  3  0  2]
 [ 4  0  1 10  0  3  0  0  1  3  1  3 57 13  1  0  3]
 [ 3  2  1  6  1  3  0  0 19  3  5  4  1 49  1  0  2]
 [ 2  0  0  0  1  6  0  0  1  0  0  0  1  0 87  0  2]
 [17  2  0  0  2  2  3  0  0  2  1  8  8 10 38  0  7]
 [ 1  1  0  1  0  1  3  0  5  0  1  0  1  0  0  0 86]]
CONFUSION MATRIX NORMALISED
[[0.02411765 0.         0.         0.00117647 0.00470588 0.
  0.01058824 0.         0.00529412 0.00176471 0.00470588 0.00117647
  0.00058824 0.00117647 0.00235294 0.         0.00117647]
 [0.00941176 0.01882353 0.00117647 0.00529412 0.00117647 0.00058824
  0.00352941 0.         0.00352941 0.00176471 0.         0.00235294
  0.00294118 0.00764706 0.00058824 0.         0.        ]
 [0.         0.00058824 0.05705882 0.         0.         0.
  0.         0.         0.00058824 0.         0.         0.00058824
  0.         0.         0.         0.         0.        ]
 [0.00058824 0.         0.         0.05235294 0.         0.00058824
  0.00176471 0.         0.00117647 0.00117647 0.00058824 0.
  0.         0.         0.         0.         0.00058824]
 [0.00235294 0.         0.         0.00117647 0.04294118 0.00588235
  0.00058824 0.         0.00176471 0.         0.00294118 0.
  0.         0.         0.00058824 0.         0.00058824]
 [0.00352941 0.         0.         0.00058824 0.         0.04882353
  0.00117647 0.         0.00235294 0.         0.         0.
  0.00058824 0.00176471 0.         0.         0.        ]
 [0.00176471 0.00058824 0.         0.00058824 0.00411765 0.
  0.03882353 0.         0.00176471 0.00117647 0.00117647 0.00235294
  0.00176471 0.         0.00058824 0.         0.00411765]
 [0.00882353 0.00176471 0.00058824 0.00470588 0.00588235 0.00647059
  0.00470588 0.         0.01235294 0.00294118 0.00117647 0.
  0.00176471 0.00058824 0.00176471 0.         0.00529412]
 [0.00235294 0.         0.         0.         0.00058824 0.
  0.00176471 0.         0.05058824 0.00117647 0.00117647 0.00058824
  0.         0.         0.         0.         0.00058824]
 [0.00352941 0.00058824 0.         0.00058824 0.         0.00294118
  0.00235294 0.         0.00705882 0.03470588 0.00058824 0.00235294
  0.         0.00058824 0.         0.         0.00352941]
 [0.         0.         0.         0.         0.         0.00176471
  0.         0.         0.         0.         0.05470588 0.00117647
  0.         0.         0.         0.         0.00117647]
 [0.00294118 0.         0.         0.00058824 0.00352941 0.00058824
  0.00411765 0.         0.         0.00117647 0.00764706 0.03470588
  0.00058824 0.         0.00176471 0.         0.00117647]
 [0.00235294 0.         0.00058824 0.00588235 0.         0.00176471
  0.         0.         0.00058824 0.00176471 0.00058824 0.00176471
  0.03352941 0.00764706 0.00058824 0.         0.00176471]
 [0.00176471 0.00117647 0.00058824 0.00352941 0.00058824 0.00176471
  0.         0.         0.01117647 0.00176471 0.00294118 0.00235294
  0.00058824 0.02882353 0.00058824 0.         0.00117647]
 [0.00117647 0.         0.         0.         0.00058824 0.00352941
  0.         0.         0.00058824 0.         0.         0.
  0.00058824 0.         0.05117647 0.         0.00117647]
 [0.01       0.00117647 0.         0.         0.00117647 0.00117647
  0.00176471 0.         0.         0.00117647 0.00058824 0.00470588
  0.00470588 0.00588235 0.02235294 0.         0.00411765]
 [0.00058824 0.00058824 0.         0.00058824 0.         0.00058824
  0.00176471 0.         0.00294118 0.         0.00058824 0.
  0.00058824 0.         0.         0.         0.05058824]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.32      0.41      0.36       100
           1       0.74      0.32      0.45       100
           2       0.95      0.97      0.96       100
           3       0.68      0.89      0.77       100
           4       0.66      0.73      0.69       100
           5       0.64      0.83      0.72       100
           6       0.53      0.66      0.59       100
           7       0.00      0.00      0.00       100
           8       0.50      0.86      0.63       100
           9       0.69      0.59      0.63       100
          10       0.69      0.93      0.79       100
          11       0.64      0.59      0.61       100
          12       0.70      0.57      0.63       100
          13       0.53      0.49      0.51       100
          14       0.62      0.87      0.72       100
          15       0.00      0.00      0.00       100
          16       0.66      0.86      0.74       100

    accuracy                           0.62      1700
   macro avg       0.56      0.62      0.58      1700
weighted avg       0.56      0.62      0.58      1700


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 07/11/2022 19:52:58
