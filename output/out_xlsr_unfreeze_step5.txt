Sat Nov 5 18:49:59 AEDT 2022
2022-11-05 18:50:02.052092: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-05 18:50:02.559657: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-05 18:50:02.697096: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-05 18:50:04.672357: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-05 18:50:04.673797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-05 18:50:04.673809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_unfreezestep5.py
Started: 05/11/2022 18:50:18

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-unfreeze-step5
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 5
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-araic-unfreeze-step5
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-araic-unfreeze-step5_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.7307,  0.9481,  1.0579,  ...,  0.0000,  0.0000,  0.0000],
        [-2.0694, -1.7304, -1.1127,  ...,  0.0000,  0.0000,  0.0000],
        [-0.5482, -0.2694,  0.0813,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 2.4786,  2.1879,  1.3243,  ..., -0.0029, -0.0145, -0.0657],
        [ 1.4762,  1.0124,  0.4864,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0384, -0.0402, -0.0384,  ..., -0.0872, -0.0740, -0.0684]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 3, 3, 1, 2, 2, 2, 0, 3, 2, 2, 2, 1, 2, 2, 3, 2, 3, 3, 3, 0, 2, 2, 3,
        2, 2, 2, 3, 2, 3, 2, 3, 0, 0, 0, 3, 2, 1, 3, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 1.4398,  1.5743,  1.9288,  ..., -0.5717, -0.5958, -0.7482],
        [ 0.5632,  0.6197,  0.5524,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.1792,  1.0213,  1.0969,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0447,  0.0358,  0.0273,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.9473,  0.5286,  0.6493,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0148, -0.0473, -0.0436,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 0, 2, 2, 2, 3, 2, 1, 3, 2, 0, 1, 3, 1, 1, 2, 2, 2, 0, 3, 0, 3, 1, 1,
        0, 0, 3, 2, 3, 1, 3, 2, 3, 3, 0, 2, 0, 3, 1, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.1085, -0.1363, -0.2499,  ...,  0.0000,  0.0000,  0.0000],
        [-0.7232, -1.2899, -0.6286,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4425, -0.6423, -0.5603,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.5014, -0.5050, -0.4906,  ...,  0.0000,  0.0000,  0.0000],
        [-0.9951, -0.5809, -0.3378,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0410, -0.1380, -0.4082,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 3, 1, 3, 1, 3, 3, 1, 2, 0, 2, 1, 3, 2, 2, 2, 1, 3, 0, 0, 3, 2, 2, 1,
        1, 0, 2, 3, 1, 1, 2, 3, 1, 2, 0, 2, 2, 2, 0, 3])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 0 Train Acc 35.09885787963867% Val Acc 25.5% Train Loss 0.6730813384056091 Val Loss 1.4296581745147705
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 1 Train Acc 46.726234436035156% Val Acc 31.30000114440918% Train Loss 0.6009277105331421 Val Loss 1.4795031547546387
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 2 Train Acc 58.429656982421875% Val Acc 52.60000228881836% Train Loss 0.5072228312492371 Val Loss 1.1345032453536987
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 3 Train Acc 71.00760650634766% Val Acc 57.0% Train Loss 0.3824443817138672 Val Loss 1.09634530544281
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 4 Train Acc 78.44866943359375% Val Acc 64.30000305175781% Train Loss 0.29741960763931274 Val Loss 1.0276988744735718
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 5 Train Acc 83.20912170410156% Val Acc 75.0% Train Loss 0.2303318828344345 Val Loss 0.7190660238265991
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 6 Train Acc 87.41064453125% Val Acc 74.30000305175781% Train Loss 0.17905797064304352 Val Loss 0.8333904147148132
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 7 Train Acc 90.03421783447266% Val Acc 73.20000457763672% Train Loss 0.14131297171115875 Val Loss 0.8744284510612488
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 8 Train Acc 92.39543914794922% Val Acc 76.70000457763672% Train Loss 0.11199428886175156 Val Loss 0.752979040145874
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 9 Train Acc 93.64258575439453% Val Acc 66.0999984741211% Train Loss 0.09151297062635422 Val Loss 1.4205341339111328
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 10 Train Acc 94.41064453125% Val Acc 71.5% Train Loss 0.0797405019402504 Val Loss 1.092352271080017
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 11 Train Acc 94.73384094238281% Val Acc 72.0% Train Loss 0.07511585205793381 Val Loss 1.3742755651474
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 12 Train Acc 95.42205047607422% Val Acc 67.70000457763672% Train Loss 0.06559350341558456 Val Loss 1.4483011960983276
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 13 Train Acc 94.97338104248047% Val Acc 65.5% Train Loss 0.07351569831371307 Val Loss 1.542429804801941
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 14 Train Acc 95.43345642089844% Val Acc 71.0% Train Loss 0.06517394632101059 Val Loss 1.4036192893981934
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 15 Train Acc 95.39543914794922% Val Acc 66.20000457763672% Train Loss 0.06595180928707123 Val Loss 1.8233203887939453
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 16 Train Acc 95.72623443603516% Val Acc 68.0999984741211% Train Loss 0.06291933357715607 Val Loss 1.5467106103897095
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 17 Train Acc 95.5855484008789% Val Acc 70.80000305175781% Train Loss 0.06520655751228333 Val Loss 1.2937979698181152
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 18 Train Acc 96.11406707763672% Val Acc 72.70000457763672% Train Loss 0.059492118656635284 Val Loss 1.1727393865585327
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 19 Train Acc 95.91254425048828% Val Acc 70.5999984741211% Train Loss 0.06250927597284317 Val Loss 1.2723896503448486
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 20 Train Acc 95.49429321289062% Val Acc 66.0% Train Loss 0.06514982134103775 Val Loss 1.512908697128296
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 21 Train Acc 95.74524688720703% Val Acc 69.0999984741211% Train Loss 0.06600871682167053 Val Loss 1.2148360013961792
EPOCH unfeeze : 2
Trainable Parameters : 151419140
