Wed Oct 26 01:54:40 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr.py
Started: 26/10/2022 01:54:55

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-arabic
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 24
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-arabic
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-arabic_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.9287, -1.0135, -1.0226,  ...,  1.1410,  1.5354,  1.8006],
        [-0.0142, -0.0085,  0.0227,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2211, -0.0701, -0.0163,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.9231, -0.8277, -0.5481,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2084,  0.0875, -0.2038,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3501, -0.1570,  0.0221,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 0, 2, 3, 3, 3, 2, 2, 0, 1, 1, 1, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2])}
Training DataCustom Files: 10502
Training Data Files: 438
Val Data Sample
{'input_values': tensor([[ 0.3040,  0.7251,  0.9327,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1043, -0.0726,  0.0185,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1429, -0.1463, -0.1432,  ..., -0.3790, -0.2862, -0.2804],
        ...,
        [-0.0666, -0.0058, -0.0245,  ...,  1.4959,  1.5087,  1.3864],
        [ 0.0032,  0.0227,  0.0067,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.5369,  0.5113,  0.4016,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 1, 0, 2, 0, 2, 0, 0, 2, 1, 0, 2, 0, 3, 3, 2, 0, 2, 3, 2, 2, 0, 3])}
Test CustomData Files: 813
Test Data Files: 34
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.weight', 'projector.bias', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.1414,  0.0578, -0.0170,  ...,  0.0000,  0.0000,  0.0000],
        [-2.2588, -1.5384, -1.2386,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.2650,  1.2309,  1.1583,  ...,  0.0496,  0.0555,  0.0526],
        ...,
        [ 0.5472,  0.7734,  0.8065,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2148,  0.1436, -0.0419,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3935, -0.2919, -0.3880,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 3, 0, 3, 2, 0, 0, 0, 2, 0, 3, 0, 2, 2, 2, 2, 0, 0, 3, 3, 2, 2, 3, 3])}
Test CustomData Files: 398
Test Data Files: 17
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  2 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
formats: can't open input file `/srv/scratch/z5208494/dataset/train_segments/yFM4x8SuPlA_011360-011922.wav': WAVE: RIFF header not found
Traceback (most recent call last):
  File "run_xlsr.py", line 721, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_xlsr.py", line 546, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_xlsr.py", line 560, in _train
    data = next(tr_itt)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 530, in __next__
    data = self._next_data()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1224, in _next_data
    return self._process_data(data)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1250, in _process_data
    data.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/z5208494/thesis/customData.py", line 49, in __getitem__
    speech = speech_file_to_array_fn(audiopath, self.sampling_rate)
  File "/home/z5208494/thesis/customData.py", line 18, in speech_file_to_array_fn
    speech_array, sampling_rate = torchaudio.load(path)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torchaudio/backend/sox_io_backend.py", line 153, in load
    return torch.ops.torchaudio.sox_io_load_audio_file(
RuntimeError: Error loading audio file: failed to open file /srv/scratch/z5208494/dataset/train_segments/yFM4x8SuPlA_011360-011922.wav

Wed Oct 26 02:05:15 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr.py
Started: 26/10/2022 02:05:29

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-arabic
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 24
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-arabic
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-arabic_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-1.0626, -0.7514, -0.4735,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4377, -2.0439, -1.9447,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.3170,  1.4003,  1.2487,  ..., -0.0378, -0.0295, -0.0247],
        ...,
        [-0.0049,  0.0770,  0.2057,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.1181,  1.1625,  1.1734,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2033, -0.2638, -0.2433,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 3, 1, 0, 0, 3, 0, 3, 2, 2, 0, 2, 0, 0, 1, 0, 2, 0, 2, 2, 2, 0, 3])}
Training DataCustom Files: 10502
Training Data Files: 438
Val Data Sample
{'input_values': tensor([[ 0.1570, -0.4453,  0.2624,  ...,  0.0000,  0.0000,  0.0000],
        [-1.3966, -1.2876, -1.0763,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0736, -0.0909, -0.0920,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0547, -0.1318, -0.1880,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0064, -0.0030, -0.0439,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0274, -0.0297, -0.0925,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 2, 3, 1, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 3, 2, 0, 1, 2, 2, 1, 0, 3])}
Test CustomData Files: 813
Test Data Files: 34
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.4305,  0.6189,  0.6607,  ...,  3.1094,  2.9794,  2.7483],
        [-0.3915, -0.1187,  0.1507,  ...,  0.1375,  0.0724,  0.0578],
        [-0.3090, -0.4151, -0.7958,  ...,  0.0613,  0.0926,  0.0364],
        ...,
        [ 1.7907,  1.8049,  1.7620,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0805,  0.0778,  0.0727,  ...,  1.2001,  1.6445,  1.9643],
        [-0.0652, -0.1658, -0.1846,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 3, 0, 2, 0, 1, 3, 2, 3, 1, 1, 2, 1, 2, 0, 3, 3, 2, 0, 0, 1, 3, 3])}
Test CustomData Files: 398
Test Data Files: 17
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  2 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Epoch 0 Train Acc 33.27625274658203% Val Acc 25.52941131591797% Train Loss 0.6733289361000061 Val Loss 1.4551507234573364
Trainable Parameters : 264452
Epoch 1 Train Acc 40.134700775146484% Val Acc 25.52941131591797% Train Loss 0.6484883427619934 Val Loss 1.4391613006591797
Trainable Parameters : 264452
Epoch 2 Train Acc 41.340179443359375% Val Acc 32.82352828979492% Train Loss 0.6371275186538696 Val Loss 1.4041719436645508
Trainable Parameters : 264452
Epoch 3 Train Acc 43.49771499633789% Val Acc 30.117647171020508% Train Loss 0.623380184173584 Val Loss 1.423128604888916
Trainable Parameters : 264452
Epoch 4 Train Acc 45.65068435668945% Val Acc 31.41176414489746% Train Loss 0.6090423464775085 Val Loss 1.4103764295578003
Trainable Parameters : 264452
Epoch 5 Train Acc 48.65753173828125% Val Acc 33.64706039428711% Train Loss 0.5927813053131104 Val Loss 1.3560711145401
Trainable Parameters : 264452
Epoch 6 Train Acc 50.18492889404297% Val Acc 42.17647171020508% Train Loss 0.5780284404754639 Val Loss 1.323957085609436
Trainable Parameters : 264452
Epoch 7 Train Acc 52.5068473815918% Val Acc 40.411766052246094% Train Loss 0.5628010034561157 Val Loss 1.325575828552246
Trainable Parameters : 264452
Epoch 8 Train Acc 53.01141357421875% Val Acc 40.05882263183594% Train Loss 0.5516501665115356 Val Loss 1.269608497619629
Trainable Parameters : 264452
Epoch 9 Train Acc 54.37214279174805% Val Acc 47.11764907836914% Train Loss 0.5426492691040039 Val Loss 1.2631289958953857
Trainable Parameters : 264452
Epoch 10 Train Acc 55.64383316040039% Val Acc 47.05882263183594% Train Loss 0.5315949320793152 Val Loss 1.2729138135910034
Trainable Parameters : 264452
Epoch 11 Train Acc 55.49771499633789% Val Acc 42.29411697387695% Train Loss 0.5298858880996704 Val Loss 1.3972995281219482
Trainable Parameters : 264452
Epoch 12 Train Acc 56.56620788574219% Val Acc 41.52941131591797% Train Loss 0.5228195190429688 Val Loss 1.3352625370025635
Trainable Parameters : 264452
Epoch 13 Train Acc 57.21917724609375% Val Acc 50.70588302612305% Train Loss 0.5191373825073242 Val Loss 1.1756298542022705
Trainable Parameters : 264452
Epoch 14 Train Acc 57.721458435058594% Val Acc 49.70588302612305% Train Loss 0.5143151879310608 Val Loss 1.199742317199707
Trainable Parameters : 264452
Epoch 15 Train Acc 57.801368713378906% Val Acc 46.17647171020508% Train Loss 0.5098878741264343 Val Loss 1.2666324377059937
Trainable Parameters : 264452
Epoch 16 Train Acc 57.593605041503906% Val Acc 42.64706039428711% Train Loss 0.5113004446029663 Val Loss 1.391427993774414
Trainable Parameters : 264452
Epoch 17 Train Acc 57.75114059448242% Val Acc 40.764705657958984% Train Loss 0.5096413493156433 Val Loss 1.36093270778656
Trainable Parameters : 264452
Epoch 18 Train Acc 57.92465591430664% Val Acc 50.882354736328125% Train Loss 0.5102409720420837 Val Loss 1.1535955667495728
Trainable Parameters : 264452
Epoch 19 Train Acc 58.5159797668457% Val Acc 48.82352828979492% Train Loss 0.5028010010719299 Val Loss 1.1831659078598022
Trainable Parameters : 264452
Epoch 20 Train Acc 58.13013458251953% Val Acc 50.411766052246094% Train Loss 0.5107613801956177 Val Loss 1.1673990488052368
Trainable Parameters : 264452
Epoch 21 Train Acc 58.8607292175293% Val Acc 48.29411697387695% Train Loss 0.5039105415344238 Val Loss 1.1957751512527466
Trainable Parameters : 264452
Epoch 22 Train Acc 58.34931182861328% Val Acc 46.82352828979492% Train Loss 0.5046986937522888 Val Loss 1.2222014665603638
Trainable Parameters : 264452
Epoch 23 Train Acc 58.6004524230957% Val Acc 50.70588302612305% Train Loss 0.5018040537834167 Val Loss 1.1783591508865356
Trainable Parameters : 264452
Epoch 24 Train Acc 58.79680252075195% Val Acc 45.764705657958984% Train Loss 0.49827900528907776 Val Loss 1.3459922075271606
Trainable Parameters : 264452
Epoch 25 Train Acc 59.19862747192383% Val Acc 51.47058868408203% Train Loss 0.4979277551174164 Val Loss 1.192039966583252
Trainable Parameters : 264452
Epoch 26 Train Acc 59.6620979309082% Val Acc 47.11764907836914% Train Loss 0.4923700988292694 Val Loss 1.4084974527359009
Trainable Parameters : 264452
Epoch 27 Train Acc 59.73515701293945% Val Acc 46.70588302612305% Train Loss 0.4921439588069916 Val Loss 1.272492527961731
Trainable Parameters : 264452
Epoch 28 Train Acc 59.88127517700195% Val Acc 49.94117736816406% Train Loss 0.4894443154335022 Val Loss 1.294138789176941
Trainable Parameters : 264452
Epoch 29 Train Acc 60.573055267333984% Val Acc 39.52941131591797% Train Loss 0.48812365531921387 Val Loss 1.6355689764022827
Trainable Parameters : 264452
Epoch 30 Train Acc 59.83333206176758% Val Acc 45.411766052246094% Train Loss 0.48771950602531433 Val Loss 1.3119182586669922
Trainable Parameters : 264452
Epoch 31 Train Acc 60.96803283691406% Val Acc 52.64706039428711% Train Loss 0.4807756543159485 Val Loss 1.1617745161056519
Trainable Parameters : 264452
Epoch 32 Train Acc 61.061641693115234% Val Acc 52.411766052246094% Train Loss 0.4784262180328369 Val Loss 1.2659059762954712
Trainable Parameters : 264452
Epoch 33 Train Acc 60.91780471801758% Val Acc 49.52941131591797% Train Loss 0.4812118113040924 Val Loss 1.248695969581604
Trainable Parameters : 264452
Epoch 34 Train Acc 61.05250930786133% Val Acc 52.11764907836914% Train Loss 0.48029038310050964 Val Loss 1.183730125427246
Trainable Parameters : 264452
Epoch 35 Train Acc 61.69178009033203% Val Acc 47.35293960571289% Train Loss 0.4746157228946686 Val Loss 1.3169550895690918
Trainable Parameters : 264452
Epoch 36 Train Acc 61.28995132446289% Val Acc 52.411766052246094% Train Loss 0.4725033640861511 Val Loss 1.1210490465164185
Trainable Parameters : 264452
Epoch 37 Train Acc 61.78995132446289% Val Acc 57.11764907836914% Train Loss 0.47386693954467773 Val Loss 1.0885034799575806
Trainable Parameters : 264452
Epoch 38 Train Acc 61.90410614013672% Val Acc 47.35293960571289% Train Loss 0.4757475256919861 Val Loss 1.2840689420700073
Trainable Parameters : 264452
Epoch 39 Train Acc 61.242008209228516% Val Acc 48.94117736816406% Train Loss 0.47536230087280273 Val Loss 1.2555314302444458
Trainable Parameters : 264452
Epoch 40 Train Acc 62.13698196411133% Val Acc 53.29411697387695% Train Loss 0.4705229699611664 Val Loss 1.112052083015442
Trainable Parameters : 264452
Epoch 41 Train Acc 62.0068473815918% Val Acc 50.17647171020508% Train Loss 0.46918100118637085 Val Loss 1.225046992301941
Trainable Parameters : 264452
Epoch 42 Train Acc 62.413238525390625% Val Acc 52.0% Train Loss 0.4685279130935669 Val Loss 1.1100200414657593
Trainable Parameters : 264452
Epoch 43 Train Acc 62.06392288208008% Val Acc 51.70588302612305% Train Loss 0.4640628397464752 Val Loss 1.1701252460479736
Trainable Parameters : 264452
Epoch 44 Train Acc 62.901824951171875% Val Acc 47.05882263183594% Train Loss 0.4649691879749298 Val Loss 1.366627812385559
Trainable Parameters : 264452
Epoch 45 Train Acc 62.43150329589844% Val Acc 49.764705657958984% Train Loss 0.46297550201416016 Val Loss 1.3378006219863892
Trainable Parameters : 264452
Epoch 46 Train Acc 62.47716522216797% Val Acc 53.17647171020508% Train Loss 0.46531227231025696 Val Loss 1.184289813041687
Trainable Parameters : 264452
Epoch 47 Train Acc 62.43150329589844% Val Acc 57.764705657958984% Train Loss 0.46610143780708313 Val Loss 1.06569504737854
Trainable Parameters : 264452
Epoch 48 Train Acc 62.294517517089844% Val Acc 46.0% Train Loss 0.46692585945129395 Val Loss 1.3644704818725586
Trainable Parameters : 264452
Epoch 49 Train Acc 62.83104705810547% Val Acc 45.52941131591797% Train Loss 0.4598049819469452 Val Loss 1.2238458395004272
Trainable Parameters : 264452
Epoch 50 Train Acc 63.00912857055664% Val Acc 55.588233947753906% Train Loss 0.4602501392364502 Val Loss 1.1368800401687622
Trainable Parameters : 264452
Epoch 51 Train Acc 62.97945022583008% Val Acc 51.411766052246094% Train Loss 0.4598035514354706 Val Loss 1.1598796844482422
Trainable Parameters : 264452
Epoch 52 Train Acc 63.253421783447266% Val Acc 52.64706039428711% Train Loss 0.4556169807910919 Val Loss 1.2420400381088257
Trainable Parameters : 264452
Epoch 53 Train Acc 63.39497375488281% Val Acc 52.0% Train Loss 0.4593473970890045 Val Loss 1.2082403898239136
Trainable Parameters : 264452
Epoch 54 Train Acc 63.440635681152344% Val Acc 49.47058868408203% Train Loss 0.4572272300720215 Val Loss 1.2352123260498047
Trainable Parameters : 264452
Epoch 55 Train Acc 62.83561325073242% Val Acc 50.0% Train Loss 0.4590710699558258 Val Loss 1.2762064933776855
Trainable Parameters : 264452
Epoch 56 Train Acc 63.65068054199219% Val Acc 48.411766052246094% Train Loss 0.4536488354206085 Val Loss 1.4107526540756226
Trainable Parameters : 264452
Epoch 57 Train Acc 63.28538513183594% Val Acc 49.764705657958984% Train Loss 0.45721688866615295 Val Loss 1.171486258506775
Trainable Parameters : 264452
Epoch 58 Train Acc 63.5068473815918% Val Acc 46.882354736328125% Train Loss 0.45666664838790894 Val Loss 1.2884838581085205
Trainable Parameters : 264452
Epoch 59 Train Acc 63.13926696777344% Val Acc 50.64706039428711% Train Loss 0.46029821038246155 Val Loss 1.2031723260879517
Trainable Parameters : 264452
Epoch 60 Train Acc 63.75798797607422% Val Acc 47.882354736328125% Train Loss 0.4534834921360016 Val Loss 1.2757463455200195
Trainable Parameters : 264452
Epoch 61 Train Acc 63.059356689453125% Val Acc 55.52941131591797% Train Loss 0.4582858681678772 Val Loss 1.1322592496871948
Trainable Parameters : 264452
Epoch 62 Train Acc 63.67579650878906% Val Acc 52.235294342041016% Train Loss 0.4538673460483551 Val Loss 1.1485567092895508
Trainable Parameters : 264452
Epoch 63 Train Acc 63.02739334106445% Val Acc 48.235294342041016% Train Loss 0.4533780813217163 Val Loss 1.2319302558898926
Trainable Parameters : 264452
Epoch 64 Train Acc 63.56392288208008% Val Acc 52.235294342041016% Train Loss 0.4499509036540985 Val Loss 1.1901090145111084
Trainable Parameters : 264452
Epoch 65 Train Acc 64.12785339355469% Val Acc 45.70588302612305% Train Loss 0.4487912356853485 Val Loss 1.3373568058013916
Trainable Parameters : 264452
Epoch 66 Train Acc 63.5913200378418% Val Acc 54.0% Train Loss 0.45341941714286804 Val Loss 1.138596773147583
Trainable Parameters : 264452
Epoch 67 Train Acc 63.67351150512695% Val Acc 47.764705657958984% Train Loss 0.45375290513038635 Val Loss 1.3641290664672852
Trainable Parameters : 264452
Wed Oct 26 13:14:48 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr.py
Started: 26/10/2022 13:15:05

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-arabic
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-arabic
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-arabic_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.0728,  0.0610,  0.0244,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0193,  0.0225,  0.0264,  ..., -0.1804, -0.0362,  0.5992],
        [-0.0118, -0.0407, -0.0351,  ...,  0.2972, -0.2831, -0.7027],
        ...,
        [ 0.0622,  0.0874,  0.0300,  ..., -0.1204, -0.1913, -0.2556],
        [ 0.0035,  0.0034,  0.0034,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.9174,  1.6647,  1.6795,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 0, 1, 1, 2, 0, 3, 3, 2, 3, 3, 3, 2, 0, 1,
        0, 2, 2, 0, 3, 0, 2, 1, 3, 2, 0, 3, 3, 3, 3, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-1.1756, -1.1705, -1.1413,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2488,  0.2929,  0.2729,  ..., -0.8031, -1.1543, -1.2080],
        [ 1.1446,  0.8048,  0.4088,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.2127, -0.3280, -0.5188,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1845,  0.0788, -0.2168,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.9313,  2.1412,  2.6198,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 3, 0, 1, 2, 3, 0, 2, 0, 3, 0, 2, 1, 3, 1, 2, 0, 3, 1, 1, 0, 0, 1,
        3, 1, 1, 1, 1, 2, 2, 1, 2, 3, 3, 3, 3, 1, 1, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.4901, -0.5992, -0.6270,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3309, -1.5383, -3.1599,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3428, -0.3258, -0.3007,  ..., -0.7700, -0.8532, -0.8035],
        ...,
        [ 0.3832,  0.7052,  0.6135,  ..., -0.1732, -0.2441, -0.2638],
        [-0.2423, -0.0207, -0.0499,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8641, -0.7856, -0.6484,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 2, 0, 3, 2, 0, 1, 1, 1, 1, 3, 0, 0, 1, 2, 1, 0, 0, 1, 2, 3, 0, 1,
        0, 2, 1, 0, 3, 1, 1, 0, 1, 2, 2, 0, 0, 0, 2, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Epoch 0 Train Acc 34.89353561401367% Val Acc 25.5% Train Loss 0.6799164414405823 Val Loss 1.4225431680679321
Trainable Parameters : 264452
Epoch 1 Train Acc 40.0% Val Acc 26.200000762939453% Train Loss 0.6544025540351868 Val Loss 1.4616971015930176
Trainable Parameters : 264452
Epoch 2 Train Acc 39.992393493652344% Val Acc 24.700000762939453% Train Loss 0.6474860906600952 Val Loss 1.4548799991607666
Trainable Parameters : 264452
Epoch 3 Train Acc 40.35741424560547% Val Acc 24.100000381469727% Train Loss 0.6399323344230652 Val Loss 1.447874665260315
Trainable Parameters : 264452
Epoch 4 Train Acc 41.67680740356445% Val Acc 30.399999618530273% Train Loss 0.6306757926940918 Val Loss 1.4389740228652954
Trainable Parameters : 264452
Epoch 5 Train Acc 43.28897476196289% Val Acc 33.20000076293945% Train Loss 0.6212206482887268 Val Loss 1.4101006984710693
Trainable Parameters : 264452
Epoch 6 Train Acc 45.90114212036133% Val Acc 34.29999923706055% Train Loss 0.6098395586013794 Val Loss 1.3621875047683716
Trainable Parameters : 264452
Epoch 7 Train Acc 47.55513381958008% Val Acc 33.29999923706055% Train Loss 0.599247395992279 Val Loss 1.3607757091522217
Trainable Parameters : 264452
Epoch 8 Train Acc 49.418251037597656% Val Acc 34.400001525878906% Train Loss 0.5881377458572388 Val Loss 1.3400887250900269
Trainable Parameters : 264452
Epoch 9 Train Acc 50.90114212036133% Val Acc 42.400001525878906% Train Loss 0.5752237439155579 Val Loss 1.3160241842269897
Trainable Parameters : 264452
Epoch 10 Train Acc 51.7718620300293% Val Acc 36.79999923706055% Train Loss 0.5664069652557373 Val Loss 1.3564060926437378
Trainable Parameters : 264452
Epoch 11 Train Acc 53.186309814453125% Val Acc 38.29999923706055% Train Loss 0.5567854046821594 Val Loss 1.410620927810669
Trainable Parameters : 264452
Epoch 12 Train Acc 53.69581604003906% Val Acc 38.5% Train Loss 0.5494652986526489 Val Loss 1.3386445045471191
Trainable Parameters : 264452
Epoch 13 Train Acc 54.79847717285156% Val Acc 46.400001525878906% Train Loss 0.5432721376419067 Val Loss 1.2361892461776733
Trainable Parameters : 264452
Epoch 14 Train Acc 55.60076141357422% Val Acc 44.20000076293945% Train Loss 0.5364061594009399 Val Loss 1.2499316930770874
Trainable Parameters : 264452
Epoch 15 Train Acc 55.85931396484375% Val Acc 41.29999923706055% Train Loss 0.5304874777793884 Val Loss 1.3036936521530151
Trainable Parameters : 264452
Epoch 16 Train Acc 57.11787033081055% Val Acc 43.79999923706055% Train Loss 0.523083508014679 Val Loss 1.3190306425094604
Trainable Parameters : 264452
Epoch 17 Train Acc 57.04182434082031% Val Acc 37.79999923706055% Train Loss 0.5226173996925354 Val Loss 1.3890832662582397
Trainable Parameters : 264452
Epoch 18 Train Acc 57.372623443603516% Val Acc 48.900001525878906% Train Loss 0.5171819925308228 Val Loss 1.2257709503173828
Trainable Parameters : 264452
Epoch 19 Train Acc 57.28517150878906% Val Acc 48.10000228881836% Train Loss 0.5133897066116333 Val Loss 1.2396882772445679
Trainable Parameters : 264452
Epoch 20 Train Acc 58.064640045166016% Val Acc 52.29999923706055% Train Loss 0.5087679028511047 Val Loss 1.1813791990280151
Trainable Parameters : 264452
Epoch 21 Train Acc 58.35741424560547% Val Acc 47.60000228881836% Train Loss 0.5061825513839722 Val Loss 1.2304857969284058
Trainable Parameters : 264452
Epoch 22 Train Acc 59.08745193481445% Val Acc 55.60000228881836% Train Loss 0.5043776035308838 Val Loss 1.1435483694076538
Trainable Parameters : 264452
Epoch 23 Train Acc 58.3612174987793% Val Acc 48.400001525878906% Train Loss 0.5054899454116821 Val Loss 1.1476584672927856
Trainable Parameters : 264452
Epoch 24 Train Acc 58.29657745361328% Val Acc 44.900001525878906% Train Loss 0.5008649826049805 Val Loss 1.2971277236938477
Trainable Parameters : 264452
Wed Oct 26 15:51:16 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr.py
Started: 26/10/2022 15:51:29

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-arabic
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-arabic
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-arabic_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0520, -0.0266, -0.0162,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.7719, -0.0839, -0.5655,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.9030,  1.9353,  1.8573,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.2729, -0.4994, -0.7092,  ...,  0.8663,  0.8126,  0.7023],
        [ 0.0164,  0.3748,  0.7884,  ...,  0.0000,  0.0000,  0.0000],
        [-1.3774, -1.4910, -1.5819,  ..., -0.0291, -0.1589, -0.0433]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([2, 2, 2, 0, 3, 2, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 3, 1, 2, 2, 3, 0, 0,
        2, 2, 0, 2, 3, 3, 2, 2, 3, 2, 2, 0, 2, 0, 2, 0])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 3.4158,  3.8323,  4.0183,  ...,  0.3022,  0.5530,  0.7937],
        [-0.1110, -0.7798, -0.7432,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0601,  0.0613,  0.0276,  ..., -0.1758, -0.3008, -0.3269],
        ...,
        [ 0.3543,  0.3481,  0.2343,  ...,  0.0000,  0.0000,  0.0000],
        [-2.0193, -1.9234, -1.5163,  ..., -2.5809, -2.4363, -1.8568],
        [ 0.3838,  0.6235,  0.7181,  ...,  0.1717,  0.1045,  0.0041]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 1, 3, 3, 0, 2, 1, 2, 3, 0, 3, 3, 0, 3, 1, 0, 1, 2, 0, 2, 3, 0, 2, 2,
        0, 2, 1, 0, 1, 1, 1, 0, 0, 2, 1, 2, 2, 0, 3, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.5971, -0.4314, -1.0923,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1049,  0.1425,  0.3314,  ...,  0.7773, -0.1158, -0.7908],
        [ 0.4043,  0.4221,  0.4275,  ...,  0.5444,  0.4924,  0.3421],
        ...,
        [ 0.0125,  0.0177, -0.0100,  ..., -0.5080, -0.2045,  0.0963],
        [-0.8414, -1.3162, -1.0439,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2295,  0.0475, -0.0246,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 2, 0, 1, 3, 3, 3, 0, 2, 2, 0, 3, 1, 2, 1, 3, 1, 1, 2, 3, 1, 2, 3,
        0, 1, 2, 1, 1, 3, 0, 3, 3, 3, 0, 1, 2, 2, 1, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
                                                                                                                                                         Epoch 0 Train Acc 33.75284957885742% Val Acc 25.5% Train Loss 0.6794810891151428 Val Loss 1.4140316247940063
Trainable Parameters : 264452
                                                                                                                                              Epoch 1 Train Acc 40.0% Val Acc 26.200000762939453% Train Loss 0.6541920900344849 Val Loss 1.4447439908981323
Trainable Parameters : 264452
                                                                                                                                                           Epoch 2 Train Acc 39.98479080200195% Val Acc 24.700000762939453% Train Loss 0.6474565267562866 Val Loss 1.4421334266662598
Trainable Parameters : 264452
                                                                                                                                            Epoch 3 Train Acc 40.26235580444336% Val Acc 23.899999618530273% Train Loss 0.6398999094963074 Val Loss 1.441353440284729
Trainable Parameters : 264452
Epoch 29 Train Acc 59.19771957397461% Val Acc 38.900001525878906% Train Loss 0.4963570237159729 Val Loss 1.6110498905181885
Trainable Parameters : 264452
Epoch 30 Train Acc 59.26235580444336% Val Acc 50.10000228881836% Train Loss 0.4959612786769867 Val Loss 1.190015196800232
Trainable Parameters : 264452
Epoch 31 Train Acc 59.49810028076172% Val Acc 49.20000076293945% Train Loss 0.4940662980079651 Val Loss 1.2115634679794312
Trainable Parameters : 264452
Epoch 32 Train Acc 59.380226135253906% Val Acc 50.79999923706055% Train Loss 0.49453896284103394 Val Loss 1.1821529865264893
Trainable Parameters : 264452
Epoch 33 Train Acc 59.20912551879883% Val Acc 48.0% Train Loss 0.4952348470687866 Val Loss 1.2775706052780151
Trainable Parameters : 264452
Epoch 34 Train Acc 59.9467658996582% Val Acc 46.0% Train Loss 0.4935235381126404 Val Loss 1.2948179244995117
Trainable Parameters : 264452
Epoch 35 Train Acc 59.5665397644043% Val Acc 49.900001525878906% Train Loss 0.4908170998096466 Val Loss 1.3424408435821533
Trainable Parameters : 264452
Epoch 36 Train Acc 60.09885787963867% Val Acc 54.70000076293945% Train Loss 0.49526363611221313 Val Loss 1.2060520648956299
Trainable Parameters : 264452
Epoch 37 Train Acc 59.44866943359375% Val Acc 50.400001525878906% Train Loss 0.4959098994731903 Val Loss 1.1592050790786743
Trainable Parameters : 264452
Epoch 38 Train Acc 60.42585372924805% Val Acc 49.5% Train Loss 0.49038180708885193 Val Loss 1.4116568565368652
Trainable Parameters : 264452
Epoch 39 Train Acc 60.36882019042969% Val Acc 36.29999923706055% Train Loss 0.48719874024391174 Val Loss 1.484912633895874
Trainable Parameters : 264452
Epoch 40 Train Acc 59.81748962402344% Val Acc 54.900001525878906% Train Loss 0.48778456449508667 Val Loss 1.1315325498580933
Trainable Parameters : 264452
Epoch 41 Train Acc 60.14448547363281% Val Acc 50.10000228881836% Train Loss 0.48605677485466003 Val Loss 1.2018749713897705
Trainable Parameters : 264452
Epoch 42 Train Acc 60.35361099243164% Val Acc 56.10000228881836% Train Loss 0.4852229356765747 Val Loss 1.0825861692428589
Trainable Parameters : 264452
Epoch 43 Train Acc 60.783267974853516% Val Acc 51.60000228881836% Train Loss 0.48372092843055725 Val Loss 1.1613231897354126
Trainable Parameters : 264452
Epoch 44 Train Acc 61.36882019042969% Val Acc 43.0% Train Loss 0.48073476552963257 Val Loss 1.3869678974151611
Trainable Parameters : 264452
Epoch 45 Train Acc 60.85551452636719% Val Acc 52.900001525878906% Train Loss 0.4801558256149292 Val Loss 1.1301735639572144
Trainable Parameters : 264452
Epoch 46 Train Acc 61.26235580444336% Val Acc 48.60000228881836% Train Loss 0.47909316420555115 Val Loss 1.2585201263427734
Trainable Parameters : 264452
Epoch 47 Train Acc 60.51710891723633% Val Acc 56.0% Train Loss 0.4824131429195404 Val Loss 1.0789772272109985
Trainable Parameters : 264452
Epoch 48 Train Acc 61.23954391479492% Val Acc 46.70000076293945% Train Loss 0.47763964533805847 Val Loss 1.3533791303634644
Trainable Parameters : 264452
Epoch 49 Train Acc 61.661598205566406% Val Acc 46.70000076293945% Train Loss 0.4755062758922577 Val Loss 1.2667744159698486
Trainable Parameters : 264452
Epoch 50 Train Acc 61.74905014038086% Val Acc 57.20000076293945% Train Loss 0.4745130240917206 Val Loss 1.1279475688934326
Trainable Parameters : 264452
Epoch 51 Train Acc 61.29657745361328% Val Acc 49.60000228881836% Train Loss 0.4753229022026062 Val Loss 1.1954230070114136
Trainable Parameters : 264452
Epoch 52 Train Acc 61.15209197998047% Val Acc 51.29999923706055% Train Loss 0.47413933277130127 Val Loss 1.0973267555236816
Trainable Parameters : 264452
Epoch 53 Train Acc 61.866920471191406% Val Acc 50.0% Train Loss 0.47289860248565674 Val Loss 1.256270170211792
Trainable Parameters : 264452
Epoch 54 Train Acc 61.53992462158203% Val Acc 46.79999923706055% Train Loss 0.47752097249031067 Val Loss 1.3120840787887573
Trainable Parameters : 264452
Epoch 55 Train Acc 61.69961929321289% Val Acc 49.400001525878906% Train Loss 0.46985238790512085 Val Loss 1.3359040021896362
Trainable Parameters : 264452
Epoch 56 Train Acc 62.44486618041992% Val Acc 43.400001525878906% Train Loss 0.46680933237075806 Val Loss 1.6015897989273071
Trainable Parameters : 264452
Epoch 57 Train Acc 62.09885787963867% Val Acc 52.10000228881836% Train Loss 0.46946853399276733 Val Loss 1.1972357034683228
Trainable Parameters : 264452
Epoch 58 Train Acc 61.577945709228516% Val Acc 48.400001525878906% Train Loss 0.4739810824394226 Val Loss 1.187937617301941
Trainable Parameters : 264452
Epoch 59 Train Acc 62.75665283203125% Val Acc 48.10000228881836% Train Loss 0.46530959010124207 Val Loss 1.3279619216918945
Trainable Parameters : 264452
Epoch 60 Train Acc 61.923954010009766% Val Acc 47.900001525878906% Train Loss 0.4677696228027344 Val Loss 1.3831634521484375
Trainable Parameters : 264452
Epoch 61 Train Acc 62.75284957885742% Val Acc 50.70000076293945% Train Loss 0.4648852050304413 Val Loss 1.315191388130188
Trainable Parameters : 264452
Epoch 62 Train Acc 62.81748962402344% Val Acc 55.0% Train Loss 0.4662250578403473 Val Loss 1.1159428358078003
Trainable Parameters : 264452
Epoch 63 Train Acc 62.41444778442383% Val Acc 47.400001525878906% Train Loss 0.463392049074173 Val Loss 1.3500165939331055
Trainable Parameters : 264452
Epoch 64 Train Acc 62.50570297241211% Val Acc 54.79999923706055% Train Loss 0.4636794626712799 Val Loss 1.173160433769226
Trainable Parameters : 264452
Epoch 65 Train Acc 62.84030532836914% Val Acc 47.900001525878906% Train Loss 0.4615742564201355 Val Loss 1.2587047815322876
Trainable Parameters : 264452
Epoch 66 Train Acc 63.11406707763672% Val Acc 50.400001525878906% Train Loss 0.4577522575855255 Val Loss 1.1642833948135376
Trainable Parameters : 264452
Epoch 67 Train Acc 63.31178665161133% Val Acc 47.400001525878906% Train Loss 0.45819181203842163 Val Loss 1.3506864309310913
Trainable Parameters : 264452
Epoch 68 Train Acc 63.235740661621094% Val Acc 50.79999923706055% Train Loss 0.4581279754638672 Val Loss 1.152114748954773
Trainable Parameters : 264452
Epoch 69 Train Acc 63.224334716796875% Val Acc 53.20000076293945% Train Loss 0.4598579704761505 Val Loss 1.1191279888153076
Trainable Parameters : 264452
Epoch 70 Train Acc 63.969581604003906% Val Acc 44.79999923706055% Train Loss 0.4578515291213989 Val Loss 1.4231761693954468
Trainable Parameters : 264452
Epoch 71 Train Acc 63.178707122802734% Val Acc 55.0% Train Loss 0.4576968252658844 Val Loss 1.1145446300506592
Trainable Parameters : 264452
Epoch 72 Train Acc 63.186309814453125% Val Acc 53.400001525878906% Train Loss 0.45928525924682617 Val Loss 1.1420423984527588
Trainable Parameters : 264452
Epoch 73 Train Acc 63.26235580444336% Val Acc 45.400001525878906% Train Loss 0.45538753271102905 Val Loss 1.3391079902648926
Trainable Parameters : 264452
Epoch 74 Train Acc 63.65018844604492% Val Acc 49.29999923706055% Train Loss 0.4563708007335663 Val Loss 1.2656490802764893
Trainable Parameters : 264452
Epoch 75 Train Acc 63.623573303222656% Val Acc 55.10000228881836% Train Loss 0.45583999156951904 Val Loss 1.124365210533142
Trainable Parameters : 264452
Epoch 76 Train Acc 62.95817565917969% Val Acc 52.900001525878906% Train Loss 0.45945486426353455 Val Loss 1.2132610082626343
Trainable Parameters : 264452
Epoch 77 Train Acc 63.34980773925781% Val Acc 49.79999923706055% Train Loss 0.45456093549728394 Val Loss 1.2312815189361572
Trainable Parameters : 264452
Epoch 78 Train Acc 63.44486618041992% Val Acc 49.79999923706055% Train Loss 0.4532474875450134 Val Loss 1.321679949760437
Trainable Parameters : 264452
Epoch 79 Train Acc 63.63117980957031% Val Acc 47.0% Train Loss 0.45191964507102966 Val Loss 1.4160006046295166
Trainable Parameters : 264452
Epoch 80 Train Acc 63.673004150390625% Val Acc 42.900001525878906% Train Loss 0.4555215835571289 Val Loss 1.4108631610870361
Trainable Parameters : 264452
Epoch 81 Train Acc 62.96577835083008% Val Acc 45.60000228881836% Train Loss 0.45484575629234314 Val Loss 1.366188883781433
Trainable Parameters : 264452
Epoch 82 Train Acc 64.22053527832031% Val Acc 48.70000076293945% Train Loss 0.45135417580604553 Val Loss 1.3422002792358398
Trainable Parameters : 264452
Wed Oct 26 21:43:08 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr.py
Started: 26/10/2022 21:43:19

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-arabic
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-arabic
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-arabic_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.0245, -0.0728, -0.1115,  ..., -0.1502, -0.1540, -0.1552],
        [ 0.5427,  0.5563,  0.5721,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0143,  0.0087, -0.1004,  ..., -0.5056, -0.4133, -0.1367],
        ...,
        [-0.7920, -0.1674,  0.4227,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1727,  0.1807,  0.1823,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0153, -0.1422, -0.3469,  ...,  0.5358,  0.6650,  0.9096]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 3, 0, 3, 2, 0, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 2, 2, 3,
        3, 3, 2, 2, 3, 0, 3, 2, 2, 3, 1, 2, 3, 3, 3, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.0120, -0.0446, -0.0314,  ...,  0.0861,  0.0667,  0.0481],
        [ 0.0616,  0.1316,  0.0123,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1896, -1.6520, -3.0503,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.5208,  1.0416,  1.4166,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0588,  0.0363,  0.0316,  ...,  0.0000,  0.0000,  0.0000],
        [ 3.1819,  3.2698,  3.2687,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 1, 3, 3, 2, 3, 0, 1, 0, 2, 0, 0, 3, 0, 1, 1, 2, 2, 1, 1, 1, 2, 2,
        0, 0, 0, 0, 3, 1, 3, 0, 0, 3, 1, 0, 1, 0, 0, 2])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.bias', 'projector.weight', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0040,  0.0389,  0.0831,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0766, -0.0872, -0.0994,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4667, -0.4770, -0.4922,  ..., -0.9354, -0.7075, -0.1023],
        ...,
        [-0.3303, -0.0922,  0.2573,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0249, -0.0531, -0.0344,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4600, -0.0958, -0.1159,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 0, 2, 0, 2, 2, 0, 3, 0, 0, 3, 2, 2, 3, 3, 3, 1, 0, 0, 0, 2, 0, 3, 1,
        2, 2, 3, 3, 3, 0, 2, 2, 0, 0, 2, 1, 2, 1, 2, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
                                                                                                                                                          Epoch 0 Train Acc 29.152090072631836% Val Acc 25.5% Train Loss 0.6857174038887024 Val Loss 1.415047526359558
Trainable Parameters : 264452
                                                                                                                                              Epoch 1 Train Acc 40.0% Val Acc 26.200000762939453% Train Loss 0.6553674340248108 Val Loss 1.4554144144058228
Trainable Parameters : 264452
Epoch 2 Train Acc 39.992393493652344% Val Acc 24.700000762939453% Train Loss 0.6481274962425232 Val Loss 1.4458717107772827
Trainable Parameters : 264452
                                                                                                                                           Epoch 3 Train Acc 40.24334716796875% Val Acc 23.899999618530273% Train Loss 0.6403215527534485 Val Loss 1.4408880472183228
Trainable Parameters : 264452
Epoch 4 Train Acc 41.64638900756836% Val Acc 30.200000762939453% Train Loss 0.6319119930267334 Val Loss 1.4239119291305542
Trainable Parameters : 264452
                                                                                                                                                          Epoch 5 Train Acc 43.783267974853516% Val Acc 33.10000228881836% Train Loss 0.6213536858558655 Val Loss 1.4042879343032837
Trainable Parameters : 264452
Epoch 6 Train Acc 45.85171127319336% Val Acc 35.0% Train Loss 0.6109507083892822 Val Loss 1.3583258390426636
Trainable Parameters : 264452
Epoch 7 Train Acc 47.48289108276367% Val Acc 33.60000228881836% Train Loss 0.6001336574554443 Val Loss 1.3607248067855835
Trainable Parameters : 264452
  Epoch 8 Train Acc 49.16349792480469% Val Acc 35.20000076293945% Train Loss 0.5882835388183594 Val Loss 1.329188346862793
Trainable Parameters : 264452
Epoch 9 Train Acc 50.30038070678711% Val Acc 43.5% Train Loss 0.5783399343490601 Val Loss 1.3089531660079956
Trainable Parameters : 264452
               Epoch 10 Train Acc 52.01140594482422% Val Acc 36.400001525878906% Train Loss 0.5667451024055481 Val Loss 1.3429380655288696
Trainable Parameters : 264452
Epoch 11 Train Acc 53.04943084716797% Val Acc 37.0% Train Loss 0.5587106347084045 Val Loss 1.4144452810287476
Trainable Parameters : 264452
                                                                                                                                                           Epoch 12 Train Acc 53.589351654052734% Val Acc 38.20000076293945% Train Loss 0.5520000457763672 Val Loss 1.33246648311615
Trainable Parameters : 264452
Epoch 13 Train Acc 54.6692008972168% Val Acc 46.20000076293945% Train Loss 0.543208658695221 Val Loss 1.2359436750411987
Trainable Parameters : 264452
                                                                                                                                                          Epoch 14 Train Acc 54.90494155883789% Val Acc 45.10000228881836% Train Loss 0.5376124382019043 Val Loss 1.2481892108917236
Trainable Parameters : 264452
Epoch 15 Train Acc 56.55133056640625% Val Acc 40.10000228881836% Train Loss 0.530652642250061 Val Loss 1.3328847885131836
Trainable Parameters : 264452
                                                                                                                                                        Epoch 16 Train Acc 56.524715423583984% Val Acc 45.5% Train Loss 0.5239156484603882 Val Loss 1.3046481609344482
Trainable Parameters : 264452
Epoch 17 Train Acc 56.88593292236328% Val Acc 36.900001525878906% Train Loss 0.5202882289886475 Val Loss 1.4234153032302856
Trainable Parameters : 264452
Epoch 92 Train Acc 63.81368637084961% Val Acc 56.5% Train Loss 0.448856383562088 Val Loss 1.0845844745635986
Trainable Parameters : 264452
                                                                                                                                                                                                                                                                                                                   Epoch 93 Train Acc 64.19011688232422% Val Acc 52.0% Train Loss 0.4525718092918396 Val Loss 1.1868493556976318
Trainable Parameters : 264452
                                                                                                                                           Epoch 94 Train Acc 64.22433471679688% Val Acc 48.900001525878906% Train Loss 0.4468441605567932 Val Loss 1.2854773998260498
Trainable Parameters : 264452
                                                                                                                                                                                                                                                                                                                 Epoch 95 Train Acc 64.4410629272461% Val Acc 47.79999923706055% Train Loss 0.446344792842865 Val Loss 1.336572527885437
Trainable Parameters : 264452
                                                                                                                                                                                                                                                                                                      Epoch 96 Train Acc 64.24714660644531% Val Acc 55.79999923706055% Train Loss 0.4464032053947449 Val Loss 1.1227928400039673
Trainable Parameters : 264452
                                                                                                                                                                                                                                                                                                                 Epoch 97 Train Acc 63.6387825012207% Val Acc 51.29999923706055% Train Loss 0.45031747221946716 Val Loss 1.244922399520874
Trainable Parameters : 264452
                                                                                                                                                                                                                                                                                                                    Epoch 98 Train Acc 64.47908782958984% Val Acc 52.20000076293945% Train Loss 0.4467886686325073 Val Loss 1.1500762701034546
Trainable Parameters : 264452
                                                                                                                                             Configuration saved in ../output/u_train_700f_local/ADI17-xlsr-arabic/config.json
Model weights saved in ../output/u_train_700f_local/ADI17-xlsr-arabic/pytorch_model.bin
Epoch 99 Train Acc 64.41064453125% Val Acc 55.70000076293945% Train Loss 0.4457734525203705 Val Loss 1.1010596752166748

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:61.70000076293945% Loss:0.9668959975242615
CONFUSION MATRIX
[[76  5 18  1]
 [11 66 16  7]
 [15 10 70  3]
 [21 20 25 34]]
CONFUSION MATRIX NORMALISED
[[0.19095477 0.01256281 0.04522613 0.00251256]
 [0.02763819 0.16582915 0.04020101 0.01758794]
 [0.03768844 0.02512563 0.1758794  0.00753769]
 [0.05276382 0.05025126 0.06281407 0.08542714]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.62      0.76      0.68       100
           1       0.65      0.66      0.66       100
           2       0.54      0.71      0.62        98
           3       0.76      0.34      0.47       100

    accuracy                           0.62       398
   macro avg       0.64      0.62      0.61       398
weighted avg       0.64      0.62      0.61       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 26/10/2022 23:23:35
Epoch 34 Train Acc 59.67680740356445% Val Acc 43.5% Train Loss 0.4951080083847046 Val Loss 1.351915717124939
Trainable Parameters : 264452
Epoch 35 Train Acc 59.25475311279297% Val Acc 51.29999923706055% Train Loss 0.49613940715789795 Val Loss 1.2712160348892212
Trainable Parameters : 264452
Epoch 36 Train Acc 60.315589904785156% Val Acc 51.20000076293945% Train Loss 0.4907545745372772 Val Loss 1.124484896659851
Trainable Parameters : 264452
Epoch 37 Train Acc 59.889732360839844% Val Acc 46.20000076293945% Train Loss 0.49623844027519226 Val Loss 1.2191985845565796
Trainable Parameters : 264452
Epoch 38 Train Acc 60.59695816040039% Val Acc 49.10000228881836% Train Loss 0.49125298857688904 Val Loss 1.4196885824203491
Trainable Parameters : 264452
Epoch 39 Train Acc 60.06843948364258% Val Acc 36.400001525878906% Train Loss 0.4929627478122711 Val Loss 1.4946568012237549
Trainable Parameters : 264452
Epoch 40 Train Acc 60.29657745361328% Val Acc 53.5% Train Loss 0.4880899488925934 Val Loss 1.1965669393539429
Trainable Parameters : 264452
Epoch 41 Train Acc 60.235740661621094% Val Acc 43.900001525878906% Train Loss 0.4841003715991974 Val Loss 1.338813066482544
Trainable Parameters : 264452
Epoch 42 Train Acc 60.04562759399414% Val Acc 53.900001525878906% Train Loss 0.4872008264064789 Val Loss 1.119242787361145
Trainable Parameters : 264452
Epoch 43 Train Acc 60.45247268676758% Val Acc 50.20000076293945% Train Loss 0.48026609420776367 Val Loss 1.1903289556503296
Trainable Parameters : 264452
Epoch 44 Train Acc 61.09125518798828% Val Acc 41.900001525878906% Train Loss 0.47758054733276367 Val Loss 1.4080047607421875
Trainable Parameters : 264452
Epoch 45 Train Acc 60.60456085205078% Val Acc 54.60000228881836% Train Loss 0.48176637291908264 Val Loss 1.1143219470977783
Trainable Parameters : 264452
Epoch 46 Train Acc 60.5361213684082% Val Acc 47.900001525878906% Train Loss 0.48252153396606445 Val Loss 1.311169981956482
Trainable Parameters : 264452
Epoch 47 Train Acc 60.828895568847656% Val Acc 58.70000076293945% Train Loss 0.4784296154975891 Val Loss 1.0593386888504028
Trainable Parameters : 264452
Epoch 48 Train Acc 60.91254806518555% Val Acc 50.20000076293945% Train Loss 0.4794529974460602 Val Loss 1.27914297580719
Trainable Parameters : 264452
Epoch 49 Train Acc 61.89353561401367% Val Acc 47.10000228881836% Train Loss 0.47411298751831055 Val Loss 1.2730740308761597
Trainable Parameters : 264452
Epoch 50 Train Acc 62.03422164916992% Val Acc 58.5% Train Loss 0.47466230392456055 Val Loss 1.1276775598526
Trainable Parameters : 264452
Epoch 51 Train Acc 62.133079528808594% Val Acc 51.900001525878906% Train Loss 0.4724065065383911 Val Loss 1.1532272100448608
Trainable Parameters : 264452
Epoch 52 Train Acc 61.36501693725586% Val Acc 49.5% Train Loss 0.47639715671539307 Val Loss 1.1541131734848022
Trainable Parameters : 264452
Epoch 53 Train Acc 61.98479080200195% Val Acc 50.20000076293945% Train Loss 0.4708685278892517 Val Loss 1.2193408012390137
Trainable Parameters : 264452
Epoch 54 Train Acc 62.019012451171875% Val Acc 48.79999923706055% Train Loss 0.4687974154949188 Val Loss 1.31223726272583
Trainable Parameters : 264452
Epoch 55 Train Acc 62.15209197998047% Val Acc 50.79999923706055% Train Loss 0.469821572303772 Val Loss 1.3286032676696777
Trainable Parameters : 264452
Epoch 56 Train Acc 61.825096130371094% Val Acc 41.900001525878906% Train Loss 0.4664627015590668 Val Loss 1.5650843381881714
Trainable Parameters : 264452
Epoch 57 Train Acc 61.88593292236328% Val Acc 51.400001525878906% Train Loss 0.46797117590904236 Val Loss 1.1751819849014282
Trainable Parameters : 264452
Epoch 58 Train Acc 61.84790802001953% Val Acc 49.10000228881836% Train Loss 0.4744946360588074 Val Loss 1.1710965633392334
Trainable Parameters : 264452
Epoch 59 Train Acc 62.121673583984375% Val Acc 48.79999923706055% Train Loss 0.46882784366607666 Val Loss 1.2804579734802246
Trainable Parameters : 264452
Epoch 60 Train Acc 62.581748962402344% Val Acc 49.79999923706055% Train Loss 0.4654709994792938 Val Loss 1.1997929811477661
Trainable Parameters : 264452
Epoch 61 Train Acc 62.61977005004883% Val Acc 54.400001525878906% Train Loss 0.46271026134490967 Val Loss 1.2138466835021973
Trainable Parameters : 264452
Epoch 62 Train Acc 62.60456085205078% Val Acc 52.29999923706055% Train Loss 0.4667774438858032 Val Loss 1.1233898401260376
Trainable Parameters : 264452
Epoch 63 Train Acc 62.49429702758789% Val Acc 51.79999923706055% Train Loss 0.4623939096927643 Val Loss 1.185028076171875
Trainable Parameters : 264452
Epoch 64 Train Acc 62.315589904785156% Val Acc 52.10000228881836% Train Loss 0.46798333525657654 Val Loss 1.2288576364517212
Trainable Parameters : 264452
Epoch 65 Train Acc 63.167301177978516% Val Acc 46.400001525878906% Train Loss 0.4607675075531006 Val Loss 1.2892762422561646
Trainable Parameters : 264452
Epoch 66 Train Acc 63.3079833984375% Val Acc 50.29999923706055% Train Loss 0.46252456307411194 Val Loss 1.1674118041992188
Trainable Parameters : 264452
Epoch 67 Train Acc 63.076045989990234% Val Acc 41.79999923706055% Train Loss 0.46022695302963257 Val Loss 1.4527257680892944
Trainable Parameters : 264452
Epoch 68 Train Acc 62.74524688720703% Val Acc 47.5% Train Loss 0.4593895673751831 Val Loss 1.218539834022522
Trainable Parameters : 264452
Epoch 69 Train Acc 63.04943084716797% Val Acc 56.900001525878906% Train Loss 0.46188321709632874 Val Loss 1.1024755239486694
Trainable Parameters : 264452
Epoch 70 Train Acc 63.49809646606445% Val Acc 43.60000228881836% Train Loss 0.45830923318862915 Val Loss 1.4632270336151123
Trainable Parameters : 264452
Epoch 71 Train Acc 62.8745231628418% Val Acc 55.5% Train Loss 0.4553138315677643 Val Loss 1.095051884651184
Trainable Parameters : 264452
Epoch 72 Train Acc 63.30038070678711% Val Acc 53.10000228881836% Train Loss 0.45853275060653687 Val Loss 1.1389896869659424
Trainable Parameters : 264452
Epoch 73 Train Acc 62.85171127319336% Val Acc 43.400001525878906% Train Loss 0.4578966498374939 Val Loss 1.48276686668396
Trainable Parameters : 264452
Epoch 74 Train Acc 63.09125518798828% Val Acc 46.79999923706055% Train Loss 0.45804542303085327 Val Loss 1.3239004611968994
Trainable Parameters : 264452
Epoch 75 Train Acc 63.18251037597656% Val Acc 53.10000228881836% Train Loss 0.4585683047771454 Val Loss 1.1206427812576294
Trainable Parameters : 264452
Epoch 76 Train Acc 63.475284576416016% Val Acc 48.79999923706055% Train Loss 0.45819374918937683 Val Loss 1.327818751335144
Trainable Parameters : 264452
Epoch 77 Train Acc 64.15209197998047% Val Acc 50.10000228881836% Train Loss 0.45189937949180603 Val Loss 1.2884109020233154
Trainable Parameters : 264452
Epoch 78 Train Acc 63.40684509277344% Val Acc 47.70000076293945% Train Loss 0.45440033078193665 Val Loss 1.4046176671981812
Trainable Parameters : 264452
Epoch 79 Train Acc 64.42965698242188% Val Acc 43.29999923706055% Train Loss 0.45257487893104553 Val Loss 1.5177992582321167
Trainable Parameters : 264452
Epoch 80 Train Acc 63.88593292236328% Val Acc 40.20000076293945% Train Loss 0.45553329586982727 Val Loss 1.4563602209091187
Trainable Parameters : 264452
Epoch 81 Train Acc 63.79847717285156% Val Acc 51.5% Train Loss 0.45397722721099854 Val Loss 1.211216688156128
Trainable Parameters : 264452
Epoch 82 Train Acc 63.76805877685547% Val Acc 44.5% Train Loss 0.45183148980140686 Val Loss 1.440716028213501
Trainable Parameters : 264452
Epoch 83 Train Acc 63.75665283203125% Val Acc 51.20000076293945% Train Loss 0.4512913227081299 Val Loss 1.1271965503692627
Trainable Parameters : 264452
Epoch 84 Train Acc 63.83650207519531% Val Acc 53.60000228881836% Train Loss 0.45085132122039795 Val Loss 1.177759051322937
Trainable Parameters : 264452
Epoch 85 Train Acc 64.01140594482422% Val Acc 58.0% Train Loss 0.45081260800361633 Val Loss 1.0457857847213745
Trainable Parameters : 264452
Epoch 86 Train Acc 63.467681884765625% Val Acc 50.0% Train Loss 0.45095697045326233 Val Loss 1.1533889770507812
Trainable Parameters : 264452
Epoch 87 Train Acc 64.40684509277344% Val Acc 52.10000228881836% Train Loss 0.4462549090385437 Val Loss 1.2454818487167358
Trainable Parameters : 264452
Epoch 88 Train Acc 63.684410095214844% Val Acc 48.29999923706055% Train Loss 0.4531404376029968 Val Loss 1.2624311447143555
Trainable Parameters : 264452
Epoch 89 Train Acc 63.75665283203125% Val Acc 48.60000228881836% Train Loss 0.45208925008773804 Val Loss 1.207919955253601
Trainable Parameters : 264452
Epoch 90 Train Acc 64.23954010009766% Val Acc 52.5% Train Loss 0.4459323287010193 Val Loss 1.2634323835372925
Trainable Parameters : 264452
Epoch 91 Train Acc 63.69961929321289% Val Acc 48.400001525878906% Train Loss 0.44729286432266235 Val Loss 1.4058746099472046
Trainable Parameters : 264452
Epoch 92 Train Acc 63.866920471191406% Val Acc 53.10000228881836% Train Loss 0.4503048360347748 Val Loss 1.2172460556030273
Trainable Parameters : 264452
Epoch 93 Train Acc 64.5437240600586% Val Acc 48.0% Train Loss 0.44738703966140747 Val Loss 1.4000294208526611
Trainable Parameters : 264452
Epoch 94 Train Acc 63.84790802001953% Val Acc 51.0% Train Loss 0.4484253525733948 Val Loss 1.2082089185714722
Trainable Parameters : 264452
Epoch 95 Train Acc 63.9467658996582% Val Acc 52.70000076293945% Train Loss 0.44764888286590576 Val Loss 1.2449873685836792
Trainable Parameters : 264452
Epoch 96 Train Acc 64.19771575927734% Val Acc 55.10000228881836% Train Loss 0.447450190782547 Val Loss 1.1310791969299316
Trainable Parameters : 264452
Epoch 97 Train Acc 64.55133056640625% Val Acc 49.70000076293945% Train Loss 0.4438903331756592 Val Loss 1.2941290140151978
Trainable Parameters : 264452
Epoch 98 Train Acc 64.76045227050781% Val Acc 51.5% Train Loss 0.44182032346725464 Val Loss 1.2382335662841797
Trainable Parameters : 264452
Configuration saved in ../output/u_train_700f_local/ADI17-xlsr-arabic/config.json
Model weights saved in ../output/u_train_700f_local/ADI17-xlsr-arabic/pytorch_model.bin
Epoch 99 Train Acc 64.56273651123047% Val Acc 55.79999923706055% Train Loss 0.44179555773735046 Val Loss 1.0988420248031616

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:63.20000076293945% Loss:0.9573761224746704
CONFUSION MATRIX
[[79  6 14  1]
 [14 61 15 10]
 [17  8 68  5]
 [17 16 24 43]]
CONFUSION MATRIX NORMALISED
[[0.19849246 0.01507538 0.03517588 0.00251256]
 [0.03517588 0.15326633 0.03768844 0.02512563]
 [0.04271357 0.0201005  0.17085427 0.01256281]
 [0.04271357 0.04020101 0.06030151 0.1080402 ]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.62      0.79      0.70       100
           1       0.67      0.61      0.64       100
           2       0.56      0.69      0.62        98
           3       0.73      0.43      0.54       100

    accuracy                           0.63       398
   macro avg       0.65      0.63      0.62       398
weighted avg       0.65      0.63      0.62       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 27/10/2022 02:26:14
