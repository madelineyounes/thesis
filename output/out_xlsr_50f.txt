Wed Nov 16 20:01:26 AEDT 2022
2022-11-16 20:01:30.668343: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 20:01:31.510554: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 20:01:33.760231: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 20:01:33.761962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 20:01:33.761977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_50f.py
Started: 16/11/2022 20:01:48

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-50f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_50f
train_filename: u_train_50f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_50f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_50f_local/ADI17-xlsr-araic-50f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_50f_local/ADI17-xlsr-araic-50f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Traceback (most recent call last):
  File "run_xlsr_50f.py", line 378, in <module>
    traincustomdata = CustomDataset(
  File "/home/z5208494/thesis/customData.py", line 35, in __init__
    self.data_frame = pd.read_csv(csv_fp, delimiter=',')
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File data/u_train_50f.csv does not exist: 'data/u_train_50f.csv'
Wed Nov 16 20:07:11 AEDT 2022
2022-11-16 20:07:13.580951: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 20:07:13.905990: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 20:07:15.836379: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 20:07:15.837987: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 20:07:15.838004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_50f.py
Started: 16/11/2022 20:07:29

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-50f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_50f
train_filename: u_train_50f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_50f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_50f_local/ADI17-xlsr-araic-50f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_50f_local/ADI17-xlsr-araic-50f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-4.1184e-01, -6.2687e-01, -8.3080e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5479e+00,  1.1124e+00,  9.7673e-01,  ..., -4.3238e-01,
         -4.4492e-01, -4.6143e-01],
        [-1.2665e-01, -1.0489e-01,  3.0774e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.6847e+00, -4.3403e+00, -3.6533e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.5227e-03,  1.8830e-02,  2.6504e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.3278e-02, -3.1151e-02, -3.5504e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 1, 3, 2, 2, 3, 0, 2, 2, 3, 1, 0, 3, 3, 1, 0, 3, 3, 1, 0, 0, 3, 3,
        3, 1, 3, 2, 3, 3, 0, 3, 3, 2, 0, 0, 2, 0, 2, 0])}
Training DataCustom Files: 176
Training Data Files: 5
Val Data Sample
{'input_values': tensor([[-0.6731, -0.9338, -0.9729,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2721, -0.6399, -1.1309,  ...,  0.0305,  0.0499,  0.0369],
        [ 1.3172,  1.4557,  1.4066,  ..., -3.4226, -3.5324, -3.4491],
        ...,
        [-1.4845, -1.6659, -1.3234,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4742,  0.4430,  0.4316,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2366, -0.2593,  0.4906,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 2, 2, 3, 1, 3, 3, 1, 2, 2, 2, 1, 0, 1, 2, 3, 1, 3, 1, 0, 3, 1, 3, 0,
        3, 1, 0, 1, 0, 3, 0, 3, 0, 1, 1, 3, 1, 0, 2, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.bias', 'classifier.bias', 'projector.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0249, -0.0531, -0.0344,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2160,  0.1194,  0.1858,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0132, -0.0112,  0.0087,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0326, -0.0341, -0.0326,  ...,  0.6475,  1.2068,  0.8513],
        [-0.1916, -0.2201, -0.1819,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0333,  0.0404, -0.0028,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 3, 3, 0, 1, 0, 3, 2, 1, 1, 2, 3, 0, 0, 1, 2, 2, 3, 3, 0, 3, 3, 1,
        2, 0, 2, 0, 0, 3, 2, 2, 2, 1, 2, 0, 2, 2, 2, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 0 Train Acc 23.200000762939453% Val Acc 25.0% Train Loss 0.6908402442932129 Val Loss 1.386853575706482
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 1 Train Acc 26.600000381469727% Val Acc 27.100000381469727% Train Loss 0.6887115836143494 Val Loss 1.3824721574783325
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 2 Train Acc 27.600000381469727% Val Acc 25.200000762939453% Train Loss 0.6908329129219055 Val Loss 1.3884179592132568
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 3 Train Acc 21.600000381469727% Val Acc 25.100000381469727% Train Loss 0.6927470564842224 Val Loss 1.3889719247817993
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 4 Train Acc 24.399999618530273% Val Acc 26.80000114440918% Train Loss 0.689864456653595 Val Loss 1.3878039121627808
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 5 Train Acc 23.600000381469727% Val Acc 27.0% Train Loss 0.6921532154083252 Val Loss 1.3851091861724854
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 6 Train Acc 24.399999618530273% Val Acc 28.5% Train Loss 0.692354142665863 Val Loss 1.3868426084518433
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 7 Train Acc 22.600000381469727% Val Acc 28.399999618530273% Train Loss 0.6920918822288513 Val Loss 1.3809075355529785
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 8 Train Acc 24.200000762939453% Val Acc 26.600000381469727% Train Loss 0.690611720085144 Val Loss 1.3846074342727661
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 9 Train Acc 20.0% Val Acc 26.200000762939453% Train Loss 0.6924872994422913 Val Loss 1.3879809379577637
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 10 Train Acc 23.80000114440918% Val Acc 26.899999618530273% Train Loss 0.6900264024734497 Val Loss 1.3865150213241577
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 11 Train Acc 23.0% Val Acc 26.100000381469727% Train Loss 0.693516731262207 Val Loss 1.382388949394226
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 12 Train Acc 29.600000381469727% Val Acc 26.0% Train Loss 0.6889063715934753 Val Loss 1.3902455568313599
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 13 Train Acc 27.399999618530273% Val Acc 28.5% Train Loss 0.6919862031936646 Val Loss 1.3875983953475952
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 14 Train Acc 28.200000762939453% Val Acc 26.200000762939453% Train Loss 0.6879867315292358 Val Loss 1.385679841041565
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 15 Train Acc 26.80000114440918% Val Acc 24.600000381469727% Train Loss 0.6869298815727234 Val Loss 1.390866756439209
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 16 Train Acc 31.200000762939453% Val Acc 27.899999618530273% Train Loss 0.6878659129142761 Val Loss 1.3848809003829956
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 17 Train Acc 28.0% Val Acc 27.600000381469727% Train Loss 0.6890737414360046 Val Loss 1.3838876485824585
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 18 Train Acc 28.200000762939453% Val Acc 29.100000381469727% Train Loss 0.6877561211585999 Val Loss 1.3806756734848022
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 19 Train Acc 28.80000114440918% Val Acc 26.5% Train Loss 0.6879344582557678 Val Loss 1.3851287364959717
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 20 Train Acc 27.600000381469727% Val Acc 29.700000762939453% Train Loss 0.6877707839012146 Val Loss 1.3894102573394775
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 21 Train Acc 34.79999923706055% Val Acc 30.30000114440918% Train Loss 0.6863124966621399 Val Loss 1.3879200220108032
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 22 Train Acc 31.0% Val Acc 30.399999618530273% Train Loss 0.6864961981773376 Val Loss 1.3828877210617065
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 23 Train Acc 33.20000076293945% Val Acc 31.0% Train Loss 0.6870565414428711 Val Loss 1.3811949491500854
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 24 Train Acc 37.60000228881836% Val Acc 30.80000114440918% Train Loss 0.687837541103363 Val Loss 1.3941227197647095
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 25 Train Acc 31.0% Val Acc 34.0% Train Loss 0.6883327960968018 Val Loss 1.3817212581634521
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 26 Train Acc 31.399999618530273% Val Acc 32.400001525878906% Train Loss 0.6847265362739563 Val Loss 1.3921153545379639
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 27 Train Acc 33.60000228881836% Val Acc 32.5% Train Loss 0.6854775547981262 Val Loss 1.3850375413894653
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 28 Train Acc 43.0% Val Acc 35.60000228881836% Train Loss 0.6819609999656677 Val Loss 1.386379599571228
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 29 Train Acc 38.20000076293945% Val Acc 33.79999923706055% Train Loss 0.6822181344032288 Val Loss 1.382297396659851
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 30 Train Acc 35.400001525878906% Val Acc 35.5% Train Loss 0.6857027411460876 Val Loss 1.3795965909957886
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 31 Train Acc 38.0% Val Acc 33.79999923706055% Train Loss 0.6858569383621216 Val Loss 1.392781138420105
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 32 Train Acc 43.20000076293945% Val Acc 35.5% Train Loss 0.6814775466918945 Val Loss 1.3812133073806763
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 33 Train Acc 39.400001525878906% Val Acc 39.400001525878906% Train Loss 0.6837177276611328 Val Loss 1.3722164630889893
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 34 Train Acc 41.79999923706055% Val Acc 32.900001525878906% Train Loss 0.6804017424583435 Val Loss 1.3883109092712402
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 35 Train Acc 40.20000076293945% Val Acc 34.60000228881836% Train Loss 0.6784930229187012 Val Loss 1.3818353414535522
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 36 Train Acc 42.79999923706055% Val Acc 34.5% Train Loss 0.6802656650543213 Val Loss 1.3819631338119507
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 37 Train Acc 42.79999923706055% Val Acc 32.70000076293945% Train Loss 0.6789156198501587 Val Loss 1.3772720098495483
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 38 Train Acc 43.60000228881836% Val Acc 33.400001525878906% Train Loss 0.6778351664543152 Val Loss 1.381266474723816
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 39 Train Acc 42.0% Val Acc 34.20000076293945% Train Loss 0.6764699816703796 Val Loss 1.380574345588684
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 40 Train Acc 48.0% Val Acc 29.899999618530273% Train Loss 0.6742538213729858 Val Loss 1.3887561559677124
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 41 Train Acc 46.60000228881836% Val Acc 31.5% Train Loss 0.6742041707038879 Val Loss 1.384018063545227
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 42 Train Acc 42.60000228881836% Val Acc 32.20000076293945% Train Loss 0.6756477355957031 Val Loss 1.3863162994384766
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 43 Train Acc 49.20000076293945% Val Acc 30.100000381469727% Train Loss 0.6761456727981567 Val Loss 1.3831052780151367
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 44 Train Acc 47.79999923706055% Val Acc 32.0% Train Loss 0.6711534857749939 Val Loss 1.380625605583191
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 45 Train Acc 46.20000076293945% Val Acc 33.0% Train Loss 0.6680547595024109 Val Loss 1.3847085237503052
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 46 Train Acc 47.79999923706055% Val Acc 32.900001525878906% Train Loss 0.6684014797210693 Val Loss 1.3786778450012207
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 47 Train Acc 45.79999923706055% Val Acc 35.900001525878906% Train Loss 0.6713233590126038 Val Loss 1.3760435581207275
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 48 Train Acc 48.60000228881836% Val Acc 32.10000228881836% Train Loss 0.6651328206062317 Val Loss 1.382112979888916
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Epoch 49 Train Acc 50.79999923706055% Val Acc 30.80000114440918% Train Loss 0.6650135517120361 Val Loss 1.3844596147537231
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 50 Train Acc 51.20000076293945% Val Acc 34.400001525878906% Train Loss 0.6671265959739685 Val Loss 1.3713589906692505
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 51 Train Acc 53.79999923706055% Val Acc 34.29999923706055% Train Loss 0.6587874293327332 Val Loss 1.3634783029556274
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 52 Train Acc 52.0% Val Acc 31.700000762939453% Train Loss 0.661595344543457 Val Loss 1.3778527975082397
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 53 Train Acc 53.20000076293945% Val Acc 34.5% Train Loss 0.6573988199234009 Val Loss 1.3773174285888672
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 54 Train Acc 53.60000228881836% Val Acc 33.0% Train Loss 0.6539599299430847 Val Loss 1.377456545829773
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 55 Train Acc 53.79999923706055% Val Acc 33.10000228881836% Train Loss 0.6533002257347107 Val Loss 1.3699222803115845
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 56 Train Acc 52.79999923706055% Val Acc 30.700000762939453% Train Loss 0.6484493613243103 Val Loss 1.3785953521728516
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 57 Train Acc 56.79999923706055% Val Acc 29.899999618530273% Train Loss 0.6472749710083008 Val Loss 1.3874003887176514
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 58 Train Acc 55.20000076293945% Val Acc 32.70000076293945% Train Loss 0.6416286826133728 Val Loss 1.3701846599578857
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 59 Train Acc 55.20000076293945% Val Acc 30.100000381469727% Train Loss 0.64016193151474 Val Loss 1.3785605430603027
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 60 Train Acc 55.79999923706055% Val Acc 32.29999923706055% Train Loss 0.640117347240448 Val Loss 1.3715876340866089
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 61 Train Acc 61.79999923706055% Val Acc 33.0% Train Loss 0.6283570528030396 Val Loss 1.367148756980896
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 62 Train Acc 59.0% Val Acc 33.0% Train Loss 0.6324413418769836 Val Loss 1.3648427724838257
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 63 Train Acc 59.60000228881836% Val Acc 30.600000381469727% Train Loss 0.6248544454574585 Val Loss 1.363657832145691
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 64 Train Acc 61.0% Val Acc 35.10000228881836% Train Loss 0.6148319244384766 Val Loss 1.3523415327072144
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 65 Train Acc 65.80000305175781% Val Acc 33.400001525878906% Train Loss 0.6021733283996582 Val Loss 1.3513222932815552
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 66 Train Acc 61.400001525878906% Val Acc 29.80000114440918% Train Loss 0.6037030816078186 Val Loss 1.3668546676635742
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 67 Train Acc 63.0% Val Acc 28.399999618530273% Train Loss 0.5904240012168884 Val Loss 1.3690110445022583
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 68 Train Acc 57.400001525878906% Val Acc 33.29999923706055% Train Loss 0.589735209941864 Val Loss 1.3484282493591309
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 69 Train Acc 66.80000305175781% Val Acc 28.5% Train Loss 0.5722290277481079 Val Loss 1.3693362474441528
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 70 Train Acc 65.80000305175781% Val Acc 32.79999923706055% Train Loss 0.5646771192550659 Val Loss 1.3589067459106445
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 71 Train Acc 65.0% Val Acc 30.399999618530273% Train Loss 0.561366856098175 Val Loss 1.386514663696289
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 72 Train Acc 68.4000015258789% Val Acc 30.100000381469727% Train Loss 0.5533768534660339 Val Loss 1.3777183294296265
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 73 Train Acc 67.20000457763672% Val Acc 31.5% Train Loss 0.5308970212936401 Val Loss 1.3763974905014038
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 74 Train Acc 65.20000457763672% Val Acc 29.899999618530273% Train Loss 0.5287608504295349 Val Loss 1.4014948606491089
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 75 Train Acc 65.20000457763672% Val Acc 35.79999923706055% Train Loss 0.5200769305229187 Val Loss 1.3856366872787476
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 76 Train Acc 68.5999984741211% Val Acc 33.0% Train Loss 0.5125967264175415 Val Loss 1.4008066654205322
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 77 Train Acc 65.80000305175781% Val Acc 29.80000114440918% Train Loss 0.4950975477695465 Val Loss 1.4356493949890137
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 78 Train Acc 71.5999984741211% Val Acc 28.899999618530273% Train Loss 0.4623924195766449 Val Loss 1.4508509635925293
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 79 Train Acc 69.4000015258789% Val Acc 33.400001525878906% Train Loss 0.46318545937538147 Val Loss 1.4478068351745605
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 80 Train Acc 72.4000015258789% Val Acc 33.29999923706055% Train Loss 0.44605207443237305 Val Loss 1.433634638786316
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 81 Train Acc 72.80000305175781% Val Acc 34.900001525878906% Train Loss 0.4317643642425537 Val Loss 1.4486489295959473
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 82 Train Acc 75.80000305175781% Val Acc 36.79999923706055% Train Loss 0.4071521461009979 Val Loss 1.4195529222488403
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 83 Train Acc 76.20000457763672% Val Acc 35.400001525878906% Train Loss 0.38534507155418396 Val Loss 1.4968782663345337
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 84 Train Acc 71.4000015258789% Val Acc 33.10000228881836% Train Loss 0.40195009112358093 Val Loss 1.563117265701294
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 85 Train Acc 78.5999984741211% Val Acc 36.70000076293945% Train Loss 0.36319002509117126 Val Loss 1.4698933362960815
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 86 Train Acc 78.4000015258789% Val Acc 33.10000228881836% Train Loss 0.33887967467308044 Val Loss 1.5803977251052856
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 87 Train Acc 78.0% Val Acc 36.20000076293945% Train Loss 0.3221629559993744 Val Loss 1.5045772790908813
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 88 Train Acc 80.80000305175781% Val Acc 35.5% Train Loss 0.32551559805870056 Val Loss 1.5481562614440918
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 89 Train Acc 84.5999984741211% Val Acc 34.5% Train Loss 0.2974611818790436 Val Loss 1.6479816436767578
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 90 Train Acc 87.80000305175781% Val Acc 34.10000228881836% Train Loss 0.27424970269203186 Val Loss 1.7043966054916382
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 91 Train Acc 83.80000305175781% Val Acc 33.900001525878906% Train Loss 0.2766522765159607 Val Loss 1.6984878778457642
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 92 Train Acc 86.20000457763672% Val Acc 36.400001525878906% Train Loss 0.2394982874393463 Val Loss 1.766068458557129
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 93 Train Acc 84.80000305175781% Val Acc 37.400001525878906% Train Loss 0.24434569478034973 Val Loss 1.6301872730255127
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 94 Train Acc 93.20000457763672% Val Acc 32.5% Train Loss 0.2165059596300125 Val Loss 2.010462760925293
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 95 Train Acc 92.4000015258789% Val Acc 31.700000762939453% Train Loss 0.20613005757331848 Val Loss 1.941774606704712
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 96 Train Acc 91.5999984741211% Val Acc 29.700000762939453% Train Loss 0.1883547604084015 Val Loss 1.9277076721191406
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 97 Train Acc 94.0% Val Acc 32.400001525878906% Train Loss 0.17210344970226288 Val Loss 2.08494234085083
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 98 Train Acc 94.80000305175781% Val Acc 35.400001525878906% Train Loss 0.17460626363754272 Val Loss 1.9424809217453003
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Configuration saved in /srv/scratch/z5208494/output/u_train_50f_local/ADI17-xlsr-araic-50f/config.json
Model weights saved in /srv/scratch/z5208494/output/u_train_50f_local/ADI17-xlsr-araic-50f/pytorch_model.bin
Epoch 99 Train Acc 93.80000305175781% Val Acc 40.10000228881836% Train Loss 0.17114518582820892 Val Loss 1.9087278842926025

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:42.400001525878906% Loss:1.6975898742675781
CONFUSION MATRIX
[[23 36 17 24]
 [ 8 47 10 35]
 [ 5  7 45 41]
 [ 3 25 18 54]]
CONFUSION MATRIX NORMALISED
[[0.05778894 0.09045226 0.04271357 0.06030151]
 [0.0201005  0.11809045 0.02512563 0.0879397 ]
 [0.01256281 0.01758794 0.11306533 0.10301508]
 [0.00753769 0.06281407 0.04522613 0.13567839]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.59      0.23      0.33       100
           1       0.41      0.47      0.44       100
           2       0.50      0.46      0.48        98
           3       0.35      0.54      0.43       100

    accuracy                           0.42       398
   macro avg       0.46      0.42      0.42       398
weighted avg       0.46      0.42      0.42       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 16/11/2022 20:34:33
