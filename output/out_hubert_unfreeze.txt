Sun Nov 6 22:16:29 AEDT 2022
2022-11-06 22:16:31.624031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-06 22:16:31.919150: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-06 22:16:33.559476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-06 22:16:33.559775: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-06 22:16:33.559825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_hubert2.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_hubert_unfreeze.py
Started: 06/11/2022 22:16:48

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-hubert-unfreeze
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-hubert-unfreeze
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-hubert-unfreeze_finetuned_results.csv
--> pretrained_mod: facebook/hubert-base-ls960

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 7.3062e-01,  5.5326e+00,  1.5959e+01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.9366e+00, -3.5550e+00, -4.0260e+00,  ..., -8.8373e-02,
         -7.4836e-02, -1.0814e-01],
        [ 2.9429e+00,  3.5390e+00,  3.9686e+00,  ..., -6.5419e-02,
          8.5660e-03, -9.8437e-02],
        ...,
        [-4.4605e-02,  2.0410e-01,  3.7949e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.6831e-01, -5.0843e-01, -2.3638e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.2495e-02, -4.0585e-02, -6.1780e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 3, 0, 2, 2, 3, 0, 2, 0, 2, 1, 1, 2, 2, 3, 1, 3, 2, 0, 2, 3, 2, 1, 2,
        2, 3, 1, 2, 1, 3, 1, 3, 2, 3, 0, 2, 2, 1, 3, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.1657, -0.1266, -0.1126,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2011,  0.2387,  0.2733,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0614,  0.0629,  0.0494,  ...,  0.7316,  0.5907,  1.0034],
        ...,
        [ 0.2130,  0.4055,  0.6065,  ...,  0.0947,  0.0785,  0.0367],
        [ 0.8563,  0.1170, -0.2756,  ...,  0.0000,  0.0000,  0.0000],
        [ 2.2691,  2.2280,  1.7782,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 2, 3, 1, 0, 2, 0, 3, 0, 3, 1, 3, 0, 1, 0, 2, 2, 1, 2, 0, 3, 2, 1,
        2, 0, 0, 2, 0, 0, 2, 0, 0, 1, 3, 0, 1, 0, 0, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['projector.weight', 'projector.bias', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'input_values': tensor([[-0.2430,  0.4951,  1.2642,  ..., -1.6295, -1.5697, -1.4536],
        [-0.9951, -0.5809, -0.3378,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1632,  0.1285,  0.1211,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0677, -0.0702, -0.0585,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0859,  0.0620,  0.0274,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.6544,  1.0482,  1.3101,  ..., -0.0133,  0.0113, -0.0037]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 1, 1, 1, 2, 0, 3, 2, 1, 3, 2, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 0, 3,
        1, 0, 2, 3, 3, 0, 3, 0, 2, 2, 3, 0, 2, 2, 1, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
Traceback (most recent call last):
  File "run_hubert_unfreeze.py", line 491, in <module>
    for param in model.wav2vec2.encoder.layers[i].parameters():
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DataParallel' object has no attribute 'wav2vec2'
Mon Nov 7 00:42:11 AEDT 2022
2022-11-07 00:42:13.580169: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-07 00:42:13.812340: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-07 00:42:13.851073: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-07 00:42:15.570091: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-07 00:42:15.570212: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-07 00:42:15.570222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_hubert2.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_hubert_unfreeze.py
Started: 07/11/2022 00:42:29

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-hubert-unfreeze
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-hubert-unfreeze
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-hubert-unfreeze_finetuned_results.csv
--> pretrained_mod: facebook/hubert-base-ls960

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 7.4783e-03,  1.9694e-03,  2.4941e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2443e+00,  1.1593e+00,  9.9715e-01,  ..., -8.3146e-03,
          2.4121e-01,  4.7985e-01],
        [ 4.9892e+00,  3.9049e+00,  3.3774e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 4.9357e-02,  9.5271e-01,  1.8098e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.8228e-01,  6.9042e-01,  6.7414e-01,  ...,  1.0636e+00,
          9.5096e-01,  8.3019e-01],
        [-1.7937e+00, -1.7738e+00, -1.6961e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 3, 3, 2, 2, 0, 0, 1, 2, 2, 2, 3, 2, 0, 2, 2, 3, 2, 0, 2, 3, 2, 3, 3,
        3, 2, 0, 2, 2, 2, 1, 2, 1, 0, 3, 1, 1, 0, 2, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.0736, -0.0909, -0.0920,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2913,  0.2363,  0.2965,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.7705,  1.3181,  0.6450,  ...,  1.8724,  1.7984,  1.7190],
        ...,
        [-0.0452, -0.0422, -0.0515,  ...,  0.0000,  0.0000,  0.0000],
        [-1.7203, -1.6405, -1.7557,  ..., -1.4002, -1.1597, -0.8528],
        [ 0.1578,  0.1401,  0.1212,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 3, 3, 1, 3, 3, 1, 2, 1, 0, 3, 3, 0, 0, 3, 3, 3, 3, 2, 2, 1, 1, 0,
        3, 0, 2, 2, 0, 2, 3, 2, 3, 3, 1, 3, 0, 3, 3, 3])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['classifier.bias', 'projector.bias', 'classifier.weight', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.4360, -0.5051, -0.5577,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0080,  0.0080,  0.0080,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0131,  0.0355,  0.0252,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0412, -0.0685, -0.0758,  ...,  0.0000,  0.0000,  0.0000],
        [ 2.3649,  2.5257,  2.4876,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2564, -0.1534, -0.0796,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 2, 1, 2, 3, 2, 3, 0, 3, 0, 0, 2, 0, 1, 1, 2, 0, 0, 2, 3, 2, 2, 0,
        0, 1, 2, 2, 3, 2, 0, 1, 1, 2, 3, 2, 0, 1, 1, 3])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 85253124
Epoch 0 Train Acc 42.174903869628906% Val Acc 33.29999923706055% Train Loss 0.6399759650230408 Val Loss 1.3993264436721802
EPOCH unfeeze : 1
Trainable Parameters : 85253124
Epoch 1 Train Acc 54.01140594482422% Val Acc 40.20000076293945% Train Loss 0.5582648515701294 Val Loss 1.4002741575241089
EPOCH unfeeze : 2
Trainable Parameters : 85253124
Epoch 2 Train Acc 62.74905014038086% Val Acc 56.70000076293945% Train Loss 0.474632203578949 Val Loss 1.1040185689926147
EPOCH unfeeze : 3
Trainable Parameters : 85253124
Epoch 3 Train Acc 69.31938934326172% Val Acc 49.10000228881836% Train Loss 0.4042671322822571 Val Loss 1.4190659523010254
EPOCH unfeeze : 4
Trainable Parameters : 85253124
Epoch 4 Train Acc 73.91635131835938% Val Acc 64.70000457763672% Train Loss 0.350772887468338 Val Loss 1.0300099849700928
EPOCH unfeeze : 5
Trainable Parameters : 85253124
Epoch 5 Train Acc 76.71102905273438% Val Acc 66.5% Train Loss 0.3116689622402191 Val Loss 0.9511523246765137
EPOCH unfeeze : 6
Trainable Parameters : 85253124
Epoch 6 Train Acc 79.65779113769531% Val Acc 58.20000076293945% Train Loss 0.2813716530799866 Val Loss 1.2308458089828491
EPOCH unfeeze : 7
Trainable Parameters : 85253124
Epoch 7 Train Acc 82.69581604003906% Val Acc 65.5% Train Loss 0.24173833429813385 Val Loss 1.1684211492538452
EPOCH unfeeze : 8
Trainable Parameters : 85253124
Epoch 8 Train Acc 82.99620056152344% Val Acc 73.0999984741211% Train Loss 0.23433980345726013 Val Loss 0.8756784796714783
EPOCH unfeeze : 9
Trainable Parameters : 85253124
Epoch 9 Train Acc 84.47528076171875% Val Acc 61.60000228881836% Train Loss 0.21470697224140167 Val Loss 1.2939250469207764
EPOCH unfeeze : 10
Trainable Parameters : 85253124
Epoch 10 Train Acc 86.09505462646484% Val Acc 72.5% Train Loss 0.20344863831996918 Val Loss 1.1129757165908813
EPOCH unfeeze : 11
Trainable Parameters : 85253124
Epoch 11 Train Acc 86.08364868164062% Val Acc 70.80000305175781% Train Loss 0.19951215386390686 Val Loss 1.1308718919754028
EPOCH unfeeze : 12
Trainable Parameters : 85253124
Epoch 12 Train Acc 86.8973388671875% Val Acc 64.80000305175781% Train Loss 0.18377608060836792 Val Loss 1.1481163501739502
EPOCH unfeeze : 13
Trainable Parameters : 85253124
Epoch 13 Train Acc 87.80608367919922% Val Acc 61.29999923706055% Train Loss 0.17697292566299438 Val Loss 1.4911746978759766
EPOCH unfeeze : 14
Trainable Parameters : 85253124
Epoch 14 Train Acc 88.1330795288086% Val Acc 68.20000457763672% Train Loss 0.17050236463546753 Val Loss 1.044677734375
EPOCH unfeeze : 15
Trainable Parameters : 85253124
Epoch 15 Train Acc 88.65779113769531% Val Acc 56.5% Train Loss 0.16754701733589172 Val Loss 1.415338397026062
EPOCH unfeeze : 16
Trainable Parameters : 85253124
Epoch 16 Train Acc 88.52471160888672% Val Acc 63.0% Train Loss 0.16195324063301086 Val Loss 1.6988563537597656
EPOCH unfeeze : 17
Trainable Parameters : 85253124
Epoch 17 Train Acc 88.205322265625% Val Acc 64.70000457763672% Train Loss 0.16579410433769226 Val Loss 1.5507748126983643
EPOCH unfeeze : 18
Trainable Parameters : 85253124
Epoch 18 Train Acc 88.45246887207031% Val Acc 72.20000457763672% Train Loss 0.16975656151771545 Val Loss 1.0982074737548828
EPOCH unfeeze : 19
Trainable Parameters : 85253124
Epoch 19 Train Acc 88.12928009033203% Val Acc 68.70000457763672% Train Loss 0.1704782396554947 Val Loss 1.4034260511398315
EPOCH unfeeze : 20
Trainable Parameters : 85253124
Epoch 20 Train Acc 88.39163208007812% Val Acc 59.400001525878906% Train Loss 0.16712673008441925 Val Loss 1.3683160543441772
EPOCH unfeeze : 21
Trainable Parameters : 85253124
Epoch 21 Train Acc 87.93155670166016% Val Acc 58.5% Train Loss 0.17436008155345917 Val Loss 1.5756343603134155
EPOCH unfeeze : 22
Trainable Parameters : 85253124
Epoch 22 Train Acc 88.59315490722656% Val Acc 67.30000305175781% Train Loss 0.16406551003456116 Val Loss 1.158061146736145
EPOCH unfeeze : 23
Trainable Parameters : 85253124
Epoch 23 Train Acc 87.3384017944336% Val Acc 69.5% Train Loss 0.18232202529907227 Val Loss 1.1241730451583862
EPOCH unfeeze : 24
Trainable Parameters : 85253124
Epoch 24 Train Acc 87.65399169921875% Val Acc 59.79999923706055% Train Loss 0.17847822606563568 Val Loss 1.622979760169983
EPOCH unfeeze : 25
Trainable Parameters : 85253124
Epoch 25 Train Acc 87.09505462646484% Val Acc 69.30000305175781% Train Loss 0.18497934937477112 Val Loss 1.1298555135726929
EPOCH unfeeze : 26
Trainable Parameters : 85253124
Epoch 26 Train Acc 86.08744812011719% Val Acc 58.0% Train Loss 0.19803179800510406 Val Loss 1.5202852487564087
EPOCH unfeeze : 27
Trainable Parameters : 85253124
Epoch 27 Train Acc 85.09886169433594% Val Acc 61.900001525878906% Train Loss 0.2090345323085785 Val Loss 1.419751524925232
EPOCH unfeeze : 28
Trainable Parameters : 85253124
Epoch 28 Train Acc 85.83270263671875% Val Acc 61.0% Train Loss 0.20540153980255127 Val Loss 1.4291620254516602
EPOCH unfeeze : 29
Trainable Parameters : 85253124
Epoch 29 Train Acc 85.11026763916016% Val Acc 60.20000076293945% Train Loss 0.20893873274326324 Val Loss 1.6419525146484375
EPOCH unfeeze : 30
Trainable Parameters : 85253124
Epoch 30 Train Acc 83.32699584960938% Val Acc 63.60000228881836% Train Loss 0.23355616629123688 Val Loss 1.3580679893493652
EPOCH unfeeze : 31
Trainable Parameters : 85253124
Epoch 31 Train Acc 82.4828872680664% Val Acc 61.0% Train Loss 0.24327804148197174 Val Loss 1.1418193578720093
EPOCH unfeeze : 32
Trainable Parameters : 85253124
Epoch 32 Train Acc 81.6463851928711% Val Acc 59.900001525878906% Train Loss 0.2624412775039673 Val Loss 1.2099087238311768
EPOCH unfeeze : 33
Trainable Parameters : 85253124
Epoch 33 Train Acc 80.32699584960938% Val Acc 58.20000076293945% Train Loss 0.27837085723876953 Val Loss 1.391862154006958
EPOCH unfeeze : 34
Trainable Parameters : 85253124
Epoch 34 Train Acc 77.65019226074219% Val Acc 59.10000228881836% Train Loss 0.31330665946006775 Val Loss 1.6244328022003174
EPOCH unfeeze : 35
Trainable Parameters : 85253124
Epoch 35 Train Acc 75.00379943847656% Val Acc 52.60000228881836% Train Loss 0.34585681557655334 Val Loss 1.1580638885498047
EPOCH unfeeze : 36
Trainable Parameters : 85253124
Epoch 36 Train Acc 71.1330795288086% Val Acc 41.29999923706055% Train Loss 0.3944753408432007 Val Loss 1.3028308153152466
EPOCH unfeeze : 37
Trainable Parameters : 85253124
Epoch 37 Train Acc 60.737640380859375% Val Acc 36.5% Train Loss 0.5037328004837036 Val Loss 1.7131513357162476
EPOCH unfeeze : 38
Trainable Parameters : 85253124
Epoch 38 Train Acc 41.927757263183594% Val Acc 24.30000114440918% Train Loss 0.6406229734420776 Val Loss 1.503875970840454
EPOCH unfeeze : 39
Trainable Parameters : 85253124
Epoch 39 Train Acc 40.064640045166016% Val Acc 26.5% Train Loss 0.6543631553649902 Val Loss 1.4616936445236206
EPOCH unfeeze : 40
Trainable Parameters : 85253124
Epoch 40 Train Acc 40.46007537841797% Val Acc 27.5% Train Loss 0.6523263454437256 Val Loss 1.4371092319488525
EPOCH unfeeze : 41
Trainable Parameters : 85253124
Epoch 41 Train Acc 39.79847717285156% Val Acc 27.399999618530273% Train Loss 0.6572504639625549 Val Loss 1.4576960802078247
EPOCH unfeeze : 42
Trainable Parameters : 85253124
Epoch 42 Train Acc 40.05323028564453% Val Acc 25.700000762939453% Train Loss 0.6561173796653748 Val Loss 1.459519386291504
EPOCH unfeeze : 43
Trainable Parameters : 85253124
Epoch 43 Train Acc 40.00380325317383% Val Acc 27.200000762939453% Train Loss 0.655775785446167 Val Loss 1.4536455869674683
EPOCH unfeeze : 44
Trainable Parameters : 85253124
Epoch 44 Train Acc 39.980987548828125% Val Acc 25.30000114440918% Train Loss 0.6551792025566101 Val Loss 1.4682987928390503
EPOCH unfeeze : 45
Trainable Parameters : 85253124
Epoch 45 Train Acc 39.9467658996582% Val Acc 25.80000114440918% Train Loss 0.655848503112793 Val Loss 1.469295859336853
EPOCH unfeeze : 46
Trainable Parameters : 85253124
Epoch 46 Train Acc 39.96197509765625% Val Acc 23.5% Train Loss 0.655750572681427 Val Loss 1.4598976373672485
EPOCH unfeeze : 47
Trainable Parameters : 85253124
Epoch 47 Train Acc 40.026615142822266% Val Acc 23.200000762939453% Train Loss 0.6554415225982666 Val Loss 1.4853935241699219
EPOCH unfeeze : 48
Trainable Parameters : 85253124
Epoch 48 Train Acc 40.05323028564453% Val Acc 25.600000381469727% Train Loss 0.6553447246551514 Val Loss 1.455456256866455
EPOCH unfeeze : 49
Trainable Parameters : 85253124
Epoch 49 Train Acc 40.03422164916992% Val Acc 26.899999618530273% Train Loss 0.6554656028747559 Val Loss 1.4590139389038086
EPOCH unfeeze : 0
Trainable Parameters : 85253124
Epoch 50 Train Acc 40.04182434082031% Val Acc 25.5% Train Loss 0.6553659439086914 Val Loss 1.4787780046463013
EPOCH unfeeze : 1
Trainable Parameters : 85253124
Epoch 51 Train Acc 40.030418395996094% Val Acc 25.80000114440918% Train Loss 0.6555600166320801 Val Loss 1.4690035581588745
EPOCH unfeeze : 2
Trainable Parameters : 85253124
Epoch 52 Train Acc 40.01520919799805% Val Acc 25.30000114440918% Train Loss 0.6550938487052917 Val Loss 1.4869626760482788
EPOCH unfeeze : 3
Trainable Parameters : 85253124
Epoch 53 Train Acc 40.026615142822266% Val Acc 23.80000114440918% Train Loss 0.6550792455673218 Val Loss 1.4730809926986694
EPOCH unfeeze : 4
Trainable Parameters : 85253124
Epoch 54 Train Acc 40.026615142822266% Val Acc 25.30000114440918% Train Loss 0.6553654074668884 Val Loss 1.482317328453064
EPOCH unfeeze : 5
Trainable Parameters : 85253124
Epoch 55 Train Acc 40.04562759399414% Val Acc 22.899999618530273% Train Loss 0.6551801562309265 Val Loss 1.5323023796081543
EPOCH unfeeze : 6
Trainable Parameters : 85253124
Epoch 56 Train Acc 40.019012451171875% Val Acc 25.899999618530273% Train Loss 0.6551306843757629 Val Loss 1.4817088842391968
EPOCH unfeeze : 7
Trainable Parameters : 85253124
Epoch 57 Train Acc 40.01140594482422% Val Acc 25.0% Train Loss 0.6552258133888245 Val Loss 1.4796251058578491
EPOCH unfeeze : 8
Trainable Parameters : 85253124
Epoch 58 Train Acc 40.04182434082031% Val Acc 25.100000381469727% Train Loss 0.6552574038505554 Val Loss 1.4646281003952026
EPOCH unfeeze : 9
Trainable Parameters : 85253124
Epoch 59 Train Acc 40.05703353881836% Val Acc 26.899999618530273% Train Loss 0.6548635363578796 Val Loss 1.50056791305542
EPOCH unfeeze : 10
Trainable Parameters : 85253124
Epoch 60 Train Acc 40.03422164916992% Val Acc 25.5% Train Loss 0.6551905274391174 Val Loss 1.458276629447937
EPOCH unfeeze : 11
Trainable Parameters : 85253124
Epoch 61 Train Acc 40.10646438598633% Val Acc 23.600000381469727% Train Loss 0.6551439166069031 Val Loss 1.4842451810836792
EPOCH unfeeze : 12
Trainable Parameters : 85253124
Epoch 62 Train Acc 39.99619674682617% Val Acc 23.5% Train Loss 0.6556097269058228 Val Loss 1.482053518295288
EPOCH unfeeze : 13
Trainable Parameters : 85253124
Epoch 63 Train Acc 40.02281188964844% Val Acc 23.899999618530273% Train Loss 0.6552550196647644 Val Loss 1.4737218618392944
EPOCH unfeeze : 14
Trainable Parameters : 85253124
Epoch 64 Train Acc 39.96197509765625% Val Acc 24.399999618530273% Train Loss 0.6552045345306396 Val Loss 1.486279845237732
EPOCH unfeeze : 15
Trainable Parameters : 85253124
Epoch 65 Train Acc 40.03422164916992% Val Acc 25.899999618530273% Train Loss 0.6551793813705444 Val Loss 1.4588332176208496
EPOCH unfeeze : 16
Trainable Parameters : 85253124
Epoch 66 Train Acc 40.00380325317383% Val Acc 26.100000381469727% Train Loss 0.6553253531455994 Val Loss 1.4657373428344727
EPOCH unfeeze : 17
Trainable Parameters : 85253124
Epoch 67 Train Acc 40.019012451171875% Val Acc 25.30000114440918% Train Loss 0.655096709728241 Val Loss 1.4831513166427612
EPOCH unfeeze : 18
Trainable Parameters : 85253124
Epoch 68 Train Acc 39.96577835083008% Val Acc 25.200000762939453% Train Loss 0.6552764773368835 Val Loss 1.4864451885223389
EPOCH unfeeze : 19
Trainable Parameters : 85253124
Epoch 69 Train Acc 40.019012451171875% Val Acc 24.80000114440918% Train Loss 0.6551836729049683 Val Loss 1.5034844875335693
EPOCH unfeeze : 20
Trainable Parameters : 85253124
Epoch 70 Train Acc 40.03422164916992% Val Acc 23.899999618530273% Train Loss 0.6551621556282043 Val Loss 1.5111253261566162
EPOCH unfeeze : 21
Trainable Parameters : 85253124
Epoch 71 Train Acc 40.05323028564453% Val Acc 28.200000762939453% Train Loss 0.6551775336265564 Val Loss 1.4763323068618774
EPOCH unfeeze : 22
Trainable Parameters : 85253124
Epoch 72 Train Acc 40.02281188964844% Val Acc 24.899999618530273% Train Loss 0.6550706028938293 Val Loss 1.5071052312850952
EPOCH unfeeze : 23
Trainable Parameters : 85253124
Epoch 73 Train Acc 40.00760269165039% Val Acc 22.5% Train Loss 0.6553542017936707 Val Loss 1.502301573753357
EPOCH unfeeze : 24
Trainable Parameters : 85253124
Epoch 74 Train Acc 40.030418395996094% Val Acc 21.899999618530273% Train Loss 0.6554752588272095 Val Loss 1.4976110458374023
EPOCH unfeeze : 25
Trainable Parameters : 85253124
Epoch 75 Train Acc 40.00760269165039% Val Acc 26.30000114440918% Train Loss 0.654985785484314 Val Loss 1.4726403951644897
EPOCH unfeeze : 26
Trainable Parameters : 85253124
Epoch 76 Train Acc 40.030418395996094% Val Acc 27.600000381469727% Train Loss 0.655178964138031 Val Loss 1.4823113679885864
EPOCH unfeeze : 27
Trainable Parameters : 85253124
Epoch 77 Train Acc 39.980987548828125% Val Acc 25.100000381469727% Train Loss 0.6553519368171692 Val Loss 1.514133095741272
EPOCH unfeeze : 28
Trainable Parameters : 85253124
Epoch 78 Train Acc 40.01520919799805% Val Acc 23.5% Train Loss 0.6551162004470825 Val Loss 1.4952843189239502
EPOCH unfeeze : 29
Trainable Parameters : 85253124
Epoch 79 Train Acc 40.038021087646484% Val Acc 25.80000114440918% Train Loss 0.655112087726593 Val Loss 1.5024017095565796
EPOCH unfeeze : 30
Trainable Parameters : 85253124
Epoch 80 Train Acc 40.038021087646484% Val Acc 25.700000762939453% Train Loss 0.6551164388656616 Val Loss 1.4733532667160034
EPOCH unfeeze : 31
Trainable Parameters : 85253124
Epoch 81 Train Acc 39.98479080200195% Val Acc 25.30000114440918% Train Loss 0.6551296710968018 Val Loss 1.463808298110962
EPOCH unfeeze : 32
Trainable Parameters : 85253124
Epoch 82 Train Acc 40.04562759399414% Val Acc 22.100000381469727% Train Loss 0.6547600626945496 Val Loss 1.5071932077407837
EPOCH unfeeze : 33
Trainable Parameters : 85253124
Epoch 83 Train Acc 40.04562759399414% Val Acc 25.80000114440918% Train Loss 0.655113935470581 Val Loss 1.475403904914856
EPOCH unfeeze : 34
Trainable Parameters : 85253124
Epoch 84 Train Acc 40.0% Val Acc 24.200000762939453% Train Loss 0.6549224257469177 Val Loss 1.522022008895874
EPOCH unfeeze : 35
Trainable Parameters : 85253124
Epoch 85 Train Acc 40.01520919799805% Val Acc 26.200000762939453% Train Loss 0.6552815437316895 Val Loss 1.4834612607955933
EPOCH unfeeze : 36
Trainable Parameters : 85253124
Epoch 86 Train Acc 40.03422164916992% Val Acc 25.399999618530273% Train Loss 0.6549986600875854 Val Loss 1.5182700157165527
EPOCH unfeeze : 37
Trainable Parameters : 85253124
Epoch 87 Train Acc 40.038021087646484% Val Acc 25.5% Train Loss 0.654870867729187 Val Loss 1.4832484722137451
EPOCH unfeeze : 38
Trainable Parameters : 85253124
Epoch 88 Train Acc 40.06083679199219% Val Acc 23.399999618530273% Train Loss 0.655098021030426 Val Loss 1.5010854005813599
EPOCH unfeeze : 39
Trainable Parameters : 85253124
Epoch 89 Train Acc 39.98859405517578% Val Acc 24.0% Train Loss 0.6549863219261169 Val Loss 1.4716376066207886
EPOCH unfeeze : 40
Trainable Parameters : 85253124
Epoch 90 Train Acc 40.038021087646484% Val Acc 22.700000762939453% Train Loss 0.6550936698913574 Val Loss 1.5028294324874878
EPOCH unfeeze : 41
Trainable Parameters : 85253124
Epoch 91 Train Acc 39.992393493652344% Val Acc 27.0% Train Loss 0.6548686027526855 Val Loss 1.4653825759887695
EPOCH unfeeze : 42
Trainable Parameters : 85253124
Epoch 92 Train Acc 40.00760269165039% Val Acc 23.0% Train Loss 0.6552019715309143 Val Loss 1.497885823249817
EPOCH unfeeze : 43
Trainable Parameters : 85253124
Epoch 93 Train Acc 40.04562759399414% Val Acc 21.80000114440918% Train Loss 0.6550357937812805 Val Loss 1.5006221532821655
EPOCH unfeeze : 44
Trainable Parameters : 85253124
Epoch 94 Train Acc 40.026615142822266% Val Acc 25.5% Train Loss 0.654961884021759 Val Loss 1.5084155797958374
EPOCH unfeeze : 45
Trainable Parameters : 85253124
Epoch 95 Train Acc 40.09885787963867% Val Acc 23.30000114440918% Train Loss 0.6545494794845581 Val Loss 1.5415219068527222
EPOCH unfeeze : 46
Trainable Parameters : 85253124
Epoch 96 Train Acc 40.019012451171875% Val Acc 24.200000762939453% Train Loss 0.6551548838615417 Val Loss 1.469677448272705
EPOCH unfeeze : 47
Trainable Parameters : 85253124
Epoch 97 Train Acc 40.09505844116211% Val Acc 25.100000381469727% Train Loss 0.6551629304885864 Val Loss 1.4784971475601196
EPOCH unfeeze : 48
Trainable Parameters : 85253124
Epoch 98 Train Acc 40.07984924316406% Val Acc 25.0% Train Loss 0.6548441648483276 Val Loss 1.4735146760940552
EPOCH unfeeze : 49
Trainable Parameters : 85253124
Epoch 99 Train Acc 40.04182434082031% Val Acc 25.100000381469727% Train Loss 0.6549398899078369 Val Loss 1.4668325185775757

------> EVALUATING MODEL... ------------------------------------------ 

/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Final Test Acc:24.700000762939453% Loss:1.4793288707733154
CONFUSION MATRIX
[[  0   0 100   0]
 [  0   0 100   0]
 [  0   0  98   0]
 [  0   0 100   0]]
CONFUSION MATRIX NORMALISED
[[0.         0.         0.25125628 0.        ]
 [0.         0.         0.25125628 0.        ]
 [0.         0.         0.24623116 0.        ]
 [0.         0.         0.25125628 0.        ]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       0.00      0.00      0.00       100
           2       0.25      1.00      0.40        98
           3       0.00      0.00      0.00       100

    accuracy                           0.25       398
   macro avg       0.06      0.25      0.10       398
weighted avg       0.06      0.25      0.10       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 07/11/2022 06:22:11
