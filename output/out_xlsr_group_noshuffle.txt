Wed Nov 2 18:39:48 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_group.py
Started: 02/11/2022 18:40:01

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-group-noshuffle
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
group_size: 4
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-group-noshuffle
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-group-noshuffle_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-1.0190e-01, -3.2528e-02, -2.5984e-02,  ...,  2.5224e-01,
          1.4884e-01,  4.3572e-02],
        [-2.7371e-03, -3.1377e-04, -8.3915e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1684e+00, -1.1966e+00, -7.6227e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.1671e+00, -6.2004e-01,  1.1060e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.7674e+00, -2.6100e+00, -2.4223e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0468e-01,  1.0468e-01,  1.2572e-01,  ..., -1.3336e+00,
         -1.4835e+00, -1.5133e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.4927, -0.4918, -0.4810,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.8563,  0.1170, -0.2756,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0396,  0.0817, -0.1430,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0776,  0.3548, -0.0021,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0193,  0.0315,  0.0193,  ...,  0.0000,  0.0000,  0.0000],
        [-0.6061, -0.3279, -0.0192,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.bias', 'lm_head.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.0261,  0.0750,  0.1125,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8380, -0.8204, -0.7205,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0621, -0.0449, -0.0356,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.2072, -0.2131, -0.2381,  ...,  0.0000,  0.0000,  0.0000],
        [-1.4472, -1.3934, -1.3386,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.4402,  0.3020,  0.1914,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Epoch 0 Train Acc 26.878326416015625% Val Acc 0.0% Train Loss 0.6875525712966919 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 1 Train Acc 27.026615142822266% Val Acc 0.0% Train Loss 0.687518298625946 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 2 Train Acc 27.1292781829834% Val Acc 0.0% Train Loss 0.6869717836380005 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 3 Train Acc 27.106464385986328% Val Acc 0.0% Train Loss 0.6878412365913391 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 4 Train Acc 27.25475311279297% Val Acc 0.0% Train Loss 0.6874904632568359 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 5 Train Acc 27.657794952392578% Val Acc 0.0% Train Loss 0.687407374382019 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 6 Train Acc 27.292776107788086% Val Acc 0.0% Train Loss 0.687371015548706 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 7 Train Acc 27.178707122802734% Val Acc 0.0% Train Loss 0.6871156692504883 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 8 Train Acc 27.63117790222168% Val Acc 0.0% Train Loss 0.6870444416999817 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 9 Train Acc 27.292776107788086% Val Acc 0.0% Train Loss 0.6871611475944519 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 10 Train Acc 27.235740661621094% Val Acc 0.0% Train Loss 0.6873456239700317 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 11 Train Acc 27.19391632080078% Val Acc 0.0% Train Loss 0.6873756647109985 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 12 Train Acc 27.18250846862793% Val Acc 0.0% Train Loss 0.6875879168510437 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 13 Train Acc 27.433460235595703% Val Acc 0.0% Train Loss 0.687408983707428 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 14 Train Acc 27.342205047607422% Val Acc 0.0% Train Loss 0.6871255040168762 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 15 Train Acc 26.97718620300293% Val Acc 0.0% Train Loss 0.6876077055931091 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 16 Train Acc 27.197717666625977% Val Acc 0.0% Train Loss 0.6876810789108276 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 17 Train Acc 26.84790802001953% Val Acc 0.0% Train Loss 0.687660813331604 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 18 Train Acc 27.247148513793945% Val Acc 0.0% Train Loss 0.687674343585968 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 19 Train Acc 27.475284576416016% Val Acc 0.0% Train Loss 0.6871417164802551 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 20 Train Acc 27.25475311279297% Val Acc 0.0% Train Loss 0.6873185038566589 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 21 Train Acc 27.673004150390625% Val Acc 0.0% Train Loss 0.6873816251754761 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 22 Train Acc 27.315589904785156% Val Acc 0.0% Train Loss 0.6872523427009583 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 23 Train Acc 27.064638137817383% Val Acc 0.0% Train Loss 0.6877102255821228 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 24 Train Acc 27.292776107788086% Val Acc 0.0% Train Loss 0.6872134804725647 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 25 Train Acc 27.475284576416016% Val Acc 0.0% Train Loss 0.6865875720977783 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 26 Train Acc 27.167299270629883% Val Acc 0.0% Train Loss 0.6872643232345581 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 27 Train Acc 26.95437240600586% Val Acc 0.0% Train Loss 0.68754643201828 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 28 Train Acc 27.06844139099121% Val Acc 0.0% Train Loss 0.6873020529747009 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 29 Train Acc 27.019010543823242% Val Acc 0.0% Train Loss 0.6872280240058899 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 30 Train Acc 27.292776107788086% Val Acc 0.0% Train Loss 0.687170147895813 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 31 Train Acc 27.70342254638672% Val Acc 0.0% Train Loss 0.6874723434448242 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 32 Train Acc 27.281368255615234% Val Acc 0.0% Train Loss 0.6873336434364319 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 33 Train Acc 27.657794952392578% Val Acc 0.0% Train Loss 0.6872938275337219 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 34 Train Acc 27.0% Val Acc 0.0% Train Loss 0.6873863339424133 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 35 Train Acc 27.414447784423828% Val Acc 0.0% Train Loss 0.6873376965522766 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 36 Train Acc 27.186311721801758% Val Acc 0.0% Train Loss 0.6870521306991577 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 37 Train Acc 27.117870330810547% Val Acc 0.0% Train Loss 0.6874731779098511 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 38 Train Acc 27.330799102783203% Val Acc 0.0% Train Loss 0.6872479319572449 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 39 Train Acc 27.673004150390625% Val Acc 0.0% Train Loss 0.687488853931427 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 40 Train Acc 26.94296646118164% Val Acc 0.0% Train Loss 0.6873033046722412 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 41 Train Acc 27.281368255615234% Val Acc 0.0% Train Loss 0.6874794960021973 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 42 Train Acc 27.205322265625% Val Acc 0.0% Train Loss 0.6871940493583679 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 43 Train Acc 27.490493774414062% Val Acc 0.0% Train Loss 0.6871646642684937 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 44 Train Acc 27.201520919799805% Val Acc 0.0% Train Loss 0.6875694394111633 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 45 Train Acc 27.410646438598633% Val Acc 0.0% Train Loss 0.687440812587738 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 46 Train Acc 27.361215591430664% Val Acc 0.0% Train Loss 0.6874071359634399 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 47 Train Acc 27.410646438598633% Val Acc 0.0% Train Loss 0.6871949434280396 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 48 Train Acc 27.47148323059082% Val Acc 0.0% Train Loss 0.6870700120925903 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 49 Train Acc 27.42205238342285% Val Acc 0.0% Train Loss 0.6874789595603943 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 50 Train Acc 27.368820190429688% Val Acc 0.0% Train Loss 0.6873916983604431 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 51 Train Acc 27.21673011779785% Val Acc 0.0% Train Loss 0.6870449185371399 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 52 Train Acc 27.410646438598633% Val Acc 0.0% Train Loss 0.6871037483215332 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 53 Train Acc 26.80228042602539% Val Acc 0.0% Train Loss 0.6874377727508545 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 54 Train Acc 26.452470779418945% Val Acc 0.0% Train Loss 0.6874395608901978 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 55 Train Acc 27.37642478942871% Val Acc 0.0% Train Loss 0.6872842311859131 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 56 Train Acc 27.072242736816406% Val Acc 0.0% Train Loss 0.6874983310699463 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 57 Train Acc 27.558935165405273% Val Acc 0.0% Train Loss 0.6874715685844421 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 58 Train Acc 27.410646438598633% Val Acc 0.0% Train Loss 0.6871548891067505 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 59 Train Acc 27.80608367919922% Val Acc 0.0% Train Loss 0.6875337362289429 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 60 Train Acc 27.406843185424805% Val Acc 0.0% Train Loss 0.6874250769615173 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 61 Train Acc 27.21673011779785% Val Acc 0.0% Train Loss 0.6875283122062683 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 62 Train Acc 27.25475311279297% Val Acc 0.0% Train Loss 0.6872470378875732 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 63 Train Acc 27.75665283203125% Val Acc 0.0% Train Loss 0.6871134638786316 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 64 Train Acc 27.292776107788086% Val Acc 0.0% Train Loss 0.6872824430465698 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 65 Train Acc 26.92015266418457% Val Acc 0.0% Train Loss 0.6872756481170654 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 66 Train Acc 27.349809646606445% Val Acc 0.0% Train Loss 0.6873263716697693 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 67 Train Acc 27.520912170410156% Val Acc 0.0% Train Loss 0.6874434351921082 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 68 Train Acc 27.015209197998047% Val Acc 0.0% Train Loss 0.6874917149543762 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 69 Train Acc 27.106464385986328% Val Acc 0.0% Train Loss 0.6874132752418518 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 70 Train Acc 27.239543914794922% Val Acc 0.0% Train Loss 0.6876527070999146 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 71 Train Acc 27.098859786987305% Val Acc 0.0% Train Loss 0.6870828866958618 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 72 Train Acc 26.916349411010742% Val Acc 0.0% Train Loss 0.6874995827674866 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 73 Train Acc 27.444866180419922% Val Acc 0.0% Train Loss 0.6876395344734192 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 74 Train Acc 27.353612899780273% Val Acc 0.0% Train Loss 0.687628448009491 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 75 Train Acc 26.69961929321289% Val Acc 0.0% Train Loss 0.6874801516532898 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 76 Train Acc 27.167299270629883% Val Acc 0.0% Train Loss 0.6873900294303894 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 77 Train Acc 27.72243309020996% Val Acc 0.0% Train Loss 0.6872652173042297 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 78 Train Acc 27.695817947387695% Val Acc 0.0% Train Loss 0.6873711347579956 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 79 Train Acc 27.247148513793945% Val Acc 0.0% Train Loss 0.6874301433563232 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 80 Train Acc 27.095056533813477% Val Acc 0.0% Train Loss 0.6871476173400879 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 81 Train Acc 27.07984733581543% Val Acc 0.0% Train Loss 0.6874095797538757 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 82 Train Acc 27.266159057617188% Val Acc 0.0% Train Loss 0.6868853569030762 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 83 Train Acc 27.7680606842041% Val Acc 0.0% Train Loss 0.687514066696167 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 84 Train Acc 27.38022804260254% Val Acc 0.0% Train Loss 0.6873299479484558 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 85 Train Acc 27.78326988220215% Val Acc 0.0% Train Loss 0.6871613264083862 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 86 Train Acc 27.04562759399414% Val Acc 0.0% Train Loss 0.6872381567955017 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 87 Train Acc 27.353612899780273% Val Acc 0.0% Train Loss 0.6873483657836914 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 88 Train Acc 27.88593101501465% Val Acc 0.0% Train Loss 0.6871545314788818 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 89 Train Acc 27.600759506225586% Val Acc 0.0% Train Loss 0.6868871450424194 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 90 Train Acc 27.97718620300293% Val Acc 0.0% Train Loss 0.6872470378875732 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 91 Train Acc 27.338403701782227% Val Acc 0.0% Train Loss 0.6873138546943665 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 92 Train Acc 27.25475311279297% Val Acc 0.0% Train Loss 0.6875002384185791 Val Loss 1.441785454750061
Trainable Parameters : 264452
Epoch 93 Train Acc 26.912548065185547% Val Acc 0.0% Train Loss 0.6870467066764832 Val Loss 1.441785454750061
Trainable Parameters : 264452
