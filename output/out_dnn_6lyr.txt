Fri Nov 4 01:20:41 AEDT 2022
python3: can't open file 'run_dnn_downstream_6layer.py': [Errno 2] No such file or directory
Fri Nov 4 23:34:00 AEDT 2022
2022-11-04 23:34:01.190530: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-04 23:34:01.397893: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-04 23:34:01.431612: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-04 23:34:02.447870: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 23:34:02.447953: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 23:34:02.447962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Traceback (most recent call last):
  File "run_dnn_downstream_6layers.py", line 24, in <module>
    import customTransform as T
ModuleNotFoundError: No module named 'customTransform'
Fri Nov 4 23:37:39 AEDT 2022
2022-11-04 23:37:40.192428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-04 23:37:40.400048: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-04 23:37:40.433853: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-04 23:37:41.674303: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 23:37:41.674384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-04 23:37:41.674393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_dnn_downstream.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_dnn_downstream_6layers.py
Started: 04/11/2022 23:37:53

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-dnn_6layer
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-dnn_6layer
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-dnn_6layer_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.1368, -0.0879, -0.1142,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0378, -0.0163,  0.0417,  ..., -1.1425, -0.5771,  0.3829],
        [-0.7885, -0.4556, -0.2730,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0916, -0.1023, -0.1492,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0727, -0.1631, -0.0281,  ..., -0.0161, -0.0430, -0.0707],
        [ 0.0115,  0.0094,  0.0051,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 3, 3, 2, 0, 1, 0, 0, 1, 3, 1, 2, 3, 2, 2, 3, 2, 0, 3, 2, 0, 3, 2,
        0, 0, 2, 3, 0, 2, 0, 2, 1, 2, 1, 3, 3, 2, 3, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.8807, -1.0383, -1.0760,  ..., -0.7737,  2.4600,  0.0100],
        [-0.0736, -0.0909, -0.0920,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.7012, -0.4444, -0.2291,  ...,  0.1717,  0.1538,  0.1031],
        ...,
        [-0.5019, -0.7137, -1.1866,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0285, -0.0454, -0.0714,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1412,  0.0715,  0.0805,  ..., -3.8234, -3.9128, -2.6692]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 2, 1, 0, 2, 0, 2, 3, 1, 0, 1, 1, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 1, 1,
        1, 0, 0, 1, 0, 0, 3, 1, 3, 2, 2, 0, 1, 2, 0, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'projector.bias', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 0.1092,  0.0927,  0.0786,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8183, -0.7481, -0.5842,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0249, -0.0531, -0.0344,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-2.9606, -3.0739, -2.6681,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2346, -0.2851, -0.6653,  ...,  0.0000,  0.0000,  0.0000],
        [-0.8079, -0.6294, -0.4277,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 0, 2, 3, 3, 1, 2, 3, 3, 3, 1, 3, 3, 2, 1, 3, 3, 2, 0, 0, 2, 3, 2, 3,
        0, 3, 1, 1, 2, 0, 1, 2, 0, 2, 0, 3, 1, 3, 0, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 373044
Epoch 0 Train Acc 26.68060874938965% Val Acc 25.899999618530273% Train Loss 0.6838099360466003 Val Loss 1.3915050029754639
Trainable Parameters : 373044
Epoch 1 Train Acc 26.657794952392578% Val Acc 26.100000381469727% Train Loss 0.67645263671875 Val Loss 1.3976119756698608
Trainable Parameters : 373044
Epoch 2 Train Acc 37.73384094238281% Val Acc 26.200000762939453% Train Loss 0.6407857537269592 Val Loss 1.4066165685653687
Trainable Parameters : 373044
Epoch 3 Train Acc 46.8973388671875% Val Acc 27.80000114440918% Train Loss 0.5886451601982117 Val Loss 1.4566030502319336
Trainable Parameters : 373044
Epoch 4 Train Acc 50.19391632080078% Val Acc 30.5% Train Loss 0.5638933181762695 Val Loss 1.4496289491653442
Trainable Parameters : 373044
Epoch 5 Train Acc 52.65399169921875% Val Acc 32.70000076293945% Train Loss 0.5491598844528198 Val Loss 1.4107071161270142
Trainable Parameters : 373044
Epoch 6 Train Acc 56.269962310791016% Val Acc 43.29999923706055% Train Loss 0.5254421830177307 Val Loss 1.3522757291793823
Trainable Parameters : 373044
Epoch 7 Train Acc 59.53992462158203% Val Acc 44.900001525878906% Train Loss 0.49764156341552734 Val Loss 1.3434531688690186
Trainable Parameters : 373044
Epoch 8 Train Acc 62.01140594482422% Val Acc 52.900001525878906% Train Loss 0.47101593017578125 Val Loss 1.1486161947250366
Trainable Parameters : 373044
Epoch 9 Train Acc 64.61216735839844% Val Acc 53.20000076293945% Train Loss 0.4473770558834076 Val Loss 1.2231316566467285
Trainable Parameters : 373044
Epoch 10 Train Acc 64.39543914794922% Val Acc 54.20000076293945% Train Loss 0.44443830847740173 Val Loss 1.2213572263717651
Trainable Parameters : 373044
Epoch 11 Train Acc 64.2813720703125% Val Acc 50.400001525878906% Train Loss 0.4421072006225586 Val Loss 1.3100343942642212
Trainable Parameters : 373044
Epoch 12 Train Acc 64.49429321289062% Val Acc 44.29999923706055% Train Loss 0.4433651268482208 Val Loss 1.3241440057754517
Trainable Parameters : 373044
Epoch 13 Train Acc 64.9239501953125% Val Acc 56.70000076293945% Train Loss 0.4418620467185974 Val Loss 1.0680017471313477
Trainable Parameters : 373044
Epoch 14 Train Acc 64.63497924804688% Val Acc 56.29999923706055% Train Loss 0.44180309772491455 Val Loss 1.10382878780365
Trainable Parameters : 373044
Epoch 15 Train Acc 64.74524688720703% Val Acc 49.70000076293945% Train Loss 0.43640002608299255 Val Loss 1.2067292928695679
Trainable Parameters : 373044
Epoch 16 Train Acc 64.52471160888672% Val Acc 45.79999923706055% Train Loss 0.44061601161956787 Val Loss 1.4567434787750244
Trainable Parameters : 373044
Epoch 17 Train Acc 65.04562377929688% Val Acc 43.900001525878906% Train Loss 0.4347427189350128 Val Loss 1.4493764638900757
Trainable Parameters : 373044
Epoch 18 Train Acc 64.86312103271484% Val Acc 56.79999923706055% Train Loss 0.44050881266593933 Val Loss 1.061873435974121
Trainable Parameters : 373044
Epoch 19 Train Acc 64.52091217041016% Val Acc 50.60000228881836% Train Loss 0.44185489416122437 Val Loss 1.2089685201644897
Trainable Parameters : 373044
Epoch 20 Train Acc 64.75285339355469% Val Acc 51.70000076293945% Train Loss 0.43752145767211914 Val Loss 1.191048502922058
Trainable Parameters : 373044
Epoch 21 Train Acc 64.53231811523438% Val Acc 52.20000076293945% Train Loss 0.43790557980537415 Val Loss 1.1649888753890991
Trainable Parameters : 373044
Epoch 22 Train Acc 64.71102905273438% Val Acc 57.29999923706055% Train Loss 0.4377745985984802 Val Loss 1.0711908340454102
Trainable Parameters : 373044
Epoch 23 Train Acc 64.94676971435547% Val Acc 52.10000228881836% Train Loss 0.4390169680118561 Val Loss 1.1411632299423218
Trainable Parameters : 373044
Epoch 24 Train Acc 64.84410858154297% Val Acc 53.0% Train Loss 0.43747013807296753 Val Loss 1.1310844421386719
Trainable Parameters : 373044
Epoch 25 Train Acc 64.80228424072266% Val Acc 57.29999923706055% Train Loss 0.4337874948978424 Val Loss 1.045197606086731
Trainable Parameters : 373044
Epoch 26 Train Acc 64.43726348876953% Val Acc 49.900001525878906% Train Loss 0.437723308801651 Val Loss 1.2368953227996826
Trainable Parameters : 373044
Epoch 27 Train Acc 65.19011688232422% Val Acc 49.20000076293945% Train Loss 0.43488889932632446 Val Loss 1.3556188344955444
Trainable Parameters : 373044
Epoch 28 Train Acc 64.60456085205078% Val Acc 57.5% Train Loss 0.4323437213897705 Val Loss 1.1242576837539673
Trainable Parameters : 373044
Epoch 29 Train Acc 65.49049377441406% Val Acc 38.5% Train Loss 0.4330161511898041 Val Loss 1.6629297733306885
Trainable Parameters : 373044
Epoch 30 Train Acc 64.62737274169922% Val Acc 44.900001525878906% Train Loss 0.4381848871707916 Val Loss 1.28965425491333
Trainable Parameters : 373044
Epoch 31 Train Acc 64.7490463256836% Val Acc 56.79999923706055% Train Loss 0.4379492700099945 Val Loss 1.0598877668380737
Trainable Parameters : 373044
Epoch 32 Train Acc 64.49429321289062% Val Acc 53.400001525878906% Train Loss 0.43623244762420654 Val Loss 1.136639952659607
Trainable Parameters : 373044
Epoch 33 Train Acc 65.205322265625% Val Acc 51.29999923706055% Train Loss 0.4314672648906708 Val Loss 1.2677435874938965
Trainable Parameters : 373044
Epoch 34 Train Acc 65.0% Val Acc 55.400001525878906% Train Loss 0.43230777978897095 Val Loss 1.0961977243423462
Trainable Parameters : 373044
Epoch 35 Train Acc 65.2509536743164% Val Acc 53.400001525878906% Train Loss 0.4336868226528168 Val Loss 1.1382132768630981
Trainable Parameters : 373044
Epoch 36 Train Acc 65.22813415527344% Val Acc 52.29999923706055% Train Loss 0.4329565167427063 Val Loss 1.1071995496749878
Trainable Parameters : 373044
Epoch 37 Train Acc 65.40303802490234% Val Acc 51.400001525878906% Train Loss 0.42902708053588867 Val Loss 1.154384732246399
Trainable Parameters : 373044
Epoch 38 Train Acc 65.5133056640625% Val Acc 53.70000076293945% Train Loss 0.4296266436576843 Val Loss 1.1619138717651367
Trainable Parameters : 373044
Epoch 39 Train Acc 65.98478698730469% Val Acc 48.79999923706055% Train Loss 0.4215866029262543 Val Loss 1.2066656351089478
Trainable Parameters : 373044
Epoch 40 Train Acc 65.50189971923828% Val Acc 53.5% Train Loss 0.4250049889087677 Val Loss 1.0901463031768799
Trainable Parameters : 373044
Epoch 41 Train Acc 66.18251037597656% Val Acc 50.29999923706055% Train Loss 0.4204845726490021 Val Loss 1.2567112445831299
Trainable Parameters : 373044
Epoch 42 Train Acc 66.67300415039062% Val Acc 51.900001525878906% Train Loss 0.4181005656719208 Val Loss 1.1595979928970337
Trainable Parameters : 373044
Epoch 43 Train Acc 66.77566528320312% Val Acc 43.5% Train Loss 0.4106752574443817 Val Loss 1.454093098640442
Trainable Parameters : 373044
Epoch 44 Train Acc 67.15969848632812% Val Acc 46.70000076293945% Train Loss 0.407258003950119 Val Loss 1.2045080661773682
Trainable Parameters : 373044
Epoch 45 Train Acc 67.3384017944336% Val Acc 52.70000076293945% Train Loss 0.4062833786010742 Val Loss 1.1570953130722046
Trainable Parameters : 373044
Epoch 46 Train Acc 67.9277572631836% Val Acc 45.29999923706055% Train Loss 0.403008371591568 Val Loss 1.3410643339157104
Trainable Parameters : 373044
Epoch 47 Train Acc 67.87452697753906% Val Acc 58.79999923706055% Train Loss 0.40542474389076233 Val Loss 0.977403461933136
Trainable Parameters : 373044
Epoch 48 Train Acc 67.98859405517578% Val Acc 43.29999923706055% Train Loss 0.3949890434741974 Val Loss 1.3357638120651245
Trainable Parameters : 373044
Epoch 49 Train Acc 68.90113830566406% Val Acc 46.79999923706055% Train Loss 0.3932281732559204 Val Loss 1.2202296257019043
Trainable Parameters : 373044
Epoch 50 Train Acc 68.4866943359375% Val Acc 50.20000076293945% Train Loss 0.38923928141593933 Val Loss 1.3074655532836914
Trainable Parameters : 373044
Epoch 51 Train Acc 68.73384094238281% Val Acc 53.0% Train Loss 0.3901599943637848 Val Loss 1.1669349670410156
Trainable Parameters : 373044
Epoch 52 Train Acc 69.5171127319336% Val Acc 54.79999923706055% Train Loss 0.3824787139892578 Val Loss 1.0892776250839233
Trainable Parameters : 373044
Epoch 53 Train Acc 69.96197509765625% Val Acc 52.79999923706055% Train Loss 0.38240882754325867 Val Loss 1.0922435522079468
Trainable Parameters : 373044
Epoch 54 Train Acc 70.13687896728516% Val Acc 40.0% Train Loss 0.37707170844078064 Val Loss 1.6746294498443604
Trainable Parameters : 373044
Epoch 55 Train Acc 70.84790802001953% Val Acc 50.10000228881836% Train Loss 0.36792123317718506 Val Loss 1.273105502128601
Trainable Parameters : 373044
Epoch 56 Train Acc 71.01901245117188% Val Acc 49.20000076293945% Train Loss 0.3663885295391083 Val Loss 1.6431620121002197
Trainable Parameters : 373044
Epoch 57 Train Acc 70.73384094238281% Val Acc 55.5% Train Loss 0.36641862988471985 Val Loss 1.1241943836212158
Trainable Parameters : 373044
Epoch 58 Train Acc 71.34220123291016% Val Acc 49.70000076293945% Train Loss 0.36473217606544495 Val Loss 1.2700859308242798
Trainable Parameters : 373044
Epoch 59 Train Acc 71.205322265625% Val Acc 48.79999923706055% Train Loss 0.3581637144088745 Val Loss 1.3628337383270264
Trainable Parameters : 373044
Epoch 60 Train Acc 71.98478698730469% Val Acc 52.79999923706055% Train Loss 0.3581942021846771 Val Loss 1.4015365839004517
Trainable Parameters : 373044
Epoch 61 Train Acc 71.98098754882812% Val Acc 50.60000228881836% Train Loss 0.3536105155944824 Val Loss 1.2532329559326172
Trainable Parameters : 373044
Epoch 62 Train Acc 72.34220123291016% Val Acc 53.60000228881836% Train Loss 0.35267287492752075 Val Loss 1.1027246713638306
Trainable Parameters : 373044
Epoch 63 Train Acc 72.53231811523438% Val Acc 50.10000228881836% Train Loss 0.34509047865867615 Val Loss 1.2390064001083374
Trainable Parameters : 373044
Epoch 64 Train Acc 73.19391632080078% Val Acc 58.29999923706055% Train Loss 0.3445449769496918 Val Loss 1.0676690340042114
Trainable Parameters : 373044
Epoch 65 Train Acc 73.2509536743164% Val Acc 46.70000076293945% Train Loss 0.3368058204650879 Val Loss 1.7613693475723267
Trainable Parameters : 373044
Epoch 66 Train Acc 73.23194122314453% Val Acc 51.0% Train Loss 0.3358246386051178 Val Loss 1.2170875072479248
Trainable Parameters : 373044
Epoch 67 Train Acc 73.66539764404297% Val Acc 53.10000228881836% Train Loss 0.3334471583366394 Val Loss 1.2802245616912842
Trainable Parameters : 373044
Epoch 68 Train Acc 74.33460235595703% Val Acc 49.900001525878906% Train Loss 0.3275104761123657 Val Loss 1.2805253267288208
Trainable Parameters : 373044
Epoch 69 Train Acc 74.33079528808594% Val Acc 53.5% Train Loss 0.3317568302154541 Val Loss 1.1835941076278687
Trainable Parameters : 373044
Epoch 70 Train Acc 74.83650207519531% Val Acc 50.60000228881836% Train Loss 0.3217090964317322 Val Loss 1.4544769525527954
Trainable Parameters : 373044
Epoch 71 Train Acc 74.8669204711914% Val Acc 54.20000076293945% Train Loss 0.3158511519432068 Val Loss 1.1982824802398682
Trainable Parameters : 373044
Epoch 72 Train Acc 74.65399169921875% Val Acc 44.60000228881836% Train Loss 0.3195107877254486 Val Loss 1.6186456680297852
Trainable Parameters : 373044
Epoch 73 Train Acc 75.2357406616211% Val Acc 56.29999923706055% Train Loss 0.3169277012348175 Val Loss 1.2320501804351807
Trainable Parameters : 373044
Epoch 74 Train Acc 76.0% Val Acc 50.20000076293945% Train Loss 0.30890265107154846 Val Loss 1.247646450996399
Trainable Parameters : 373044
Epoch 75 Train Acc 76.00379943847656% Val Acc 50.10000228881836% Train Loss 0.30521050095558167 Val Loss 1.4240425825119019
Trainable Parameters : 373044
Epoch 76 Train Acc 76.5171127319336% Val Acc 48.400001525878906% Train Loss 0.3049383759498596 Val Loss 1.4090827703475952
Trainable Parameters : 373044
Epoch 77 Train Acc 76.26235961914062% Val Acc 52.20000076293945% Train Loss 0.3013595640659332 Val Loss 1.2877031564712524
Trainable Parameters : 373044
Epoch 78 Train Acc 76.60456085205078% Val Acc 45.29999923706055% Train Loss 0.29536959528923035 Val Loss 1.9101136922836304
Trainable Parameters : 373044
Epoch 79 Train Acc 76.58935546875% Val Acc 49.60000228881836% Train Loss 0.2983717620372772 Val Loss 1.3790923357009888
Trainable Parameters : 373044
Epoch 80 Train Acc 76.94676971435547% Val Acc 53.60000228881836% Train Loss 0.29531702399253845 Val Loss 1.234214186668396
Trainable Parameters : 373044
Epoch 81 Train Acc 77.97718811035156% Val Acc 52.79999923706055% Train Loss 0.28329116106033325 Val Loss 1.4380720853805542
Trainable Parameters : 373044
Epoch 82 Train Acc 78.44866943359375% Val Acc 51.70000076293945% Train Loss 0.28021398186683655 Val Loss 1.5686691999435425
Trainable Parameters : 373044
Epoch 83 Train Acc 77.68441009521484% Val Acc 48.400001525878906% Train Loss 0.28669771552085876 Val Loss 1.5040963888168335
Trainable Parameters : 373044
Epoch 84 Train Acc 78.05703735351562% Val Acc 56.400001525878906% Train Loss 0.2787267565727234 Val Loss 1.2813957929611206
Trainable Parameters : 373044
Epoch 85 Train Acc 78.31558990478516% Val Acc 57.20000076293945% Train Loss 0.2732832431793213 Val Loss 1.2374422550201416
Trainable Parameters : 373044
Epoch 86 Train Acc 78.85551452636719% Val Acc 49.400001525878906% Train Loss 0.27492791414260864 Val Loss 1.5735766887664795
Trainable Parameters : 373044
Epoch 87 Train Acc 78.9277572631836% Val Acc 50.0% Train Loss 0.2699262201786041 Val Loss 1.6222114562988281
Trainable Parameters : 373044
Epoch 88 Train Acc 79.16729736328125% Val Acc 52.10000228881836% Train Loss 0.26788926124572754 Val Loss 1.5906517505645752
Trainable Parameters : 373044
Epoch 89 Train Acc 79.8517074584961% Val Acc 48.5% Train Loss 0.26338836550712585 Val Loss 1.614645004272461
Trainable Parameters : 373044
Epoch 90 Train Acc 79.88973236083984% Val Acc 52.900001525878906% Train Loss 0.2589132487773895 Val Loss 1.313295602798462
Trainable Parameters : 373044
Epoch 91 Train Acc 79.94676971435547% Val Acc 48.400001525878906% Train Loss 0.25862976908683777 Val Loss 1.7551664113998413
Trainable Parameters : 373044
Epoch 92 Train Acc 80.00379943847656% Val Acc 56.400001525878906% Train Loss 0.2511620819568634 Val Loss 1.5617914199829102
Trainable Parameters : 373044
Epoch 93 Train Acc 80.52471160888672% Val Acc 46.10000228881836% Train Loss 0.24955056607723236 Val Loss 1.6671627759933472
Trainable Parameters : 373044
Epoch 94 Train Acc 81.4144515991211% Val Acc 47.60000228881836% Train Loss 0.24435439705848694 Val Loss 1.751344084739685
Trainable Parameters : 373044
Epoch 95 Train Acc 81.47148132324219% Val Acc 55.29999923706055% Train Loss 0.24249646067619324 Val Loss 1.4309393167495728
Trainable Parameters : 373044
Epoch 96 Train Acc 81.28136444091797% Val Acc 50.29999923706055% Train Loss 0.24067889153957367 Val Loss 1.775314211845398
Trainable Parameters : 373044
Epoch 97 Train Acc 81.31558990478516% Val Acc 51.10000228881836% Train Loss 0.24089038372039795 Val Loss 1.547808289527893
Trainable Parameters : 373044
Epoch 98 Train Acc 82.14828491210938% Val Acc 47.10000228881836% Train Loss 0.23244917392730713 Val Loss 1.7242863178253174
Trainable Parameters : 373044
Epoch 99 Train Acc 82.14828491210938% Val Acc 50.60000228881836% Train Loss 0.23601573705673218 Val Loss 1.5288697481155396

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:55.29999923706055% Loss:1.3348839282989502
CONFUSION MATRIX
[[76  3 18  3]
 [14 39 34 13]
 [21  4 64  9]
 [26  7 27 40]]
CONFUSION MATRIX NORMALISED
[[0.19095477 0.00753769 0.04522613 0.00753769]
 [0.03517588 0.09798995 0.08542714 0.03266332]
 [0.05276382 0.01005025 0.16080402 0.02261307]
 [0.06532663 0.01758794 0.0678392  0.10050251]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.55      0.76      0.64       100
           1       0.74      0.39      0.51       100
           2       0.45      0.65      0.53        98
           3       0.62      0.40      0.48       100

    accuracy                           0.55       398
   macro avg       0.59      0.55      0.54       398
weighted avg       0.59      0.55      0.54       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 05/11/2022 04:23:42
