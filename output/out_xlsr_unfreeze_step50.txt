Sat Nov 5 18:49:59 AEDT 2022
2022-11-05 18:50:01.804466: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-05 18:50:02.223355: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-05 18:50:02.379038: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-05 18:50:03.893490: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-05 18:50:03.895732: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-05 18:50:03.895741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_unfreezestep50.py
Started: 05/11/2022 18:50:15

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-unfreeze-step50
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-araic-unfreeze-step50
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-araic-unfreeze-step50_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.1436,  0.1665,  0.2106,  ...,  0.6395,  0.6588,  0.6846],
        [-0.3399, -0.2992, -0.2528,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.0203,  0.6096,  0.1190,  ...,  0.3064,  0.4433,  0.5910],
        ...,
        [ 0.6666,  0.6227,  0.5657,  ...,  0.0000,  0.0000,  0.0000],
        [-1.5236, -1.2011,  0.2556,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3023, -0.2546, -0.2586,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 0, 3, 3, 3, 0, 3, 1, 0, 3, 0, 0, 3, 3, 3, 1, 3, 2, 3, 0, 0, 2, 3,
        3, 2, 0, 0, 3, 1, 2, 1, 3, 2, 3, 2, 3, 0, 2, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.9666, -0.8888, -0.8663,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1866, -0.2453, -0.2470,  ...,  0.1692,  0.1082, -0.0073],
        [ 0.5748,  0.6773,  0.5441,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.3766, -0.3857, -0.3947,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0763, -0.0827, -0.0816,  ..., -1.2423, -1.2498, -1.2029],
        [ 0.5306,  0.2476,  0.3834,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 2, 0, 1, 0, 3, 1, 1, 3, 0, 2, 2, 0, 1, 3, 2, 0, 1, 2, 1, 1, 1, 1,
        2, 0, 3, 3, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 3, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.9518, -1.2243, -1.4265,  ...,  0.0000,  0.0000,  0.0000],
        [-2.2727, -2.8250, -3.4539,  ...,  0.0000,  0.0000,  0.0000],
        [-2.9606, -3.0739, -2.6681,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0897,  0.1176,  0.1415,  ...,  0.5721,  0.6490,  0.4760],
        [ 0.2601,  0.8723,  1.0315,  ..., -0.6052, -0.5835, -0.0195],
        [-1.1076, -1.1066, -1.5257,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 3, 1, 0, 3, 2, 1, 0, 0, 2, 3, 1, 1, 3, 0, 1, 0, 3, 2, 1, 1, 1, 1,
        0, 0, 3, 3, 1, 3, 3, 2, 2, 1, 1, 1, 1, 3, 3, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 0 Train Acc 36.83650207519531% Val Acc 25.5% Train Loss 0.6666994690895081 Val Loss 1.439935326576233
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 1 Train Acc 46.338401794433594% Val Acc 33.0% Train Loss 0.6025509834289551 Val Loss 1.4693621397018433
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 2 Train Acc 58.20912551879883% Val Acc 51.400001525878906% Train Loss 0.509811520576477 Val Loss 1.1623330116271973
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 3 Train Acc 71.08744812011719% Val Acc 56.900001525878906% Train Loss 0.3807500898838043 Val Loss 1.1607264280319214
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 4 Train Acc 78.22053527832031% Val Acc 62.70000076293945% Train Loss 0.29518336057662964 Val Loss 1.1264482736587524
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 5 Train Acc 83.55133056640625% Val Acc 71.0% Train Loss 0.22682902216911316 Val Loss 0.8516530394554138
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 6 Train Acc 87.22813415527344% Val Acc 72.0% Train Loss 0.1802043616771698 Val Loss 0.9422345161437988
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 7 Train Acc 89.94296264648438% Val Acc 72.0% Train Loss 0.14020080864429474 Val Loss 1.02073335647583
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 8 Train Acc 92.47148132324219% Val Acc 71.9000015258789% Train Loss 0.10958030819892883 Val Loss 0.9904466867446899
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 9 Train Acc 93.15969848632812% Val Acc 73.80000305175781% Train Loss 0.09819716215133667 Val Loss 0.9631056189537048
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 10 Train Acc 94.25855255126953% Val Acc 79.5% Train Loss 0.08219370245933533 Val Loss 0.892359733581543
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 11 Train Acc 94.4144515991211% Val Acc 60.29999923706055% Train Loss 0.07863259315490723 Val Loss 2.306159734725952
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 12 Train Acc 94.99239349365234% Val Acc 76.5% Train Loss 0.07229958474636078 Val Loss 0.9995118975639343
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 13 Train Acc 95.384033203125% Val Acc 74.4000015258789% Train Loss 0.06655590981245041 Val Loss 1.0646761655807495
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 14 Train Acc 95.68441009521484% Val Acc 67.5999984741211% Train Loss 0.06338706612586975 Val Loss 1.3742214441299438
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 15 Train Acc 95.74524688720703% Val Acc 68.5% Train Loss 0.062443722039461136 Val Loss 1.3320602178573608
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 16 Train Acc 95.14068603515625% Val Acc 61.10000228881836% Train Loss 0.0710097998380661 Val Loss 1.7860416173934937
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 17 Train Acc 95.8973388671875% Val Acc 71.70000457763672% Train Loss 0.0605027973651886 Val Loss 1.3329758644104004
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 18 Train Acc 95.90113830566406% Val Acc 72.0% Train Loss 0.06229180097579956 Val Loss 0.9692036509513855
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 19 Train Acc 95.90113830566406% Val Acc 64.9000015258789% Train Loss 0.06115317344665527 Val Loss 1.6772171258926392
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 20 Train Acc 95.85931396484375% Val Acc 63.70000076293945% Train Loss 0.06319630146026611 Val Loss 1.7257622480392456
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 21 Train Acc 95.4828872680664% Val Acc 71.20000457763672% Train Loss 0.06577203422784805 Val Loss 1.4900480508804321
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 22 Train Acc 95.6615982055664% Val Acc 59.70000076293945% Train Loss 0.06435390561819077 Val Loss 1.8096227645874023
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 23 Train Acc 95.3079833984375% Val Acc 73.0% Train Loss 0.06727700680494308 Val Loss 1.152875304222107
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 24 Train Acc 96.37261962890625% Val Acc 66.0999984741211% Train Loss 0.05802462622523308 Val Loss 1.4960130453109741
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 25 Train Acc 95.31938934326172% Val Acc 65.4000015258789% Train Loss 0.06933528184890747 Val Loss 1.505348563194275
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 26 Train Acc 94.95817565917969% Val Acc 70.30000305175781% Train Loss 0.07079297304153442 Val Loss 1.2506719827651978
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 27 Train Acc 95.6615982055664% Val Acc 65.80000305175781% Train Loss 0.06404829770326614 Val Loss 1.909096121788025
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 28 Train Acc 94.80988311767578% Val Acc 65.80000305175781% Train Loss 0.07473231106996536 Val Loss 1.5464285612106323
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 29 Train Acc 95.26615905761719% Val Acc 67.0999984741211% Train Loss 0.07293569296598434 Val Loss 1.6461546421051025
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 30 Train Acc 94.80228424072266% Val Acc 66.70000457763672% Train Loss 0.07479161024093628 Val Loss 1.443650722503662
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 31 Train Acc 95.06083679199219% Val Acc 67.20000457763672% Train Loss 0.0740884318947792 Val Loss 1.3889576196670532
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 32 Train Acc 94.80988311767578% Val Acc 69.5% Train Loss 0.07550511509180069 Val Loss 1.2734971046447754
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 33 Train Acc 94.76045227050781% Val Acc 62.10000228881836% Train Loss 0.07506166398525238 Val Loss 1.4935872554779053
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 34 Train Acc 94.57414245605469% Val Acc 69.4000015258789% Train Loss 0.0812780037522316 Val Loss 1.5149307250976562
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 35 Train Acc 94.49429321289062% Val Acc 64.0999984741211% Train Loss 0.08140897005796432 Val Loss 1.6769905090332031
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 36 Train Acc 94.13687896728516% Val Acc 69.0999984741211% Train Loss 0.08601781725883484 Val Loss 1.1133326292037964
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 37 Train Acc 94.19391632080078% Val Acc 70.70000457763672% Train Loss 0.0840255618095398 Val Loss 0.9413349032402039
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 38 Train Acc 94.6463851928711% Val Acc 67.80000305175781% Train Loss 0.07758808135986328 Val Loss 1.2353250980377197
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 39 Train Acc 94.80988311767578% Val Acc 60.400001525878906% Train Loss 0.0742015466094017 Val Loss 1.8796501159667969
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 40 Train Acc 95.04182434082031% Val Acc 68.0999984741211% Train Loss 0.07186809927225113 Val Loss 1.380095362663269
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 41 Train Acc 95.41064453125% Val Acc 66.5% Train Loss 0.07056856155395508 Val Loss 1.2157405614852905
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 42 Train Acc 95.8212890625% Val Acc 68.9000015258789% Train Loss 0.06360295414924622 Val Loss 1.3468354940414429
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 43 Train Acc 96.57034301757812% Val Acc 61.70000076293945% Train Loss 0.053291890770196915 Val Loss 1.7139291763305664
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 44 Train Acc 95.6920166015625% Val Acc 68.0999984741211% Train Loss 0.06471087783575058 Val Loss 1.5102391242980957
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 45 Train Acc 96.53992462158203% Val Acc 63.900001525878906% Train Loss 0.05103558301925659 Val Loss 1.5663381814956665
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 46 Train Acc 96.46007537841797% Val Acc 63.10000228881836% Train Loss 0.05078643560409546 Val Loss 1.6256989240646362
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 47 Train Acc 96.63497924804688% Val Acc 68.20000457763672% Train Loss 0.052967920899391174 Val Loss 1.284741759300232
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 48 Train Acc 96.60456085205078% Val Acc 66.5999984741211% Train Loss 0.050829049199819565 Val Loss 1.5043123960494995
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Epoch 49 Train Acc 97.03421783447266% Val Acc 67.80000305175781% Train Loss 0.04460388794541359 Val Loss 1.6288608312606812
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 50 Train Acc 97.22053527832031% Val Acc 59.400001525878906% Train Loss 0.04442134127020836 Val Loss 2.3222196102142334
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 51 Train Acc 97.28897094726562% Val Acc 67.0% Train Loss 0.042414288967847824 Val Loss 1.617242693901062
EPOCH unfeeze : 2
Trainable Parameters : 151419140
