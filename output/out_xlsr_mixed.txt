Sun Nov 6 22:13:21 AEDT 2022
2022-11-06 22:13:24.314513: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-06 22:13:24.906729: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-06 22:13:27.101581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-06 22:13:27.103911: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-06 22:13:27.103931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_mixed.py
Started: 06/11/2022 22:13:41

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-mixed
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-mixed
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-mixed_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 20 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.2693, -0.2123, -0.5357,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1281, -0.0925, -0.0349,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3046, -0.3846, -0.4365,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.2935, -0.5868, -0.4485,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1054,  0.0089, -0.0226,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0250, -0.1220, -0.1276,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 3, 1, 3, 0, 2, 2, 2, 0, 0, 2, 1, 3, 0, 2, 3, 2, 0, 2, 2, 2, 1, 2, 0,
        0, 3, 2, 3, 2, 0, 1, 0, 2, 2, 3, 0, 3, 3, 2, 2])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[-0.0126,  0.0071, -0.0079,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0359,  0.0364,  0.0382,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1456, -0.1769, -0.0933,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.3543,  0.3481,  0.2343,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0900,  0.0495,  0.1690,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0052,  0.0076,  0.0197,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 1, 1, 0, 1, 1, 1, 2, 3, 0, 0, 2, 0, 3, 2, 1, 1, 0, 3, 2, 2, 1, 0,
        3, 0, 1, 1, 2, 0, 1, 2, 3, 3, 0, 3, 0, 0, 2, 3])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.weight', 'projector.bias', 'classifier.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-2.5642e-01, -1.5344e-01, -7.9598e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.9275e-01, -7.7829e-01, -4.7369e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.9671e-03,  7.9671e-03,  7.9671e-03,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.7514e-01, -5.6809e-01, -9.1949e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.7745e-01,  5.9476e-01,  6.4199e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.7859e-04, -6.7859e-04, -8.4466e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 3, 0, 0, 1, 0, 3, 0, 3, 2, 1, 3, 1, 1, 1, 2, 1, 0, 0, 3, 3, 1, 1, 3,
        3, 1, 2, 2, 2, 3, 0, 0, 0, 3, 1, 1, 0, 2, 1, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  4 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Traceback (most recent call last):
  File "run_xlsr_mixed.py", line 721, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_xlsr_mixed.py", line 546, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_xlsr_mixed.py", line 566, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_xlsr_mixed.py", line 609, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1793, in forward
    outputs = self.wav2vec2(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1304, in forward
    encoder_outputs = self.encoder(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 876, in forward
    layer_outputs = layer(
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 715, in forward
    hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 652, in forward
    hidden_states = self.intermediate_dropout(hidden_states)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1279, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 0; 31.75 GiB total capacity; 29.88 GiB already allocated; 120.00 MiB free; 30.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Mon Nov 7 01:26:06 AEDT 2022
2022-11-07 01:26:08.382489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-07 01:26:08.768958: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-11-07 01:26:08.918735: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-07 01:26:10.663140: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-07 01:26:10.665348: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-07 01:26:10.665358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_mixed.py
Started: 07/11/2022 01:26:22

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-mixed
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-mixed
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-mixed_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 20 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.1573,  0.2637,  0.3103,  ...,  0.0000,  0.0000,  0.0000],
        [ 2.3106,  1.7824,  1.2956,  ...,  0.1081,  0.1003,  0.1003],
        [ 0.1930, -0.1649, -0.4711,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0831, -0.0530, -0.1776,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 3, 2])}
Training DataCustom Files: 10502
Training Data Files: 2626
Val Data Sample
{'input_values': tensor([[-0.1850, -0.1661, -0.1711,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.4173,  1.2800,  1.1212,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0337,  0.0651, -0.0119,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.9325,  1.6524,  1.6133,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 3, 1, 3])}
Test CustomData Files: 813
Test Data Files: 204
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0215, -0.0529,  0.1133,  ...,  0.0000,  0.0000,  0.0000],
        [-0.7232, -1.2899, -0.6286,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0210, -0.0335, -0.0089,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2295,  0.0475, -0.0246,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 3, 0, 2])}
Test CustomData Files: 398
Test Data Files: 100
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  4 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Epoch 0 Train Acc 40.9843864440918% Val Acc 33.25% Train Loss 0.645602822303772 Val Loss 1.4396593570709229
Trainable Parameters : 264452
Epoch 1 Train Acc 47.69611740112305% Val Acc 35.75% Train Loss 0.597018837928772 Val Loss 1.4161312580108643
Trainable Parameters : 264452
Epoch 2 Train Acc 51.961158752441406% Val Acc 41.75% Train Loss 0.5685254335403442 Val Loss 1.3014123439788818
Trainable Parameters : 264452
Epoch 3 Train Acc 52.465728759765625% Val Acc 30.25% Train Loss 0.5568887591362 Val Loss 1.6584442853927612
Trainable Parameters : 264452
Epoch 4 Train Acc 54.81721496582031% Val Acc 42.25% Train Loss 0.5414535999298096 Val Loss 1.39066743850708
Trainable Parameters : 264452
Epoch 5 Train Acc 56.39756393432617% Val Acc 47.5% Train Loss 0.5267149806022644 Val Loss 1.2586658000946045
Trainable Parameters : 264452
Epoch 6 Train Acc 57.330543518066406% Val Acc 50.0% Train Loss 0.5211913585662842 Val Loss 1.1797630786895752
Trainable Parameters : 264452
Epoch 7 Train Acc 57.53998565673828% Val Acc 43.0% Train Loss 0.5109866857528687 Val Loss 1.4297171831130981
Trainable Parameters : 264452
Epoch 8 Train Acc 59.177459716796875% Val Acc 55.75% Train Loss 0.503045380115509 Val Loss 1.1245259046554565
Trainable Parameters : 264452
Epoch 9 Train Acc 58.653846740722656% Val Acc 49.25% Train Loss 0.5010823607444763 Val Loss 1.324800968170166
Trainable Parameters : 264452
Epoch 10 Train Acc 60.14851760864258% Val Acc 51.75% Train Loss 0.49435362219810486 Val Loss 1.2890095710754395
Trainable Parameters : 264452
Epoch 11 Train Acc 60.453163146972656% Val Acc 40.0% Train Loss 0.4886883497238159 Val Loss 1.7051055431365967
Trainable Parameters : 264452
Epoch 12 Train Acc 60.73876953125% Val Acc 39.25% Train Loss 0.4868825674057007 Val Loss 1.5038644075393677
Trainable Parameters : 264452
Epoch 13 Train Acc 61.10053634643555% Val Acc 55.25% Train Loss 0.4813803732395172 Val Loss 1.1133793592453003
Trainable Parameters : 264452
Epoch 14 Train Acc 60.88157272338867% Val Acc 52.5% Train Loss 0.48097363114356995 Val Loss 1.152430534362793
Trainable Parameters : 264452
Epoch 15 Train Acc 61.329017639160156% Val Acc 46.75% Train Loss 0.47864440083503723 Val Loss 1.2769488096237183
Trainable Parameters : 264452
Epoch 16 Train Acc 62.36671829223633% Val Acc 47.75% Train Loss 0.47182372212409973 Val Loss 1.401475191116333
Trainable Parameters : 264452
Epoch 17 Train Acc 62.25247573852539% Val Acc 43.75% Train Loss 0.4726507365703583 Val Loss 1.4409518241882324
Trainable Parameters : 264452
Epoch 18 Train Acc 62.63328552246094% Val Acc 54.75% Train Loss 0.4679543375968933 Val Loss 1.1657785177230835
Trainable Parameters : 264452
Epoch 19 Train Acc 62.25247573852539% Val Acc 54.75% Train Loss 0.4692288935184479 Val Loss 1.1117687225341797
Trainable Parameters : 264452
Epoch 20 Train Acc 62.57616424560547% Val Acc 56.5% Train Loss 0.4662612974643707 Val Loss 1.1179577112197876
Trainable Parameters : 264452
Epoch 21 Train Acc 62.82368850708008% Val Acc 53.0% Train Loss 0.4627395272254944 Val Loss 1.2180691957473755
Trainable Parameters : 264452
Epoch 22 Train Acc 62.89984893798828% Val Acc 56.75% Train Loss 0.4635489284992218 Val Loss 1.1055045127868652
Trainable Parameters : 264452
Epoch 23 Train Acc 62.680885314941406% Val Acc 56.0% Train Loss 0.46596065163612366 Val Loss 1.1000131368637085
Trainable Parameters : 264452
Epoch 24 Train Acc 62.747528076171875% Val Acc 49.5% Train Loss 0.4608558416366577 Val Loss 1.3369213342666626
Trainable Parameters : 264452
Epoch 25 Train Acc 64.27075958251953% Val Acc 57.75% Train Loss 0.45653554797172546 Val Loss 1.1114985942840576
Trainable Parameters : 264452
Epoch 26 Train Acc 63.16641616821289% Val Acc 46.25% Train Loss 0.459142804145813 Val Loss 1.437957525253296
Trainable Parameters : 264452
Epoch 27 Train Acc 64.06130981445312% Val Acc 52.25% Train Loss 0.4523179829120636 Val Loss 1.2558547258377075
Trainable Parameters : 264452
Epoch 28 Train Acc 63.67098617553711% Val Acc 52.5% Train Loss 0.4540218114852905 Val Loss 1.2786824703216553
Trainable Parameters : 264452
Epoch 29 Train Acc 64.327880859375% Val Acc 45.75% Train Loss 0.4524178206920624 Val Loss 1.4886252880096436
Trainable Parameters : 264452
Epoch 30 Train Acc 64.4802017211914% Val Acc 52.0% Train Loss 0.45092007517814636 Val Loss 1.194251537322998
Trainable Parameters : 264452
Epoch 31 Train Acc 63.65194320678711% Val Acc 55.25% Train Loss 0.4559207260608673 Val Loss 1.1448396444320679
Trainable Parameters : 264452
Epoch 32 Train Acc 63.690025329589844% Val Acc 54.75% Train Loss 0.4553799331188202 Val Loss 1.2017213106155396
Trainable Parameters : 264452
Epoch 33 Train Acc 64.29931640625% Val Acc 50.5% Train Loss 0.4491923749446869 Val Loss 1.2220523357391357
Trainable Parameters : 264452
Epoch 34 Train Acc 64.63252258300781% Val Acc 55.25% Train Loss 0.44935256242752075 Val Loss 1.1169884204864502
Trainable Parameters : 264452
Epoch 35 Train Acc 64.2802734375% Val Acc 52.75% Train Loss 0.44607388973236084 Val Loss 1.2584524154663086
Trainable Parameters : 264452
Epoch 36 Train Acc 64.40403747558594% Val Acc 57.0% Train Loss 0.44983339309692383 Val Loss 1.105978012084961
Trainable Parameters : 264452
Epoch 37 Train Acc 64.64204406738281% Val Acc 57.75% Train Loss 0.44650039076805115 Val Loss 1.0535043478012085
Trainable Parameters : 264452
Epoch 38 Train Acc 64.97525024414062% Val Acc 50.5% Train Loss 0.44450291991233826 Val Loss 1.3797454833984375
Trainable Parameters : 264452
Epoch 39 Train Acc 64.36595916748047% Val Acc 49.25% Train Loss 0.4471132457256317 Val Loss 1.3163927793502808
Trainable Parameters : 264452
Epoch 40 Train Acc 64.93717193603516% Val Acc 55.0% Train Loss 0.4469793438911438 Val Loss 1.1477842330932617
Trainable Parameters : 264452
Epoch 41 Train Acc 64.65156555175781% Val Acc 52.25% Train Loss 0.44314876198768616 Val Loss 1.196069598197937
Trainable Parameters : 264452
Epoch 42 Train Acc 64.59444427490234% Val Acc 57.25% Train Loss 0.44561341404914856 Val Loss 1.0689630508422852
Trainable Parameters : 264452
Epoch 43 Train Acc 65.47982025146484% Val Acc 54.75% Train Loss 0.4435926377773285 Val Loss 1.150336742401123
Trainable Parameters : 264452
Epoch 44 Train Acc 64.4421157836914% Val Acc 54.5% Train Loss 0.4432128667831421 Val Loss 1.1713166236877441
Trainable Parameters : 264452
