/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-fa98687e9ed75940
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_umbrellaDID_og.py
Started: 21/07/2022 16:08:39

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_1file
evaluation_filename: adi17_test_small
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 5
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_1file.csv
--> data_test_fp: data/adi17_test_small.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET... ------------------------------------

  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 289.06it/s]
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-02bd04b19e1b00c4.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-dc04a7aa8f60af4f.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-127fb091b1d8093e.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-309209f984cf2104.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-974d130b25f1d9fc.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-86029afe14bb1d3a.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-00fc359ae7b05e81.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d9ba4811692a40c2.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-9cbaac49c6b30502.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-fa98687e9ed75940/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-dc2c4e2f09b6d456.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_hid.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_q.weight', 'project_hid.bias', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
/home/z5208494/.local/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
--> dataset...
DatasetDict({
    train: Dataset({
        features: ['id', 'label'],
        num_rows: 12
    })
    test: Dataset({
        features: ['id', 'label'],
        num_rows: 39
    })
})
--> Printing some random samples...
                          id label
0  QbkHUIBVnpc_027147-027775   LEV
1  QbkHUIBVnpc_024459-025349   LEV
2  QbkHUIBVnpc_014824-015134   LEV
3  cFC3LxRavZQ_008960-010574   EGY
4  QbkHUIBVnpc_025428-025924   LEV
                          id label
0  4ewFIFh7LgM_010308-010664   NOR
1  yChCQ16Qq08_133255-133769   GLF
2  9JkXg8fH7Y0_009027-009343   EGY
3  3oK1s1AgqzQ_140596-141260   LEV
4  3oK1s1AgqzQ_126213-127669   LEV
SUCCESS: Prepared dataset.

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
--> Verifying data with a random sample...
12
4
Dialect Label: LEV
Input array shape: (1, 1600)
39
0
Dialect Label: NOR
Input array shape: (1, 1600)
0
Training labels 0 NOR
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining data collator...
SUCCESS: Data collator defined.
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

SAMPLE:  {'input_values': tensor([ 0.0440,  0.0084, -0.0014,  ...,  0.0636, -0.0001, -0.0277]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]), 'label': 3}
SAMPLE:  {'input_values': tensor([-0.0441, -0.0311, -0.0532,  ..., -1.6077, -1.4617, -1.3014]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]), 'label': 3}
SAMPLE:  {'input_values': tensor([ 0.0355,  0.2541, -0.0425,  ...,  0.2624, -0.3636, -0.5550]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]), 'label': 3}
SAMPLE:  {'input_values': tensor([0.1167, 0.1157, 0.1165,  ..., 0.1609, 0.2548, 0.2458]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]), 'label': 1}
OPTIMIZER Adafactor (
Parameter Group 0
    beta1: None
    clip_threshold: 1.0
    decay_rate: -0.8
    eps: (1e-30, 0.001)
    lr: 0.0
    relative_step: True
    scale_parameter: True
    warmup_init: True
    weight_decay: 0.0
)
LOGITS tensor([[ 0.0110,  0.0116,  0.0143,  0.0306],
        [ 0.0127,  0.0011,  0.0025,  0.0039],
        [-0.0426,  0.0113, -0.0378,  0.0083],
        [ 0.0089,  0.0120, -0.0594, -0.0363]], grad_fn=<AddmmBackward0>)
LABELS tensor([3, 3, 3, 1])
before loss
LOSS tensor(1.3698, grad_fn=<NllLossBackward0>)
before zero_grad optimizer
LOSS after zero tensor(0.6849, grad_fn=<DivBackward0>)
Traceback (most recent call last):
  File "run_umbrellaDID_og.py", line 1007, in <module>
    trainer.fit(trainDataLoader, testDataLoader, 10)
  File "run_umbrellaDID_og.py", line 819, in fit
    train_loss = self._train(train_loader)
  File "run_umbrellaDID_og.py", line 847, in _train
    loss.backward()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 146, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: [enforce fail at alloc_cpu.cpp:73] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 9437184 bytes. Error code 12 (Cannot allocate memory)
