Wed Nov 16 17:27:52 AEDT 2022
2022-11-16 17:27:56.489069: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 17:27:57.191449: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 17:28:00.315094: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:28:00.317327: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:28:00.317348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_200f.py
Started: 16/11/2022 17:28:15

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-200f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_200f
train_filename: u_train_200f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_200f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_200f_local/ADI17-xlsr-araic-200f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_200f_local/ADI17-xlsr-araic-200f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Traceback (most recent call last):
  File "run_xlsr_200f.py", line 378, in <module>
    traincustomdata = CustomDataset(
  File "/home/z5208494/thesis/customData.py", line 35, in __init__
    self.data_frame = pd.read_csv(csv_fp, delimiter=',')
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/apps/python/3.8.3/lib/python3.8/site-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File data/u_train_200f.csv does not exist: 'data/u_train_200f.csv'
Wed Nov 16 17:34:21 AEDT 2022
2022-11-16 17:34:23.359712: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-16 17:34:23.655227: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-16 17:34:25.623796: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:34:25.623940: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/sox/14.4.2/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64_lin/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.4:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/debugger/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/libipt/intel64/lib:/apps/intel/Composer/debugger_2019/iga/lib:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64:/apps/intel/Composer/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/apps/python/3.8.3/lib:/apps/gcc/8.4.0/lib64:/apps/gcc/8.4.0/lib:/apps/cuda/11.1/lib64:/apps/cuda/11.1/extras/CUPTI/lib64
2022-11-16 17:34:25.623954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_200f.py
Started: 16/11/2022 17:34:39

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-araic-200f
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_200f
train_filename: u_train_200f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 50
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_200f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: /srv/scratch/z5208494/output/u_train_200f_local/ADI17-xlsr-araic-200f
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_200f_local/ADI17-xlsr-araic-200f_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.9670, -1.1343, -1.1630,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0079,  0.1405, -0.0628,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0547, -0.0174,  0.0979,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.1228, -0.1603, -0.2590,  ..., -0.0394, -0.0317,  0.0180],
        [ 0.1047,  0.1047,  0.1257,  ..., -1.3336, -1.4835, -1.5133],
        [-0.2241, -0.0440,  0.0847,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 3, 1, 3, 1, 1, 0, 1, 0, 2, 0, 0, 0, 2, 0, 2, 1, 3, 1, 3, 1, 0, 2, 1,
        2, 1, 3, 0, 3, 3, 0, 1, 3, 0, 3, 3, 2, 2, 0, 0])}
Training DataCustom Files: 718
Training Data Files: 18
Val Data Sample
{'input_values': tensor([[-6.6675e-03, -9.8046e-03,  8.6166e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2500e+00, -1.2877e+00, -1.3519e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6143e-01,  1.4289e-01,  1.4418e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-2.1413e-01, -5.6220e-02,  4.8245e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.4393e-01, -4.5376e-01, -3.9984e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3849e+00, -1.4961e+00, -1.5095e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 3, 3, 2, 2, 2, 0, 1, 1, 2, 2, 0, 0, 2, 3, 1, 0, 3, 3, 3, 1, 3, 2,
        1, 3, 0, 1, 1, 3, 3, 3, 1, 2, 1, 0, 0, 2, 2, 1])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.bias', 'classifier.weight', 'projector.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.0280, -0.0213, -0.0669,  ...,  0.0000,  0.0000,  0.0000],
        [-0.4482, -0.5700, -0.6391,  ...,  0.1453, -0.3222, -0.3616],
        [-0.2086, -0.2529, -0.2576,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.6199,  0.6318,  0.5678,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2487,  0.2356,  0.2252,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.5925,  0.3576,  1.0196,  ..., -0.1081, -0.1409, -0.2969]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([2, 1, 0, 3, 2, 3, 3, 0, 1, 2, 2, 1, 1, 1, 0, 0, 3, 1, 3, 1, 3, 0, 0, 1,
        0, 3, 1, 1, 0, 1, 0, 3, 2, 2, 1, 2, 0, 1, 0, 2])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 0 Train Acc 25.83333396911621% Val Acc 23.200000762939453% Train Loss 0.6947957277297974 Val Loss 1.3914835453033447
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 1 Train Acc 25.05555534362793% Val Acc 21.700000762939453% Train Loss 0.6947082877159119 Val Loss 1.391517162322998
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 2 Train Acc 24.94444465637207% Val Acc 22.700000762939453% Train Loss 0.6942152976989746 Val Loss 1.389398217201233
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 3 Train Acc 27.83333396911621% Val Acc 22.899999618530273% Train Loss 0.6927328705787659 Val Loss 1.3917008638381958
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 4 Train Acc 27.11111068725586% Val Acc 26.200000762939453% Train Loss 0.6920359134674072 Val Loss 1.3868080377578735
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 5 Train Acc 31.0% Val Acc 24.30000114440918% Train Loss 0.6907138824462891 Val Loss 1.3902873992919922
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 6 Train Acc 29.77777862548828% Val Acc 22.399999618530273% Train Loss 0.6894654035568237 Val Loss 1.391420841217041
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 7 Train Acc 30.11111068725586% Val Acc 25.80000114440918% Train Loss 0.6880698800086975 Val Loss 1.3860459327697754
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 8 Train Acc 29.83333396911621% Val Acc 26.30000114440918% Train Loss 0.6861873865127563 Val Loss 1.387942910194397
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 9 Train Acc 29.44444465637207% Val Acc 26.200000762939453% Train Loss 0.6850312948226929 Val Loss 1.3918100595474243
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 10 Train Acc 31.27777862548828% Val Acc 24.100000381469727% Train Loss 0.6825336217880249 Val Loss 1.3962676525115967
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 11 Train Acc 34.27777862548828% Val Acc 27.200000762939453% Train Loss 0.6800820231437683 Val Loss 1.388379454612732
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 12 Train Acc 36.11111068725586% Val Acc 21.600000381469727% Train Loss 0.6787732243537903 Val Loss 1.4043391942977905
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 13 Train Acc 35.61111068725586% Val Acc 25.700000762939453% Train Loss 0.6750524640083313 Val Loss 1.4026219844818115
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 14 Train Acc 38.05555725097656% Val Acc 28.200000762939453% Train Loss 0.6710395216941833 Val Loss 1.3886339664459229
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 15 Train Acc 41.55555725097656% Val Acc 28.600000381469727% Train Loss 0.6684893369674683 Val Loss 1.3930120468139648
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 16 Train Acc 42.33333206176758% Val Acc 25.5% Train Loss 0.6610323190689087 Val Loss 1.395965814590454
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 17 Train Acc 45.22222137451172% Val Acc 28.200000762939453% Train Loss 0.6494970321655273 Val Loss 1.3705835342407227
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 18 Train Acc 46.9444465637207% Val Acc 30.200000762939453% Train Loss 0.6355960369110107 Val Loss 1.4029101133346558
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 19 Train Acc 49.38888931274414% Val Acc 30.100000381469727% Train Loss 0.6164759993553162 Val Loss 1.3745390176773071
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 20 Train Acc 50.9444465637207% Val Acc 29.5% Train Loss 0.5965682864189148 Val Loss 1.4282306432724
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 21 Train Acc 55.27777862548828% Val Acc 32.20000076293945% Train Loss 0.5744639039039612 Val Loss 1.4448148012161255
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 22 Train Acc 56.9444465637207% Val Acc 34.5% Train Loss 0.5525519847869873 Val Loss 1.4794692993164062
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 23 Train Acc 60.9444465637207% Val Acc 42.10000228881836% Train Loss 0.5288779735565186 Val Loss 1.409574031829834
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 24 Train Acc 62.38888931274414% Val Acc 37.5% Train Loss 0.5036228895187378 Val Loss 1.5106334686279297
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 25 Train Acc 66.55555725097656% Val Acc 40.400001525878906% Train Loss 0.47457948327064514 Val Loss 1.4583945274353027
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 26 Train Acc 67.66666412353516% Val Acc 37.0% Train Loss 0.4446047842502594 Val Loss 1.5602085590362549
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 27 Train Acc 71.66666412353516% Val Acc 43.79999923706055% Train Loss 0.40923798084259033 Val Loss 1.44340980052948
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 28 Train Acc 77.11111450195312% Val Acc 45.20000076293945% Train Loss 0.3642076849937439 Val Loss 1.4482063055038452
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 29 Train Acc 77.61111450195312% Val Acc 40.79999923706055% Train Loss 0.33959874510765076 Val Loss 1.588232398033142
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 30 Train Acc 79.83333587646484% Val Acc 37.400001525878906% Train Loss 0.30619126558303833 Val Loss 1.9040775299072266
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 31 Train Acc 82.72222137451172% Val Acc 42.900001525878906% Train Loss 0.26931455731391907 Val Loss 1.6627596616744995
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 32 Train Acc 84.5% Val Acc 42.400001525878906% Train Loss 0.236904576420784 Val Loss 1.7523876428604126
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 33 Train Acc 88.0% Val Acc 45.5% Train Loss 0.20359930396080017 Val Loss 1.659803032875061
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 34 Train Acc 90.27777862548828% Val Acc 41.79999923706055% Train Loss 0.17694784700870514 Val Loss 2.041516065597534
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 35 Train Acc 93.11111450195312% Val Acc 41.5% Train Loss 0.13758426904678345 Val Loss 1.9717720746994019
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 36 Train Acc 91.66666412353516% Val Acc 47.5% Train Loss 0.13707943260669708 Val Loss 1.8656069040298462
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 37 Train Acc 90.55555725097656% Val Acc 43.70000076293945% Train Loss 0.15264835953712463 Val Loss 2.091313123703003
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 38 Train Acc 95.5% Val Acc 46.20000076293945% Train Loss 0.09708315134048462 Val Loss 2.172314405441284
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 39 Train Acc 95.5% Val Acc 45.900001525878906% Train Loss 0.09120038896799088 Val Loss 2.165989637374878
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 40 Train Acc 96.33333587646484% Val Acc 48.60000228881836% Train Loss 0.07165133953094482 Val Loss 2.216830015182495
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 41 Train Acc 97.94444274902344% Val Acc 46.60000228881836% Train Loss 0.06123592332005501 Val Loss 2.324805974960327
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 42 Train Acc 95.44444274902344% Val Acc 48.0% Train Loss 0.07038760185241699 Val Loss 2.159640073776245
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 43 Train Acc 98.44444274902344% Val Acc 47.60000228881836% Train Loss 0.043413519859313965 Val Loss 2.2619686126708984
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 44 Train Acc 98.8888931274414% Val Acc 45.79999923706055% Train Loss 0.03647657111287117 Val Loss 2.4851932525634766
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 45 Train Acc 96.27777862548828% Val Acc 50.5% Train Loss 0.05760229378938675 Val Loss 2.348417043685913
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 46 Train Acc 96.44444274902344% Val Acc 49.60000228881836% Train Loss 0.05989129841327667 Val Loss 2.4829366207122803
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 47 Train Acc 97.61111450195312% Val Acc 47.5% Train Loss 0.047665443271398544 Val Loss 2.660202980041504
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 48 Train Acc 97.5% Val Acc 48.60000228881836% Train Loss 0.054613519459962845 Val Loss 2.5128562450408936
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Epoch 49 Train Acc 97.22222137451172% Val Acc 46.79999923706055% Train Loss 0.04773756489157677 Val Loss 2.4821407794952393
EPOCH unfeeze : 0
Trainable Parameters : 151419140
Epoch 50 Train Acc 98.22222137451172% Val Acc 50.29999923706055% Train Loss 0.03300037980079651 Val Loss 2.5439560413360596
EPOCH unfeeze : 1
Trainable Parameters : 151419140
Epoch 51 Train Acc 99.11111450195312% Val Acc 50.20000076293945% Train Loss 0.022996358573436737 Val Loss 2.5018672943115234
EPOCH unfeeze : 2
Trainable Parameters : 151419140
Epoch 52 Train Acc 98.33333587646484% Val Acc 49.79999923706055% Train Loss 0.036025580018758774 Val Loss 2.4164254665374756
EPOCH unfeeze : 3
Trainable Parameters : 151419140
Epoch 53 Train Acc 98.05555725097656% Val Acc 54.20000076293945% Train Loss 0.03464954346418381 Val Loss 2.3170852661132812
EPOCH unfeeze : 4
Trainable Parameters : 151419140
Epoch 54 Train Acc 98.44444274902344% Val Acc 53.70000076293945% Train Loss 0.030249008908867836 Val Loss 2.427417516708374
EPOCH unfeeze : 5
Trainable Parameters : 151419140
Epoch 55 Train Acc 97.77777862548828% Val Acc 48.70000076293945% Train Loss 0.040620170533657074 Val Loss 2.6582581996917725
EPOCH unfeeze : 6
Trainable Parameters : 151419140
Epoch 56 Train Acc 98.5% Val Acc 52.60000228881836% Train Loss 0.028357747942209244 Val Loss 2.5468389987945557
EPOCH unfeeze : 7
Trainable Parameters : 151419140
Epoch 57 Train Acc 99.0% Val Acc 44.5% Train Loss 0.02480936422944069 Val Loss 3.263021469116211
EPOCH unfeeze : 8
Trainable Parameters : 151419140
Epoch 58 Train Acc 99.8888931274414% Val Acc 52.70000076293945% Train Loss 0.008501313626766205 Val Loss 2.6388251781463623
EPOCH unfeeze : 9
Trainable Parameters : 151419140
Epoch 59 Train Acc 98.61111450195312% Val Acc 54.5% Train Loss 0.022534245625138283 Val Loss 2.5041966438293457
EPOCH unfeeze : 10
Trainable Parameters : 151419140
Epoch 60 Train Acc 99.55555725097656% Val Acc 48.0% Train Loss 0.0081794373691082 Val Loss 3.1085689067840576
EPOCH unfeeze : 11
Trainable Parameters : 151419140
Epoch 61 Train Acc 99.61111450195312% Val Acc 51.79999923706055% Train Loss 0.010200650431215763 Val Loss 2.7436318397521973
EPOCH unfeeze : 12
Trainable Parameters : 151419140
Epoch 62 Train Acc 97.77777862548828% Val Acc 47.5% Train Loss 0.0394577793776989 Val Loss 2.905139684677124
EPOCH unfeeze : 13
Trainable Parameters : 151419140
Epoch 63 Train Acc 98.33333587646484% Val Acc 49.20000076293945% Train Loss 0.025784388184547424 Val Loss 2.937166690826416
EPOCH unfeeze : 14
Trainable Parameters : 151419140
Epoch 64 Train Acc 98.55555725097656% Val Acc 52.70000076293945% Train Loss 0.023456428200006485 Val Loss 2.8384735584259033
EPOCH unfeeze : 15
Trainable Parameters : 151419140
Epoch 65 Train Acc 99.22222137451172% Val Acc 51.5% Train Loss 0.013998406007885933 Val Loss 2.8220629692077637
EPOCH unfeeze : 16
Trainable Parameters : 151419140
Epoch 66 Train Acc 98.22222137451172% Val Acc 50.5% Train Loss 0.025333695113658905 Val Loss 2.931659460067749
EPOCH unfeeze : 17
Trainable Parameters : 151419140
Epoch 67 Train Acc 98.61111450195312% Val Acc 49.79999923706055% Train Loss 0.0216697845607996 Val Loss 2.86441969871521
EPOCH unfeeze : 18
Trainable Parameters : 151419140
Epoch 68 Train Acc 99.11111450195312% Val Acc 49.10000228881836% Train Loss 0.02006872370839119 Val Loss 3.1910109519958496
EPOCH unfeeze : 19
Trainable Parameters : 151419140
Epoch 69 Train Acc 98.55555725097656% Val Acc 44.70000076293945% Train Loss 0.024831729009747505 Val Loss 3.398653745651245
EPOCH unfeeze : 20
Trainable Parameters : 151419140
Epoch 70 Train Acc 98.3888931274414% Val Acc 47.29999923706055% Train Loss 0.02615133859217167 Val Loss 3.377796173095703
EPOCH unfeeze : 21
Trainable Parameters : 151419140
Epoch 71 Train Acc 98.66666412353516% Val Acc 50.400001525878906% Train Loss 0.024014607071876526 Val Loss 3.0673673152923584
EPOCH unfeeze : 22
Trainable Parameters : 151419140
Epoch 72 Train Acc 99.77777862548828% Val Acc 51.400001525878906% Train Loss 0.00889308750629425 Val Loss 3.0962865352630615
EPOCH unfeeze : 23
Trainable Parameters : 151419140
Epoch 73 Train Acc 98.77777862548828% Val Acc 51.79999923706055% Train Loss 0.02409978210926056 Val Loss 2.920354127883911
EPOCH unfeeze : 24
Trainable Parameters : 151419140
Epoch 74 Train Acc 100.0% Val Acc 50.29999923706055% Train Loss 0.0016333982348442078 Val Loss 3.208282232284546
EPOCH unfeeze : 25
Trainable Parameters : 151419140
Epoch 75 Train Acc 99.33333587646484% Val Acc 46.5% Train Loss 0.01884966529905796 Val Loss 3.483602523803711
EPOCH unfeeze : 26
Trainable Parameters : 151419140
Epoch 76 Train Acc 98.94444274902344% Val Acc 46.400001525878906% Train Loss 0.02256462350487709 Val Loss 3.4141201972961426
EPOCH unfeeze : 27
Trainable Parameters : 151419140
Epoch 77 Train Acc 99.77777862548828% Val Acc 52.70000076293945% Train Loss 0.011935059912502766 Val Loss 3.1670289039611816
EPOCH unfeeze : 28
Trainable Parameters : 151419140
Epoch 78 Train Acc 99.66666412353516% Val Acc 53.70000076293945% Train Loss 0.013477548025548458 Val Loss 2.9511516094207764
EPOCH unfeeze : 29
Trainable Parameters : 151419140
Epoch 79 Train Acc 99.72222137451172% Val Acc 49.29999923706055% Train Loss 0.006813549902290106 Val Loss 3.3515923023223877
EPOCH unfeeze : 30
Trainable Parameters : 151419140
Epoch 80 Train Acc 99.3888931274414% Val Acc 50.10000228881836% Train Loss 0.013757050037384033 Val Loss 3.3989665508270264
EPOCH unfeeze : 31
Trainable Parameters : 151419140
Epoch 81 Train Acc 96.33333587646484% Val Acc 55.10000228881836% Train Loss 0.057218875735998154 Val Loss 2.908048152923584
EPOCH unfeeze : 32
Trainable Parameters : 151419140
Epoch 82 Train Acc 98.77777862548828% Val Acc 55.29999923706055% Train Loss 0.01994789019227028 Val Loss 2.749305486679077
EPOCH unfeeze : 33
Trainable Parameters : 151419140
Epoch 83 Train Acc 98.77777862548828% Val Acc 52.60000228881836% Train Loss 0.023234594613313675 Val Loss 3.041072368621826
EPOCH unfeeze : 34
Trainable Parameters : 151419140
Epoch 84 Train Acc 98.16666412353516% Val Acc 51.79999923706055% Train Loss 0.03249133750796318 Val Loss 2.9040744304656982
EPOCH unfeeze : 35
Trainable Parameters : 151419140
Epoch 85 Train Acc 99.8888931274414% Val Acc 50.70000076293945% Train Loss 0.004994596354663372 Val Loss 3.136143207550049
EPOCH unfeeze : 36
Trainable Parameters : 151419140
Epoch 86 Train Acc 99.8888931274414% Val Acc 40.79999923706055% Train Loss 0.005696149542927742 Val Loss 3.7538821697235107
EPOCH unfeeze : 37
Trainable Parameters : 151419140
Epoch 87 Train Acc 99.22222137451172% Val Acc 53.900001525878906% Train Loss 0.013954786583781242 Val Loss 2.918440103530884
EPOCH unfeeze : 38
Trainable Parameters : 151419140
Epoch 88 Train Acc 98.94444274902344% Val Acc 42.900001525878906% Train Loss 0.015345600433647633 Val Loss 3.685748815536499
EPOCH unfeeze : 39
Trainable Parameters : 151419140
Epoch 89 Train Acc 99.05555725097656% Val Acc 49.400001525878906% Train Loss 0.019841931760311127 Val Loss 3.217798948287964
EPOCH unfeeze : 40
Trainable Parameters : 151419140
Epoch 90 Train Acc 99.44444274902344% Val Acc 46.79999923706055% Train Loss 0.014824938960373402 Val Loss 3.5615463256835938
EPOCH unfeeze : 41
Trainable Parameters : 151419140
Epoch 91 Train Acc 98.16666412353516% Val Acc 44.60000228881836% Train Loss 0.027929889038205147 Val Loss 3.870556592941284
EPOCH unfeeze : 42
Trainable Parameters : 151419140
Epoch 92 Train Acc 99.05555725097656% Val Acc 48.400001525878906% Train Loss 0.017561741173267365 Val Loss 3.3216168880462646
EPOCH unfeeze : 43
Trainable Parameters : 151419140
Epoch 93 Train Acc 98.16666412353516% Val Acc 36.900001525878906% Train Loss 0.026368770748376846 Val Loss 4.4442548751831055
EPOCH unfeeze : 44
Trainable Parameters : 151419140
Epoch 94 Train Acc 97.44444274902344% Val Acc 47.60000228881836% Train Loss 0.04578632861375809 Val Loss 3.2833468914031982
EPOCH unfeeze : 45
Trainable Parameters : 151419140
Epoch 95 Train Acc 99.5% Val Acc 54.29999923706055% Train Loss 0.008396160788834095 Val Loss 2.8060643672943115
EPOCH unfeeze : 46
Trainable Parameters : 151419140
Epoch 96 Train Acc 98.77777862548828% Val Acc 40.29999923706055% Train Loss 0.020784828811883926 Val Loss 4.138889789581299
EPOCH unfeeze : 47
Trainable Parameters : 151419140
Epoch 97 Train Acc 99.5% Val Acc 45.900001525878906% Train Loss 0.010537082329392433 Val Loss 3.620375394821167
EPOCH unfeeze : 48
Trainable Parameters : 151419140
Epoch 98 Train Acc 99.0% Val Acc 48.5% Train Loss 0.012705096043646336 Val Loss 3.6850433349609375
EPOCH unfeeze : 49
Trainable Parameters : 151419140
Configuration saved in /srv/scratch/z5208494/output/u_train_200f_local/ADI17-xlsr-araic-200f/config.json
Model weights saved in /srv/scratch/z5208494/output/u_train_200f_local/ADI17-xlsr-araic-200f/pytorch_model.bin
Epoch 99 Train Acc 99.66666412353516% Val Acc 58.900001525878906% Train Loss 0.009822024032473564 Val Loss 2.779035806655884

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:50.20000076293945% Loss:3.021002769470215
CONFUSION MATRIX
[[41 11 22 26]
 [ 3 20 42 35]
 [ 4  2 78 14]
 [ 4  3 33 60]]
CONFUSION MATRIX NORMALISED
[[0.10301508 0.02763819 0.05527638 0.06532663]
 [0.00753769 0.05025126 0.10552764 0.0879397 ]
 [0.01005025 0.00502513 0.1959799  0.03517588]
 [0.01005025 0.00753769 0.08291457 0.15075377]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.79      0.41      0.54       100
           1       0.56      0.20      0.29       100
           2       0.45      0.80      0.57        98
           3       0.44      0.60      0.51       100

    accuracy                           0.50       398
   macro avg       0.56      0.50      0.48       398
weighted avg       0.56      0.50      0.48       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 16/11/2022 18:37:39
