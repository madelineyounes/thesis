Tue Nov 1 02:23:09 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_group.py
Started: 01/11/2022 02:23:26

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-group
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
group_size: 4
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-group
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-group_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-0.6916, -0.4774, -0.2610,  ...,  0.1977,  0.1970,  0.1560],
        [ 0.0320,  0.0217,  0.0081,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1455, -0.0695, -0.0684,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.8144,  0.8248,  0.8417,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0187, -0.0617, -0.0785,  ..., -0.7038, -0.6274, -0.5245],
        [-0.3005, -0.2632, -0.1244,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 3, 3, 2, 2, 3, 1, 2, 2, 2, 2, 0, 2, 0, 3, 2, 3, 2, 2, 2, 2, 3, 1, 2,
        0, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 0, 0, 0, 1, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 0.1709,  0.4450,  0.7561,  ...,  0.0000,  0.0000,  0.0000],
        [-4.7144, -4.3755, -4.5606,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1187, -0.2383,  0.2317,  ..., -1.9129, -2.0914, -1.8204],
        ...,
        [-0.0265, -0.0326,  0.0926,  ..., -0.2440, -0.2259, -0.2553],
        [ 0.0337,  0.0651, -0.0119,  ...,  0.0000,  0.0000,  0.0000],
        [-0.3088, -0.3590, -0.2813,  ..., -0.3848, -0.4988, -0.7131]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 1, 2, 0, 3, 2, 0, 1, 2, 1, 0, 0, 1, 3, 0, 1, 1, 0, 0, 0, 3, 3, 0, 1,
        0, 1, 2, 2, 2, 0, 3, 1, 1, 0, 0, 0, 0, 2, 1, 3])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[-0.1514, -0.6439,  0.2933,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2028,  0.1541,  0.0837,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3647,  0.3778,  0.3848,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [-0.0073, -0.0065,  0.0042,  ...,  0.0000,  0.0000,  0.0000],
        [-0.7251, -0.7119, -0.4333,  ..., -0.1654,  0.0983,  0.3480],
        [ 0.0339,  0.0242,  0.0249,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 1, 3, 0, 0, 3, 0, 0, 2, 0, 3, 1, 1, 2, 1, 3, 3, 0, 2, 0, 2, 0, 3,
        3, 3, 1, 0, 0, 1, 3, 2, 3, 3, 3, 0, 2, 2, 3, 1])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Traceback (most recent call last):
  File "run_xlsr_group.py", line 737, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_xlsr_group.py", line 548, in fit
    train_loss, train_acc = self._train(train_loader, tr_itt, loss_sum_tr, acc_sum_tr)
  File "run_xlsr_group.py", line 568, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_xlsr_group.py", line 624, in _compute_loss
    grouped_labels.add(currlabel)
AttributeError: 'list' object has no attribute 'add'
