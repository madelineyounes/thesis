Mon Oct 10 13:59:36 AEDT 2022
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_8s.py
Started: 10/10/2022 13:59:39

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-1s
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_500f_devdata
train_filename: test_u_500f
evaluation_filename: train_u_500f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/test_u_500f.csv
--> data_test_fp: data/train_u_500f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_500f_devdata_local/wav2vec-ADI17-1s
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_500f_devdata_local/wav2vec-ADI17-1s_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 8 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 0.1401,  0.1877,  0.2482,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2115, -0.1890, -0.0012,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1834,  0.3141,  0.1628,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1953,  0.3638,  0.5659,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 2, 0, 2])}
Training DataCustom Files: 1963
Training Data Files: 491
Test Data Sample
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.bias', 'project_hid.weight', 'quantizer.weight_proj.weight']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.weight', 'projector.bias', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'input_values': tensor([[ 0.0110,  0.0676,  0.1037,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1817, -0.1589, -0.1653,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0449,  0.0621,  0.0773,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1122, -0.1130, -0.1080,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 2, 2])}
Test CustomData Files: 1997
Test Data Files: 500
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  4 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 198660
Mon Oct 10 20:39:47 AEDT 2022
------------------------------------------------------------------------
                         run_8s.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_8s.py
Started: 10/10/2022 20:39:52

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: wav2vec-ADI17-8s
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_500f_devdata
train_filename: test_u_500f
evaluation_filename: train_u_500f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/test_u_500f.csv
--> data_test_fp: data/train_u_500f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_500f_devdata_local/wav2vec-ADI17-8s
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_500f_devdata_local/wav2vec-ADI17-8s_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------


------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 8 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[-4.6997e-01, -4.4066e-01, -4.0720e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.5193e+00,  2.5714e+00,  2.5204e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.3052e-01, -3.1227e-01, -3.2545e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.5360e-02, -8.9660e-04, -1.0822e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 1, 2])}
Training DataCustom Files: 1963
Training Data Files: 491
Test Data Sample
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['quantizer.codevectors', 'project_q.weight', 'project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_q.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.weight', 'classifier.bias', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'input_values': tensor([[-0.1840, -0.1931, -0.2064,  ...,  0.0000,  0.0000,  0.0000],
        [ 1.5440,  1.1962,  0.7372,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0570, -0.0956,  0.0032,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3865,  0.7256,  0.7692,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 3, 2])}
Test CustomData Files: 1997
Test Data Files: 500
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  4 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 198660
Epoch 0 Train Acc 25.712831497192383% Val Acc 21.000001907348633% Train Loss 0.6935339570045471 Val Loss 1.3946399688720703
Trainable Parameters : 198660
Epoch 1 Train Acc 28.885948181152344% Val Acc 22.80000114440918% Train Loss 0.6902812123298645 Val Loss 1.3956691026687622
Trainable Parameters : 198660
Epoch 2 Train Acc 30.617109298706055% Val Acc 22.000001907348633% Train Loss 0.6849380731582642 Val Loss 1.4005614519119263
Trainable Parameters : 198660
Epoch 3 Train Acc 34.130348205566406% Val Acc 21.600000381469727% Train Loss 0.6774298548698425 Val Loss 1.427924633026123
Trainable Parameters : 198660
Epoch 4 Train Acc 36.67617416381836% Val Acc 21.55000114440918% Train Loss 0.6689243316650391 Val Loss 1.490549087524414
Trainable Parameters : 198660
Epoch 5 Train Acc 36.201629638671875% Val Acc 22.400001525878906% Train Loss 0.6638742685317993 Val Loss 1.495416522026062
Trainable Parameters : 198660
Epoch 6 Train Acc 36.96537780761719% Val Acc 21.55000114440918% Train Loss 0.6552063226699829 Val Loss 1.5737156867980957
Trainable Parameters : 198660
Epoch 7 Train Acc 39.68024826049805% Val Acc 21.700000762939453% Train Loss 0.6467350721359253 Val Loss 1.5786069631576538
Trainable Parameters : 198660
Epoch 8 Train Acc 40.173118591308594% Val Acc 21.55000114440918% Train Loss 0.6358045935630798 Val Loss 1.6197381019592285
Trainable Parameters : 198660
Epoch 9 Train Acc 43.31364822387695% Val Acc 21.30000114440918% Train Loss 0.6221640706062317 Val Loss 1.721956491470337
Trainable Parameters : 198660
Epoch 10 Train Acc 45.0264778137207% Val Acc 21.200000762939453% Train Loss 0.6098616719245911 Val Loss 1.8651092052459717
Trainable Parameters : 198660
Epoch 11 Train Acc 47.35234451293945% Val Acc 26.850000381469727% Train Loss 0.5948920249938965 Val Loss 1.925851821899414
Trainable Parameters : 198660
Epoch 12 Train Acc 47.82688522338867% Val Acc 24.150001525878906% Train Loss 0.587855875492096 Val Loss 2.0666677951812744
Trainable Parameters : 198660
Epoch 13 Train Acc 48.930755615234375% Val Acc 25.850000381469727% Train Loss 0.5744015574455261 Val Loss 2.3633840084075928
Trainable Parameters : 198660
Epoch 14 Train Acc 51.12016677856445% Val Acc 24.600000381469727% Train Loss 0.564171314239502 Val Loss 2.2143921852111816
Trainable Parameters : 198660
Epoch 15 Train Acc 52.20570373535156% Val Acc 25.80000114440918% Train Loss 0.5526439547538757 Val Loss 2.5445773601531982
Trainable Parameters : 198660
Epoch 16 Train Acc 54.480655670166016% Val Acc 25.30000114440918% Train Loss 0.5377804636955261 Val Loss 3.3275065422058105
Trainable Parameters : 198660
Epoch 17 Train Acc 55.27902603149414% Val Acc 25.600000381469727% Train Loss 0.5322641730308533 Val Loss 2.9467031955718994
Trainable Parameters : 198660
Epoch 18 Train Acc 56.19552230834961% Val Acc 26.400001525878906% Train Loss 0.5256224274635315 Val Loss 2.4385225772857666
Trainable Parameters : 198660
Epoch 19 Train Acc 56.39918899536133% Val Acc 25.250001907348633% Train Loss 0.5205194354057312 Val Loss 2.869297981262207
Trainable Parameters : 198660
Epoch 20 Train Acc 56.7556037902832% Val Acc 25.450000762939453% Train Loss 0.5203245282173157 Val Loss 2.8366973400115967
Trainable Parameters : 198660
Epoch 21 Train Acc 59.046844482421875% Val Acc 25.350000381469727% Train Loss 0.5018625855445862 Val Loss 2.864152193069458
Trainable Parameters : 198660
Epoch 22 Train Acc 59.70875930786133% Val Acc 25.600000381469727% Train Loss 0.4991357624530792 Val Loss 3.681241989135742
Trainable Parameters : 198660
Epoch 23 Train Acc 60.217926025390625% Val Acc 27.350000381469727% Train Loss 0.4877091646194458 Val Loss 2.6415770053863525
Trainable Parameters : 198660
Epoch 24 Train Acc 60.5234260559082% Val Acc 25.150001525878906% Train Loss 0.4773731231689453 Val Loss 2.6632351875305176
Trainable Parameters : 198660
Epoch 25 Train Acc 62.15275192260742% Val Acc 25.400001525878906% Train Loss 0.47564151883125305 Val Loss 2.9717655181884766
Trainable Parameters : 198660
Epoch 26 Train Acc 62.372711181640625% Val Acc 25.450000762939453% Train Loss 0.4706103205680847 Val Loss 4.165514945983887
Trainable Parameters : 198660
Epoch 27 Train Acc 63.03462600708008% Val Acc 24.950000762939453% Train Loss 0.4563588798046112 Val Loss 3.740823984146118
Trainable Parameters : 198660
Epoch 28 Train Acc 62.64358901977539% Val Acc 25.200000762939453% Train Loss 0.45542457699775696 Val Loss 3.9382150173187256
Trainable Parameters : 198660
Epoch 29 Train Acc 63.712833404541016% Val Acc 25.900001525878906% Train Loss 0.45079120993614197 Val Loss 3.6711087226867676
Trainable Parameters : 198660
Epoch 30 Train Acc 65.2077407836914% Val Acc 25.650001525878906% Train Loss 0.4430750906467438 Val Loss 3.4055426120758057
Trainable Parameters : 198660
