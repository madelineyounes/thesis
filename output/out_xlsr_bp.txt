Wed Oct 26 16:24:24 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr_bp.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_bp.py
Started: 26/10/2022 16:24:28

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-bp
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-bp
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-bp_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
Traceback (most recent call last):
  File "run_xlsr_bp.py", line 397, in <module>
    TrainData = next(iter(trainDataLoader))
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 530, in __next__
    data = self._next_data()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1224, in _next_data
    return self._process_data(data)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1250, in _process_data
    data.reraise()
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/z5208494/thesis/customDataBP.py", line 72, in __getitem__
    audiopath, self.sampling_rate, self.norm)
AttributeError: 'CustomDataset' object has no attribute 'norm'

Wed Oct 26 20:55:26 AEDT 2022
------------------------------------------------------------------------
                         run_xlsr_bp.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_xlsr_bp.py
Started: 26/10/2022 20:55:31

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: ADI17-xlsr-bp
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
test data path: /srv/scratch/z5208494/dataset/train_segments/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: u_train_700f
train_filename: u_train_700f
validation_filename: dev_u_200f
evaluation_filename: test_u_100f
use_checkpoint: False
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

number_of_worker: 1
hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.1
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: False
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: no
batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: -1
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 500
save_total_limit: 40
fp16: True
eval_steps: 100
load_best_model_at_end: False
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/u_train_700f.csv
--> data_test_fp: data/dev_u_200f.csv
--> data_test_fp: data/test_u_100f.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/u_train_700f_local/ADI17-xlsr-bp
--> finetuned_results_fp: /srv/scratch/z5208494/output/u_train_700f_local/ADI17-xlsr-bp_finetuned_results.csv
--> pretrained_mod: elgeish/wav2vec2-large-xlsr-53-arabic

------> PREPARING DATASET LABELS... ------------------------------------


------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 10 s
Sampling Rate: 16000
Target Sampling Rate: 16000
Create a custom dataset ---> 
Check data has been processed correctly... 
Train Data Sample
{'input_values': tensor([[ 3.5502e-05, -2.5482e-05, -3.0525e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4034e-05, -6.3625e-05, -2.9573e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.3697e-05,  8.2933e-05,  2.2600e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.9048e-05, -4.3409e-05, -7.5478e-05,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.4045e-05,  1.0889e-05, -2.1748e-06,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0081e-04, -8.2455e-05,  3.7607e-06,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 3, 2, 1, 3, 1, 3, 3, 1, 2, 2, 3, 2, 3, 0, 2, 1, 2, 0, 2, 1, 3, 2, 2,
        2, 3, 3, 1, 1, 3, 3, 3, 2, 3, 3, 2, 1, 2, 3, 3])}
Training DataCustom Files: 10502
Training Data Files: 263
Val Data Sample
{'input_values': tensor([[ 5.6881e-05,  5.7811e-05,  6.2332e-05,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0121e-05, -5.3766e-05, -2.1682e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.0303e-06,  1.0617e-05,  2.2432e-05,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.2430e-05, -9.7761e-05, -2.5828e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0561e-05,  2.1110e-05,  6.6482e-05,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3701e-04, -1.4427e-04, -1.7302e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3, 1, 3, 2, 1, 1, 2, 2, 0, 0, 1, 1, 1, 0, 0, 0, 2, 2, 3, 2, 3, 0, 3, 2,
        3, 2, 1, 2, 2, 3, 3, 0, 1, 2, 1, 3, 1, 3, 3, 3])}
Test CustomData Files: 813
Test Data Files: 21
Test Data Sample
Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']
- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'projector.weight', 'classifier.weight', 'projector.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
{'input_values': tensor([[ 1.9040e-06,  1.9227e-06,  1.9988e-06,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.8213e-05,  1.8300e-05,  1.7711e-05,  ...,  4.1085e-01,
          4.4007e-01,  4.7205e-01],
        [ 1.4357e-05,  3.6415e-05,  1.3932e-04,  ...,  1.3906e-01,
          1.3558e-01,  1.3073e-01],
        ...,
        [ 2.2454e-05,  6.4121e-05,  2.4237e-04,  ...,  2.6132e-01,
          2.6025e-01,  2.5984e-01],
        [-1.0802e-05, -5.6364e-06,  1.8181e-05,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.0133e-05,  4.8006e-05,  6.4331e-04,  ...,  1.7196e+00,
          1.3760e+00,  1.0054e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([2, 3, 1, 0, 2, 1, 2, 3, 3, 1, 0, 0, 3, 2, 1, 3, 1, 0, 3, 1, 2, 0, 1, 2,
        3, 2, 1, 1, 3, 0, 1, 1, 2, 2, 3, 1, 3, 1, 0, 0])}
Test CustomData Files: 398
Test Data Files: 10
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
GPUs Used :  3 GPUs!
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Trainable Parameters : 264452
Epoch 0 Train Acc 31.817489624023438% Val Acc 25.5% Train Loss 0.6832878589630127 Val Loss 1.41941237449646
Trainable Parameters : 264452
Epoch 1 Train Acc 40.0% Val Acc 26.200000762939453% Train Loss 0.6547784805297852 Val Loss 1.4569329023361206
Trainable Parameters : 264452
Epoch 2 Train Acc 39.992393493652344% Val Acc 24.700000762939453% Train Loss 0.6497952938079834 Val Loss 1.4541258811950684
Trainable Parameters : 264452
Epoch 3 Train Acc 40.05323028564453% Val Acc 25.0% Train Loss 0.6444298028945923 Val Loss 1.447650671005249
Trainable Parameters : 264452
Epoch 4 Train Acc 40.825096130371094% Val Acc 30.600000381469727% Train Loss 0.6398239731788635 Val Loss 1.431733250617981
Trainable Parameters : 264452
Epoch 5 Train Acc 41.75284957885742% Val Acc 35.10000228881836% Train Loss 0.6347219944000244 Val Loss 1.4161018133163452
Trainable Parameters : 264452
Epoch 6 Train Acc 42.84790802001953% Val Acc 34.29999923706055% Train Loss 0.6287349462509155 Val Loss 1.367186427116394
Trainable Parameters : 264452
Epoch 7 Train Acc 43.74524688720703% Val Acc 35.5% Train Loss 0.6222114562988281 Val Loss 1.3816568851470947
Trainable Parameters : 264452
Epoch 8 Train Acc 44.76805877685547% Val Acc 37.10000228881836% Train Loss 0.6174351572990417 Val Loss 1.369520664215088
Trainable Parameters : 264452
Epoch 9 Train Acc 45.680606842041016% Val Acc 36.400001525878906% Train Loss 0.6135689616203308 Val Loss 1.3335551023483276
Trainable Parameters : 264452
Epoch 10 Train Acc 46.14828872680664% Val Acc 36.29999923706055% Train Loss 0.6091266870498657 Val Loss 1.3900870084762573
Trainable Parameters : 264452
Epoch 11 Train Acc 47.26615905761719% Val Acc 33.5% Train Loss 0.6059284210205078 Val Loss 1.4108816385269165
Trainable Parameters : 264452
Epoch 12 Train Acc 47.49049377441406% Val Acc 35.400001525878906% Train Loss 0.6012741327285767 Val Loss 1.352012038230896
Trainable Parameters : 264452
Epoch 13 Train Acc 47.418251037597656% Val Acc 38.60000228881836% Train Loss 0.6011226177215576 Val Loss 1.2992216348648071
Trainable Parameters : 264452
Epoch 14 Train Acc 47.6692008972168% Val Acc 40.400001525878906% Train Loss 0.5990085005760193 Val Loss 1.2830654382705688
Trainable Parameters : 264452
Epoch 15 Train Acc 48.29277420043945% Val Acc 35.20000076293945% Train Loss 0.5953463315963745 Val Loss 1.3682774305343628
Trainable Parameters : 264452
Epoch 16 Train Acc 47.84030532836914% Val Acc 35.5% Train Loss 0.5944157242774963 Val Loss 1.331425428390503
Trainable Parameters : 264452
Epoch 17 Train Acc 48.486690521240234% Val Acc 34.5% Train Loss 0.5921359062194824 Val Loss 1.4187699556350708
Trainable Parameters : 264452
Epoch 18 Train Acc 48.90494155883789% Val Acc 38.400001525878906% Train Loss 0.5900687575340271 Val Loss 1.3270715475082397
Trainable Parameters : 264452
Epoch 19 Train Acc 48.85931396484375% Val Acc 35.29999923706055% Train Loss 0.5880247950553894 Val Loss 1.3272289037704468
Trainable Parameters : 264452
Epoch 20 Train Acc 49.07984924316406% Val Acc 38.29999923706055% Train Loss 0.5880988836288452 Val Loss 1.278817057609558
Trainable Parameters : 264452
Epoch 21 Train Acc 49.40304183959961% Val Acc 37.900001525878906% Train Loss 0.5830828547477722 Val Loss 1.320925235748291
Trainable Parameters : 264452
Epoch 22 Train Acc 49.581748962402344% Val Acc 37.900001525878906% Train Loss 0.5836987495422363 Val Loss 1.303928017616272
Trainable Parameters : 264452
Epoch 23 Train Acc 49.77946853637695% Val Acc 40.10000228881836% Train Loss 0.5803237557411194 Val Loss 1.284387469291687
Trainable Parameters : 264452
Epoch 24 Train Acc 50.17110061645508% Val Acc 36.400001525878906% Train Loss 0.5787050724029541 Val Loss 1.3707457780838013
Trainable Parameters : 264452
Epoch 25 Train Acc 49.4638786315918% Val Acc 41.900001525878906% Train Loss 0.5802192091941833 Val Loss 1.2504327297210693
Trainable Parameters : 264452
Epoch 26 Train Acc 49.74524688720703% Val Acc 37.60000228881836% Train Loss 0.5773777365684509 Val Loss 1.419293999671936
Trainable Parameters : 264452
Epoch 27 Train Acc 50.65399169921875% Val Acc 40.10000228881836% Train Loss 0.5754428505897522 Val Loss 1.3434175252914429
Trainable Parameters : 264452
Epoch 28 Train Acc 49.90114212036133% Val Acc 43.29999923706055% Train Loss 0.5761420726776123 Val Loss 1.304814338684082
Trainable Parameters : 264452
Epoch 29 Train Acc 50.25475311279297% Val Acc 38.400001525878906% Train Loss 0.5781331062316895 Val Loss 1.406786561012268
Trainable Parameters : 264452
Epoch 30 Train Acc 50.47148132324219% Val Acc 40.0% Train Loss 0.5751562714576721 Val Loss 1.2719905376434326
Trainable Parameters : 264452
Epoch 31 Train Acc 50.71102523803711% Val Acc 38.5% Train Loss 0.5753991007804871 Val Loss 1.2942755222320557
Trainable Parameters : 264452
Epoch 32 Train Acc 50.6387825012207% Val Acc 40.79999923706055% Train Loss 0.5734338164329529 Val Loss 1.3313215970993042
Trainable Parameters : 264452
Epoch 33 Train Acc 50.129276275634766% Val Acc 41.79999923706055% Train Loss 0.575684130191803 Val Loss 1.3289966583251953
Trainable Parameters : 264452
Epoch 34 Train Acc 50.42585372924805% Val Acc 32.900001525878906% Train Loss 0.5746843218803406 Val Loss 1.4774693250656128
Trainable Parameters : 264452
Epoch 35 Train Acc 50.61216735839844% Val Acc 35.400001525878906% Train Loss 0.5743967294692993 Val Loss 1.3217653036117554
Trainable Parameters : 264452
Epoch 36 Train Acc 50.5361213684082% Val Acc 39.20000076293945% Train Loss 0.5722251534461975 Val Loss 1.2721426486968994
Trainable Parameters : 264452
Epoch 37 Train Acc 50.40684509277344% Val Acc 43.0% Train Loss 0.5741636753082275 Val Loss 1.2310017347335815
Trainable Parameters : 264452
Epoch 38 Train Acc 50.832698822021484% Val Acc 38.79999923706055% Train Loss 0.5730580687522888 Val Loss 1.364925742149353
Trainable Parameters : 264452
Epoch 39 Train Acc 50.79087448120117% Val Acc 33.79999923706055% Train Loss 0.5722758769989014 Val Loss 1.3389018774032593
Trainable Parameters : 264452
Epoch 40 Train Acc 51.11787033081055% Val Acc 42.60000228881836% Train Loss 0.5718529224395752 Val Loss 1.2708324193954468
Trainable Parameters : 264452
Epoch 41 Train Acc 51.441062927246094% Val Acc 34.900001525878906% Train Loss 0.5688270926475525 Val Loss 1.3172885179519653
Trainable Parameters : 264452
Epoch 42 Train Acc 51.00380325317383% Val Acc 47.20000076293945% Train Loss 0.5721196532249451 Val Loss 1.2042452096939087
Trainable Parameters : 264452
Epoch 43 Train Acc 51.11406707763672% Val Acc 40.70000076293945% Train Loss 0.5680616497993469 Val Loss 1.2879259586334229
Trainable Parameters : 264452
Epoch 44 Train Acc 51.927757263183594% Val Acc 41.10000228881836% Train Loss 0.5668402314186096 Val Loss 1.2802823781967163
Trainable Parameters : 264452
Epoch 45 Train Acc 51.212928771972656% Val Acc 41.70000076293945% Train Loss 0.5671767592430115 Val Loss 1.2800878286361694
Trainable Parameters : 264452
Epoch 46 Train Acc 51.384029388427734% Val Acc 39.5% Train Loss 0.5699321627616882 Val Loss 1.3212292194366455
Trainable Parameters : 264452
Epoch 47 Train Acc 51.167301177978516% Val Acc 45.70000076293945% Train Loss 0.5667121410369873 Val Loss 1.2211183309555054
Trainable Parameters : 264452
Epoch 48 Train Acc 51.76805877685547% Val Acc 41.29999923706055% Train Loss 0.5659332275390625 Val Loss 1.3245012760162354
Trainable Parameters : 264452
Epoch 49 Train Acc 52.19771957397461% Val Acc 42.400001525878906% Train Loss 0.5615545511245728 Val Loss 1.2860056161880493
Trainable Parameters : 264452
Epoch 50 Train Acc 52.01520919799805% Val Acc 42.79999923706055% Train Loss 0.5631689429283142 Val Loss 1.2450748682022095
Trainable Parameters : 264452
Epoch 51 Train Acc 51.787071228027344% Val Acc 38.29999923706055% Train Loss 0.563395619392395 Val Loss 1.332037329673767
Trainable Parameters : 264452
Epoch 52 Train Acc 51.83650207519531% Val Acc 42.29999923706055% Train Loss 0.5616222620010376 Val Loss 1.2663644552230835
Trainable Parameters : 264452
Epoch 53 Train Acc 51.821292877197266% Val Acc 43.10000228881836% Train Loss 0.5620073080062866 Val Loss 1.2601464986801147
Trainable Parameters : 264452
Epoch 54 Train Acc 52.20151901245117% Val Acc 41.60000228881836% Train Loss 0.5627295970916748 Val Loss 1.3119105100631714
Trainable Parameters : 264452
Epoch 55 Train Acc 52.09125518798828% Val Acc 41.20000076293945% Train Loss 0.5576062798500061 Val Loss 1.3043895959854126
Trainable Parameters : 264452
Epoch 56 Train Acc 52.19391632080078% Val Acc 43.0% Train Loss 0.5584130883216858 Val Loss 1.403769850730896
Trainable Parameters : 264452
Epoch 57 Train Acc 52.07984924316406% Val Acc 42.70000076293945% Train Loss 0.5586205720901489 Val Loss 1.2481945753097534
Trainable Parameters : 264452
Epoch 58 Train Acc 51.992393493652344% Val Acc 40.0% Train Loss 0.5585874319076538 Val Loss 1.2813122272491455
Trainable Parameters : 264452
Epoch 59 Train Acc 52.47148132324219% Val Acc 40.79999923706055% Train Loss 0.5595836043357849 Val Loss 1.415274739265442
Trainable Parameters : 264452
Epoch 60 Train Acc 52.205322265625% Val Acc 39.70000076293945% Train Loss 0.5592465400695801 Val Loss 1.375901699066162
Trainable Parameters : 264452
Epoch 61 Train Acc 52.520912170410156% Val Acc 40.70000076293945% Train Loss 0.5562242269515991 Val Loss 1.3648580312728882
Trainable Parameters : 264452
Epoch 62 Train Acc 52.19011306762695% Val Acc 43.60000228881836% Train Loss 0.5571416616439819 Val Loss 1.2656023502349854
Trainable Parameters : 264452
Epoch 63 Train Acc 53.20912551879883% Val Acc 38.5% Train Loss 0.55516117811203 Val Loss 1.3012961149215698
Trainable Parameters : 264452
Epoch 64 Train Acc 52.543724060058594% Val Acc 41.70000076293945% Train Loss 0.5556695461273193 Val Loss 1.2523149251937866
Trainable Parameters : 264452
Epoch 65 Train Acc 52.95817565917969% Val Acc 30.600000381469727% Train Loss 0.5555034875869751 Val Loss 1.4937870502471924
Trainable Parameters : 264452
Epoch 66 Train Acc 52.74905014038086% Val Acc 38.60000228881836% Train Loss 0.5535517334938049 Val Loss 1.326728343963623
Trainable Parameters : 264452
Epoch 67 Train Acc 53.038021087646484% Val Acc 38.70000076293945% Train Loss 0.5526247024536133 Val Loss 1.374642014503479
Trainable Parameters : 264452
Epoch 68 Train Acc 52.589351654052734% Val Acc 41.29999923706055% Train Loss 0.5521509051322937 Val Loss 1.297385573387146
Trainable Parameters : 264452
Epoch 69 Train Acc 53.224334716796875% Val Acc 42.5% Train Loss 0.552635133266449 Val Loss 1.2371493577957153
Trainable Parameters : 264452
Epoch 70 Train Acc 52.50950622558594% Val Acc 42.29999923706055% Train Loss 0.5518799424171448 Val Loss 1.3849186897277832
Trainable Parameters : 264452
Epoch 71 Train Acc 53.19011306762695% Val Acc 43.20000076293945% Train Loss 0.5485427975654602 Val Loss 1.2992275953292847
Trainable Parameters : 264452
Epoch 72 Train Acc 52.91254806518555% Val Acc 41.20000076293945% Train Loss 0.5548766255378723 Val Loss 1.2861353158950806
Trainable Parameters : 264452
Epoch 73 Train Acc 52.825096130371094% Val Acc 38.60000228881836% Train Loss 0.5526110529899597 Val Loss 1.2921310663223267
Trainable Parameters : 264452
Epoch 74 Train Acc 53.019012451171875% Val Acc 42.400001525878906% Train Loss 0.5517125725746155 Val Loss 1.2697590589523315
Trainable Parameters : 264452
Epoch 75 Train Acc 53.661598205566406% Val Acc 39.70000076293945% Train Loss 0.5507757067680359 Val Loss 1.3399580717086792
Trainable Parameters : 264452
Epoch 76 Train Acc 53.49049377441406% Val Acc 39.60000228881836% Train Loss 0.5502122640609741 Val Loss 1.3164583444595337
Trainable Parameters : 264452
Epoch 77 Train Acc 52.81368637084961% Val Acc 39.900001525878906% Train Loss 0.5542349219322205 Val Loss 1.3276914358139038
Trainable Parameters : 264452
Epoch 78 Train Acc 53.889732360839844% Val Acc 37.60000228881836% Train Loss 0.5479088425636292 Val Loss 1.3053066730499268
Trainable Parameters : 264452
Epoch 79 Train Acc 53.54752731323242% Val Acc 36.400001525878906% Train Loss 0.551315426826477 Val Loss 1.4231926202774048
Trainable Parameters : 264452
Epoch 80 Train Acc 53.43726348876953% Val Acc 40.0% Train Loss 0.5483906865119934 Val Loss 1.31077241897583
Trainable Parameters : 264452
Epoch 81 Train Acc 53.32319259643555% Val Acc 38.0% Train Loss 0.5478906035423279 Val Loss 1.3489373922348022
Trainable Parameters : 264452
Epoch 82 Train Acc 53.30038070678711% Val Acc 42.70000076293945% Train Loss 0.5509177446365356 Val Loss 1.3079332113265991
Trainable Parameters : 264452
Epoch 83 Train Acc 53.992393493652344% Val Acc 41.0% Train Loss 0.548039972782135 Val Loss 1.3134914636611938
Trainable Parameters : 264452
Epoch 84 Train Acc 53.6387825012207% Val Acc 41.60000228881836% Train Loss 0.5471980571746826 Val Loss 1.3458994626998901
Trainable Parameters : 264452
Epoch 85 Train Acc 53.315589904785156% Val Acc 43.400001525878906% Train Loss 0.5461130738258362 Val Loss 1.246504306793213
Trainable Parameters : 264452
Epoch 86 Train Acc 53.832698822021484% Val Acc 40.60000228881836% Train Loss 0.547351598739624 Val Loss 1.3843889236450195
Trainable Parameters : 264452
Epoch 87 Train Acc 54.269962310791016% Val Acc 37.70000076293945% Train Loss 0.5457255244255066 Val Loss 1.3975162506103516
Trainable Parameters : 264452
Epoch 88 Train Acc 54.05323028564453% Val Acc 41.400001525878906% Train Loss 0.5460096001625061 Val Loss 1.389749526977539
Trainable Parameters : 264452
Epoch 89 Train Acc 54.6387825012207% Val Acc 38.29999923706055% Train Loss 0.5433396697044373 Val Loss 1.44642174243927
Trainable Parameters : 264452
Epoch 90 Train Acc 54.076045989990234% Val Acc 40.20000076293945% Train Loss 0.5459425449371338 Val Loss 1.3624285459518433
Trainable Parameters : 264452
Epoch 91 Train Acc 53.85551452636719% Val Acc 41.70000076293945% Train Loss 0.5451571941375732 Val Loss 1.295582890510559
Trainable Parameters : 264452
Epoch 92 Train Acc 53.83650207519531% Val Acc 42.79999923706055% Train Loss 0.5447967052459717 Val Loss 1.2794643640518188
Trainable Parameters : 264452
Epoch 93 Train Acc 54.12547302246094% Val Acc 43.70000076293945% Train Loss 0.5455482006072998 Val Loss 1.2625010013580322
Trainable Parameters : 264452
Epoch 94 Train Acc 54.44486618041992% Val Acc 40.70000076293945% Train Loss 0.5445345044136047 Val Loss 1.3358458280563354
Trainable Parameters : 264452
Epoch 95 Train Acc 53.828895568847656% Val Acc 39.900001525878906% Train Loss 0.5446639657020569 Val Loss 1.3589824438095093
Trainable Parameters : 264452
Epoch 96 Train Acc 53.570343017578125% Val Acc 41.70000076293945% Train Loss 0.5454222559928894 Val Loss 1.3433979749679565
Trainable Parameters : 264452
Epoch 97 Train Acc 53.45247268676758% Val Acc 41.10000228881836% Train Loss 0.5460966229438782 Val Loss 1.3242923021316528
Trainable Parameters : 264452
Epoch 98 Train Acc 54.28517150878906% Val Acc 43.0% Train Loss 0.5408123135566711 Val Loss 1.300108551979065
Trainable Parameters : 264452
Configuration saved in ../output/u_train_700f_local/ADI17-xlsr-bp/config.json
Model weights saved in ../output/u_train_700f_local/ADI17-xlsr-bp/pytorch_model.bin
Epoch 99 Train Acc 54.20151901245117% Val Acc 41.0% Train Loss 0.5425912737846375 Val Loss 1.3189996480941772

------> EVALUATING MODEL... ------------------------------------------ 

Final Test Acc:43.20000076293945% Loss:1.305134654045105
CONFUSION MATRIX
[[69 16 13  2]
 [34 47 12  7]
 [38 13 44  3]
 [41 17 30 12]]
CONFUSION MATRIX NORMALISED
[[0.17336683 0.04020101 0.03266332 0.00502513]
 [0.08542714 0.11809045 0.03015075 0.01758794]
 [0.09547739 0.03266332 0.11055276 0.00753769]
 [0.10301508 0.04271357 0.07537688 0.03015075]]
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.38      0.69      0.49       100
           1       0.51      0.47      0.49       100
           2       0.44      0.45      0.45        98
           3       0.50      0.12      0.19       100

    accuracy                           0.43       398
   macro avg       0.46      0.43      0.40       398
weighted avg       0.46      0.43      0.40       398


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 27/10/2022 07:11:17
