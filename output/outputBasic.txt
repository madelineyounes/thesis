Wed Sep 21 15:23:08 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 15:23:13

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 5
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Traceback (most recent call last):
  File "run_basic.py", line 941, in <module>
    trainer.fit(trainDataLoader, testDataLoader, 10)
  File "run_basic.py", line 765, in fit
    val_loss, val_acc = self._validate(val_loader)
  File "run_basic.py", line 818, in _validate
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_basic.py", line 830, in _compute_loss
    loss = lossfct(prediction, labels.reshape((labels.shape[0])).long())
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1164, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 3014, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_nll_loss_forward)
Wed Sep 21 15:32:19 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.bias', 'quantizer.codevectors', 'project_q.weight', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
Configuration saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 15:32:22

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 5
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Epoch 0 Train Acc 25.0 Val Acc 0.0 Train Loss 0.6985337734222412 Val Loss 1.3646279573440552
Epoch 1 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 2 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 3 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 4 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 5 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 6 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 7 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 8 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 9 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       4.0
           1       0.00      0.00      0.00       0.0
           2       0.00      0.00      0.00       0.0

    accuracy                           0.00       4.0
   macro avg       0.00      0.00      0.00       4.0
weighted avg       0.00      0.00      0.00       4.0


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 21/09/2022 15:32:43
100%|██████████| 4/4 [00:00<00:00, 20.18it/s]Wed Sep 21 16:00:44 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.weight', 'project_q.bias', 'project_hid.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 16:00:48

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 1000
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Traceback (most recent call last):
  File "run_basic.py", line 942, in <module>
    trainer.fit(trainDataLoader, testDataLoader, 10)
  File "run_basic.py", line 762, in fit
    train_loss, train_acc = self._train(train_loader)
  File "run_basic.py", line 784, in _train
    loss, acc = self._compute_loss(model, inputs, labels)
  File "run_basic.py", line 828, in _compute_loss
    prediction = model(**inputs).logits
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "run_basic.py", line 563, in forward
    logits = self.classifier(hidden_states)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/z5208494/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x768 and 256x4)
Wed Sep 21 16:13:51 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.weight', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
Configuration saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 16:13:56

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 1000
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Epoch 0 Train Acc 0.0 Val Acc 0.0 Train Loss 0.6868237853050232 Val Loss 1.4080770015716553
Epoch 1 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 2 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 3 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 4 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 5 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 6 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 7 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 8 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 9 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       4.0
           1       0.00      0.00      0.00       0.0
           2       0.00      0.00      0.00       0.0

    accuracy                           0.00       4.0
   macro avg       0.00      0.00      0.00       4.0
weighted avg       0.00      0.00      0.00       4.0


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 21/09/2022 16:14:16
100%|██████████| 4/4 [00:00<00:00, 22.31it/s]Wed Sep 21 16:23:05 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.codevectors', 'project_hid.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 16:23:10

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 1000
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Traceback (most recent call last):
  File "run_basic.py", line 942, in <module>
    trainer.fit(trainDataLoader, testDataLoader, num_train_epochs)
NameError: name 'num_train_epochs' is not defined
Wed Sep 21 16:32:20 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_hid.weight', 'quantizer.codevectors', 'project_q.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 16:32:26

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
num_train_epochs: 1000
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Epoch 0 Train Acc 75.0 Val Acc 0.0 Train Loss 0.6666961908340454 Val Loss 1.423382043838501
Epoch 1 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 2 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 3 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 4 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 5 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 6 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 7 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 8 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 9 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 10 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 11 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 12 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 13 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 14 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 15 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 16 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 17 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 18 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 19 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 20 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 21 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 22 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 23 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 24 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 25 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 26 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 27 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 28 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 29 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 30 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 31 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 32 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 33 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 34 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 35 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 36 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 37 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 38 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 39 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 40 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 41 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 42 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 43 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 44 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 45 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 46 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 47 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 48 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 49 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 50 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 51 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 52 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 53 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 54 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 55 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 56 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 57 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 58 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 59 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 60 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 61 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 62 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 63 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 64 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 65 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 66 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 67 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 68 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 69 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 70 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 71 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 72 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 73 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 74 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 75 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 76 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 77 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 78 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 79 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 80 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 81 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 82 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 83 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 84 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 85 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 86 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 87 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 88 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 89 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 90 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 91 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 92 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 93 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 94 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 95 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 96 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 97 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 98 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 99 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 100 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 101 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 102 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 103 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 104 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 105 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 106 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 107 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 108 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 109 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 110 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 111 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 112 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 113 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 114 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 115 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 116 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 117 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 118 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 119 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 120 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 121 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 122 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 123 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 124 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 125 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 126 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 127 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 128 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 129 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 130 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 131 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 132 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 133 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 134 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 135 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 136 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 137 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 138 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 139 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 140 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 141 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 142 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 143 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 144 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 145 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 146 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 147 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 148 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 149 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 150 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 151 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 152 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 153 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 154 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 155 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 156 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 157 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 158 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 159 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 160 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 161 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 162 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 163 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 164 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 165 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 166 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 167 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 168 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 169 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 170 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 171 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 172 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 173 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 174 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 175 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 176 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 177 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 178 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 179 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 180 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 181 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 182 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 183 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 184 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 185 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 186 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 187 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 188 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 189 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 190 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 191 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 192 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 193 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 194 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 195 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 196 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 197 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 198 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 199 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 200 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 201 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 202 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 203 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 204 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 205 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 206 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 207 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 208 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 209 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 210 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 211 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 212 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 213 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 214 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 215 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 216 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 217 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 218 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 219 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 220 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 221 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 222 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 223 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 224 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 225 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 226 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 227 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 228 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 229 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 230 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 231 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 232 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 233 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 234 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 235 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 236 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 237 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 238 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 239 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 240 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 241 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 242 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 243 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 244 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 245 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 246 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 247 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 248 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 249 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 250 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 251 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 252 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 253 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 254 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 255 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 256 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 257 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 258 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 259 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 260 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 261 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 262 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 263 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 264 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 265 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 266 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 267 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 268 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 269 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 270 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 271 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 272 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 273 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 274 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 275 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 276 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 277 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 278 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 279 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 280 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 281 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 282 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 283 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 284 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 285 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 286 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 287 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 288 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 289 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 290 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 291 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 292 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 293 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 294 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 295 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 296 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 297 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 298 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 299 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 300 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 301 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 302 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 303 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 304 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 305 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 306 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 307 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 308 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 309 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 310 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 311 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 312 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 313 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 314 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 315 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 316 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 317 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 318 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 319 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 320 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 321 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 322 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 323 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 324 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 325 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 326 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 327 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 328 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 329 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 330 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 331 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 332 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 333 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 334 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 335 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 336 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 337 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 338 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 339 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 340 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 341 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 342 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 343 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 344 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 345 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 346 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 347 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 348 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 349 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 350 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 351 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 352 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 353 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 354 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 355 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 356 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 357 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 358 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 359 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 360 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 361 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 362 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 363 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 364 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 365 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 366 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 367 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 368 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 369 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 370 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 371 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 372 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 373 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 374 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 375 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 376 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 377 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 378 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 379 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 380 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 381 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 382 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 383 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 384 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 385 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 386 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 387 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 388 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 389 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 390 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 391 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 392 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 393 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 394 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 395 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 396 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 397 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 398 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 399 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 400 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 401 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 402 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 403 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 404 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 405 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 406 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 407 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 408 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 409 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 410 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 411 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 412 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 413 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 414 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 415 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 416 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 417 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 418 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 419 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 420 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 421 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 422 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 423 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 424 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 425 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 426 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 427 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 428 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 429 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 430 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 431 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 432 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 433 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 434 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 435 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 436 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 437 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 438 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 439 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 440 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 441 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 442 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 443 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 444 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 445 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 446 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 447 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 448 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 449 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 450 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 451 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 452 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 453 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 454 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 455 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 456 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 457 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 458 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 459 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 460 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 461 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 462 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 463 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 464 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 465 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 466 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 467 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 468 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 469 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 470 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 471 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 472 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 473 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 474 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 475 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 476 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 477 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 478 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 479 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 480 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 481 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 482 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 483 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 484 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 485 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 486 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 487 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 488 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 489 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 490 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 491 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 492 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 493 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 494 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 495 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 496 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 497 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 498 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 499 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 500 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 501 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 502 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 503 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 504 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 505 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 506 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 507 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 508 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 509 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 510 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 511 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 512 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 513 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 514 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 515 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 516 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 517 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 518 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 519 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 520 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 521 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 522 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 523 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 524 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 525 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 526 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 527 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 528 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 529 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 530 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 531 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 532 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 533 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 534 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 535 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 536 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 537 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 538 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 539 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 540 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 541 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 542 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 543 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 544 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 545 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 546 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 547 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 548 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 549 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 550 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 551 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 552 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 553 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 554 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 555 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 556 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 557 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 558 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 559 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 560 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 561 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 562 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 563 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 564 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 565 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 566 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 567 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 568 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 569 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 570 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 571 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 572 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 573 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 574 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 575 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 576 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 577 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 578 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 579 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 580 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 581 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 582 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 583 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 584 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 585 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 586 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 587 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 588 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 589 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 590 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 591 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 592 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 593 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 594 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 595 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 596 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 597 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 598 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 599 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 600 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 601 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 602 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 603 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 604 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 605 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 606 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 607 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 608 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 609 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 610 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 611 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 612 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 613 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 614 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 615 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 616 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 617 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 618 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 619 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 620 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 621 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 622 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 623 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 624 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 625 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 626 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 627 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 628 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 629 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 630 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 631 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 632 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 633 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 634 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 635 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 636 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 637 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 638 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 639 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 640 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 641 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 642 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 643 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 644 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 645 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 646 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 647 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 648 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 649 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 650 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 651 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 652 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 653 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 654 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 655 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 656 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 657 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 658 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 659 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 660 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 661 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 662 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 663 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 664 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 665 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 666 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 667 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 668 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 669 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 670 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 671 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 672 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 673 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 674 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 675 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 676 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 677 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 678 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 679 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 680 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 681 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 682 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 683 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 684 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 685 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 686 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 687 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 688 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 689 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 690 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 691 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 692 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 693 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 694 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 695 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 696 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 697 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 698 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 699 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 700 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 701 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 702 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 703 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 704 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 705 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 706 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 707 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 708 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 709 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 710 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 711 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 712 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 713 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 714 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 715 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 716 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 717 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 718 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 719 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 720 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 721 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 722 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 723 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 724 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 725 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 726 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 727 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 728 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 729 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 730 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 731 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 732 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 733 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 734 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 735 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 736 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 737 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 738 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 739 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 740 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 741 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 742 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 743 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 744 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 745 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 746 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 747 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 748 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 749 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 750 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 751 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 752 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 753 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 754 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 755 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 756 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 757 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 758 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 759 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 760 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 761 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 762 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 763 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 764 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 765 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 766 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 767 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 768 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 769 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 770 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 771 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 772 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 773 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 774 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 775 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 776 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 777 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 778 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 779 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 780 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 781 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 782 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 783 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 784 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 785 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 786 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 787 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 788 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 789 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 790 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 791 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 792 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 793 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 794 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 795 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 796 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 797 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 798 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 799 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 800 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 801 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 802 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 803 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 804 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 805 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 806 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 807 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 808 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 809 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 810 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 811 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 812 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 813 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 814 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 815 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 816 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 817 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 818 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 819 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 820 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 821 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 822 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 823 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 824 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 825 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 826 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 827 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 828 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 829 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 830 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 831 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 832 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 833 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 834 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 835 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 836 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 837 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 838 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 839 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 840 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 841 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 842 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 843 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 844 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 845 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 846 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 847 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 848 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 849 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 850 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 851 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 852 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 853 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 854 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 855 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 856 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 857 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 858 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 859 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 860 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 861 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 862 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 863 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 864 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 865 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 866 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 867 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 868 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 869 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 870 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 871 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 872 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 873 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 874 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 875 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 876 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 877 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 878 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 879 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 880 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 881 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 882 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 883 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 884 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 885 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 886 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 887 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 888 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 889 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 890 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 891 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 892 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 893 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 894 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 895 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 896 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 897 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 898 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 899 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 900 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 901 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 902 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 903 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 904 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 905 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 906 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 907 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 908 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 909 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 910 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 911 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 912 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 913 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 914 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 915 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 916 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 917 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 918 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 919 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 920 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 921 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 922 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 923 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 924 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 925 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 926 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 927 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 928 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 929 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 930 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 931 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 932 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 933 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 934 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 935 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 936 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 937 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 938 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 939 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 940 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 941 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 942 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 943 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 944 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 945 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 946 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 947 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 948 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 949 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 950 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 951 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 952 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 953 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 954 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 955 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 956 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 957 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 958 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 959 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 960 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 961 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 962 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 963 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 964 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 965 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 966 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 967 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 968 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 969 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 970 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 971 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 972 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 973 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0Configuration saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

Epoch 974 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 975 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 976 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 977 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 978 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 979 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 980 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 981 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 982 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 983 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 984 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 985 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 986 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 987 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 988 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 989 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 990 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 991 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 992 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 993 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 994 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 995 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 996 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 997 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 998 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 999 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       4.0
           1       0.00      0.00      0.00       0.0

    accuracy                           0.00       4.0
   macro avg       0.00      0.00      0.00       4.0
weighted avg       0.00      0.00      0.00       4.0


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 21/09/2022 16:32:46
100%|██████████| 4/4 [00:00<00:00, 21.79it/s]Wed Sep 21 17:06:21 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.bias', 'quantizer.weight_proj.weight', 'project_q.weight', 'project_hid.weight', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 17:06:25

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 1000
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Epoch 0 Train Acc 50.0 Val Acc 25.0 Train Loss 0.6782333850860596 Val Loss 1.3807575702667236
Epoch 1 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 2 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 3 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 4 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 5 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 6 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 7 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 8 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 9 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 10 Trainable Parameters : 85648900
Epoch 10 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 11 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 12 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 13 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 14 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 15 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 16 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 17 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 18 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 19 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 20 Trainable Parameters : 85648900
Epoch 20 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 21 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 22 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 23 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 24 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 25 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 26 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 27 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 28 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 29 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 30 Trainable Parameters : 85648900
Epoch 30 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 31 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 32 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 33 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 34 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 35 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 36 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 37 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 38 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 39 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 40 Trainable Parameters : 85648900
Epoch 40 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 41 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 42 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 43 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 44 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 45 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 46 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 47 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 48 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 49 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 50 Trainable Parameters : 85648900
Epoch 50 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 51 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 52 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 53 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 54 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 55 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 56 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 57 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 58 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 59 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 60 Trainable Parameters : 85648900
Epoch 60 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 61 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 62 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 63 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 64 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 65 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 66 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0Configuration saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

Epoch 67 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 68 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 69 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 70 Trainable Parameters : 85648900
Epoch 70 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 71 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 72 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 73 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 74 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 75 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 76 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 77 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 78 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 79 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 80 Trainable Parameters : 85648900
Epoch 80 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 81 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 82 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 83 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 84 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 85 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 86 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 87 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 88 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 89 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Updated Parameters at Epoch 90 Trainable Parameters : 85648900
Epoch 90 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 91 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 92 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 93 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 94 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 95 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 96 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 97 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 98 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
Epoch 99 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       1.00      0.25      0.40         4
           1       0.00      0.00      0.00         0
           3       0.00      0.00      0.00         0

    accuracy                           0.25         4
   macro avg       0.33      0.08      0.13         4
weighted avg       1.00      0.25      0.40         4


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 21/09/2022 17:06:48
100%|██████████| 4/4 [00:00<00:00, 22.94it/s]Wed Sep 21 17:39:49 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.weight_proj.bias', 'project_q.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.bias', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 17:39:53

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

Traceback (most recent call last):
  File "run_basic.py", line 958, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 763, in fit
    print("EPOCH unfeeze : " +  (epoch % set_unfreezing_step))
TypeError: can only concatenate str (not "int") to str
Wed Sep 21 17:57:52 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_q.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 17:57:56

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Epoch 0 Train Acc 0.0 Val Acc 0.0 Train Loss 0.714037299156189 Val Loss 1.407888650894165
EPOCH unfeeze : 1
Epoch 1 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 2 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 3 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 4 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 5 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 6 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 7 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 8 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 9 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 10 Trainable Parameters : 85648900
Epoch 10 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 11 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 12 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 13 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 14 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 15 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 16 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 17 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 18 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 19 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 20 Trainable Parameters : 85648900
Epoch 20 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 21 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 22 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 23 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 24 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 25 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 26 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 27 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 28 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 29 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 30 Trainable Parameters : 85648900
Epoch 30 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 31 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 32 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 33 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 34 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 35 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 36 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 37 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 38 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 39 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 40 Trainable Parameters : 85648900
Epoch 40 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 41 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 42 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 43 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 44 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 45 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 46 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 47 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 48 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 49 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 50 Trainable Parameters : 85648900
Epoch 50 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 51 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 52 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0Configuration saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

EPOCH unfeeze : 3
Epoch 53 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 54 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 55 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 56 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 57 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 58 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 59 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 60 Trainable Parameters : 85648900
Epoch 60 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 61 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 62 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 63 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 64 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 65 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 66 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 67 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 68 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 69 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 70 Trainable Parameters : 85648900
Epoch 70 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 71 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 72 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 73 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 74 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 75 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 76 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 77 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 78 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 79 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 80 Trainable Parameters : 85648900
Epoch 80 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 81 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 82 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 83 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 84 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 85 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 86 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 87 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 88 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 89 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 90 Trainable Parameters : 85648900
Epoch 90 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 91 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 92 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 93 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 94 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 95 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 96 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 97 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 98 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 99 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       4.0
           1       0.00      0.00      0.00       0.0
           3       0.00      0.00      0.00       0.0

    accuracy                           0.00       4.0
   macro avg       0.00      0.00      0.00       4.0
weighted avg       0.00      0.00      0.00       4.0


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 21/09/2022 17:58:17
100%|██████████| 4/4 [00:00<00:00, 21.81it/s]Wed Sep 21 18:16:01 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 18:16:04

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Traceback (most recent call last):
  File "run_basic.py", line 960, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 787, in fit
    val_loss, val_acc = self._validate(
  File "run_basic.py", line 838, in _validate
    acc_sum_val += acc.detach()
UnboundLocalError: local variable 'acc_sum_val' referenced before assignment
Wed Sep 21 18:21:56 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_q.bias']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 21/09/2022 18:21:59

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Traceback (most recent call last):
  File "run_basic.py", line 960, in <module>
    trainer.fit(trainDataLoader, testDataLoader, set_num_train_epochs)
  File "run_basic.py", line 787, in fit
    val_loss, val_acc = self._validate(
  File "run_basic.py", line 838, in _validate
    acc_sum_val += acc.detach()
UnboundLocalError: local variable 'acc_sum_val' referenced before assignment
Mon Sep 26 15:53:23 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_hid.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 26/09/2022 15:53:29

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Epoch 0 Train Acc 25.0 Val Acc 100.0 Train Loss 0.6845825910568237 Val Loss 1.3394856452941895
EPOCH unfeeze : 1
Epoch 1 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 2 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 3 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 4 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 5 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 6 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 7 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 8 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 9 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 10 Trainable Parameters : 85648900
Epoch 10 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 11 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 12 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 13 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 14 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 15 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 16 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 17 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 18 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 19 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 20 Trainable Parameters : 85648900
Epoch 20 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 21 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 22 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 23 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 24 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 25 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 26 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 27 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 28 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 29 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 30 Trainable Parameters : 85648900
Epoch 30 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 31 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 32 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 33 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 34 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 35 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 36 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 37 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 38 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 39 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 40 Trainable Parameters : 85648900
Epoch 40 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 41 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 42 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 43 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 44 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 45 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 46 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 47 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 48 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 49 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 50 Trainable Parameters : 85648900
Epoch 50 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 51 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 52 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0Configuration saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

EPOCH unfeeze : 3
Epoch 53 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 54 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 55 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 56 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 57 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 58 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 59 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 60 Trainable Parameters : 85648900
Epoch 60 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 61 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 62 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 63 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 64 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 65 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 66 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 67 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 68 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 69 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 70 Trainable Parameters : 85648900
Epoch 70 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 71 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 72 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 73 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 74 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 75 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 76 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 77 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 78 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 79 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 80 Trainable Parameters : 85648900
Epoch 80 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 81 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 82 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 83 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 84 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 85 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 86 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 87 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 88 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 89 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 90 Trainable Parameters : 85648900
Epoch 90 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 91 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 92 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 93 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 94 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 95 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 96 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 97 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 98 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 99 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         4

    accuracy                           1.00         4
   macro avg       1.00      1.00      1.00         4
weighted avg       1.00      1.00      1.00         4


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 26/09/2022 15:53:49
100%|██████████| 4/4 [00:00<00:00, 16.06it/s]Mon Sep 26 16:09:22 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.codevectors', 'project_q.bias', 'project_hid.weight', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 26/09/2022 16:09:27

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Epoch 0 Train Acc 0.0 Val Acc 25.0 Train Loss 0.716163158416748 Val Loss 1.3692882061004639
EPOCH unfeeze : 1
Epoch 1 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 2 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 3 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 4 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 5 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 6 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 7 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 8 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 9 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 10 Trainable Parameters : 85648900
Epoch 10 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 11 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 12 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 13 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 14 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 15 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 16 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 17 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 18 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 19 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 20 Trainable Parameters : 85648900
Epoch 20 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 21 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 22 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 23 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 24 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 25 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 26 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 27 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 28 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 29 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 30 Trainable Parameters : 85648900
Epoch 30 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 31 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 32 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 33 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 34 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 35 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 36 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 37 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 38 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 39 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 40 Trainable Parameters : 85648900
Epoch 40 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 41 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 42 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 43 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 44 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 45 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 46 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 47 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 48 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 49 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 50 Trainable Parameters : 85648900
Epoch 50 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 51 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 52 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0Configuration saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

EPOCH unfeeze : 3
Epoch 53 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 54 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 55 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 56 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 57 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 58 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 59 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 60 Trainable Parameters : 85648900
Epoch 60 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 61 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 62 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 63 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 64 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 65 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 66 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 67 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 68 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 69 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 70 Trainable Parameters : 85648900
Epoch 70 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 71 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 72 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 73 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 74 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 75 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 76 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 77 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 78 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 79 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 80 Trainable Parameters : 85648900
Epoch 80 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 81 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 82 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 83 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 84 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 85 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 86 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 87 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 88 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 89 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 0
Updated Parameters at Epoch 90 Trainable Parameters : 85648900
Epoch 90 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 1
Epoch 91 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 2
Epoch 92 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 3
Epoch 93 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 4
Epoch 94 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 5
Epoch 95 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 6
Epoch 96 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 7
Epoch 97 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 8
Epoch 98 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0
EPOCH unfeeze : 9
Epoch 99 Train Acc 0.0 Val Acc 0.0 Train Loss 0.0 Val Loss 0.0

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       1.00      0.25      0.40         4
           2       0.00      0.00      0.00         0

    accuracy                           0.25         4
   macro avg       0.50      0.12      0.20         4
weighted avg       1.00      0.25      0.40         4


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 26/09/2022 16:09:48
100%|██████████| 4/4 [00:00<00:00, 18.68it/s]Mon Sep 26 16:34:21 AEST 2022
/home/z5208494/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Using custom data configuration default-f889fae328bd858b
Reusing dataset csv (/srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-2f8ca336ab0f2922.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a2aaa438801f86.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71a69bfb51f8a88d.arrow
Loading cached processed dataset at /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval/csv/default-f889fae328bd858b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f54f0dedaf213951.arrow
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSpeechClassification: ['project_hid.bias', 'quantizer.weight_proj.weight', 'project_q.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
max_steps is given, it will override any value given in num_train_epochs
------------------------------------------------------------------------
                         run_umbrellaDID.py                            
------------------------------------------------------------------------
Running:  /home/z5208494/thesis/run_basic.py
Started: 26/09/2022 16:34:25

------> IMPORTING PACKAGES.... ---------------------------------------

-->Importing datasets...
-->Importing random...
-->Importing pandas & numpy...
-->Importing json...
-->Importing Wav2Vec transformers...
-->Importing torchaudio...
-->Importing torch, dataclasses & typing...
-->Importing from transformers for training...
-->Importing pyarrow for loading dataset...
-->SUCCESS! All packages imported.

------> EXPERIMENT ARGUMENTS ----------------------------------------- 

training: True
experiment_id: xlsr-ADI17-initialtest/
datasetdict_id: myST-eval
data path: /srv/scratch/z5208494/dataset/
training data path: /srv/scratch/z5208494/dataset/dev_segments/
test data path: /srv/scratch/z5208494/dataset/test_segments/
base_fp: /srv/scratch/z5208494/output/
train_name: umbrella_alldevdata
train_filename: data_u_all_data
evaluation_filename: new_adi17_test_big
use_checkpoint: False
use_pretrained_tokenizer: True
pretrained_tokenizer: facebook/wav2vec2-base
eval_pretrained: False

------> MODEL ARGUMENTS... -------------------------------------------

hidden_dropout: 0.1
activation_dropout: 0.1
attention_dropoutput: 0.1
feat_proj_dropout: 0.0
layerdrop: 0.05
mask_time_prob: 0.065
mask_time_length: 10
ctc_loss_reduction: mean
ctc_zero_infinity: False
gradient_checkpointing: True
pooling_mode: mean

------> TRAINING ARGUMENTS... ----------------------------------------

evaluation strategy: epoch
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 4e-05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-08
unfreezing_step: 10
num_train_epochs: 100
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False
eval_steps: 100
load_best_model_at_end: True
metric_for_best_model: accuracy
greater_is_better: False
group_by_length: True
push_to_hub: False

------> GENERATING FILEPATHS... --------------------------------------

--> data_train_fp: data/data_u_all_data.csv
--> data_test_fp: data/new_adi17_test_big.csv
--> data_cache_fp: /srv/scratch/z5208494/cache/huggingface/datasets/myST-eval
--> model_fp: ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/
--> finetuned_results_fp: /srv/scratch/z5208494/output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/_finetuned_results.csv
--> pretrained_mod: facebook/wav2vec2-base
--> pretrained_tokenizer: facebook/wav2vec2-base

------> PREPARING DATASET LABELS... ------------------------------------

here

------> CREATING WAV2VEC2 FEATURE EXTRACTOR... -----------------------

SUCCESS: Created feature extractor.

------> PRE-PROCESSING DATA... ----------------------------------------- 

Max Duration: 0.1 s
Sampling Rate: 16000
Create a custom dataset ---> 
SUCCESS: Data ready for training and evaluation.

------> PREPARING FOR TRAINING & EVALUATION... ----------------------- 

--> Defining pooling layer...
Number of labels: 4
--> Defining Classifer
--> Defining evaluation metric...
SUCCESS: Defined Accuracy evaluation metric.
--> Loading pre-trained checkpoint...
-------- Setting up Model --------
SUCCESS: Pre-trained checkpoint loaded.
--> Defining Custom Trainer Class...

------> STARTING TRAINING... ----------------------------------------- 

EPOCH unfeeze : 0
Epoch 0 Train Acc 0.0 Val Acc 25.0 Train Loss 0.7142728567123413 Val Loss 1.3507235050201416
EPOCH unfeeze : 1
Epoch 1 Train Acc 0.0 Val Acc 25.0 Train Loss 0.7075865268707275 Val Loss 1.3524210453033447
EPOCH unfeeze : 2
Epoch 2 Train Acc 0.0 Val Acc 25.0 Train Loss 0.7069905996322632 Val Loss 1.3551092147827148
EPOCH unfeeze : 3
Epoch 3 Train Acc 0.0 Val Acc 25.0 Train Loss 0.7118656635284424 Val Loss 1.359771966934204
EPOCH unfeeze : 4
Epoch 4 Train Acc 0.0 Val Acc 25.0 Train Loss 0.7146668434143066 Val Loss 1.3653342723846436
EPOCH unfeeze : 5
Epoch 5 Train Acc 25.0 Val Acc 25.0 Train Loss 0.7025974988937378 Val Loss 1.3720132112503052
EPOCH unfeeze : 6
Epoch 6 Train Acc 50.0 Val Acc 25.0 Train Loss 0.6874018907546997 Val Loss 1.3799083232879639
EPOCH unfeeze : 7
Epoch 7 Train Acc 50.0 Val Acc 25.0 Train Loss 0.6830098032951355 Val Loss 1.3882299661636353
EPOCH unfeeze : 8
Epoch 8 Train Acc 75.0 Val Acc 25.0 Train Loss 0.6728119254112244 Val Loss 1.4001623392105103
EPOCH unfeeze : 9
Epoch 9 Train Acc 100.0 Val Acc 25.0 Train Loss 0.6517094969749451 Val Loss 1.4134914875030518
EPOCH unfeeze : 0
Updated Parameters at Epoch 10 Trainable Parameters : 85648900
Epoch 10 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6505901217460632 Val Loss 1.425979495048523
EPOCH unfeeze : 1
Epoch 11 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6319646835327148 Val Loss 1.4424850940704346
EPOCH unfeeze : 2
Epoch 12 Train Acc 100.0 Val Acc 0.0 Train Loss 0.6109081506729126 Val Loss 1.4628901481628418
EPOCH unfeeze : 3
Epoch 13 Train Acc 100.0 Val Acc 0.0 Train Loss 0.593216598033905 Val Loss 1.4825351238250732
EPOCH unfeeze : 4
Epoch 14 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5714694857597351 Val Loss 1.504871129989624
EPOCH unfeeze : 5
Epoch 15 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5521035194396973 Val Loss 1.530126690864563
EPOCH unfeeze : 6
Epoch 16 Train Acc 100.0 Val Acc 0.0 Train Loss 0.5127332210540771 Val Loss 1.5570216178894043
EPOCH unfeeze : 7
Epoch 17 Train Acc 100.0 Val Acc 0.0 Train Loss 0.49464529752731323 Val Loss 1.5896722078323364
EPOCH unfeeze : 8
Epoch 18 Train Acc 100.0 Val Acc 0.0 Train Loss 0.4600403904914856 Val Loss 1.6208429336547852
EPOCH unfeeze : 9
Epoch 19 Train Acc 100.0 Val Acc 0.0 Train Loss 0.42879945039749146 Val Loss 1.6746737957000732
EPOCH unfeeze : 0
Updated Parameters at Epoch 20 Trainable Parameters : 85648900
Epoch 20 Train Acc 100.0 Val Acc 0.0 Train Loss 0.40522831678390503 Val Loss 1.7127249240875244
EPOCH unfeeze : 1
Epoch 21 Train Acc 100.0 Val Acc 0.0 Train Loss 0.3629179000854492 Val Loss 1.743552565574646
EPOCH unfeeze : 2
Epoch 22 Train Acc 100.0 Val Acc 0.0 Train Loss 0.3394872844219208 Val Loss 1.812727451324463
EPOCH unfeeze : 3
Epoch 23 Train Acc 100.0 Val Acc 0.0 Train Loss 0.30839255452156067 Val Loss 1.8619294166564941
EPOCH unfeeze : 4
Epoch 24 Train Acc 100.0 Val Acc 0.0 Train Loss 0.2913094162940979 Val Loss 1.9407342672348022
EPOCH unfeeze : 5
Epoch 25 Train Acc 100.0 Val Acc 0.0 Train Loss 0.25705400109291077 Val Loss 2.009220600128174
EPOCH unfeeze : 6
Epoch 26 Train Acc 100.0 Val Acc 0.0 Train Loss 0.22844725847244263 Val Loss 2.1167855262756348
EPOCH unfeeze : 7
Epoch 27 Train Acc 100.0 Val Acc 0.0 Train Loss 0.211581751704216 Val Loss 2.1628763675689697
EPOCH unfeeze : 8
Epoch 28 Train Acc 100.0 Val Acc 0.0 Train Loss 0.19008603692054749 Val Loss 2.2710976600646973
EPOCH unfeeze : 9
Epoch 29 Train Acc 100.0 Val Acc 0.0 Train Loss 0.1713123917579651 Val Loss 2.3516087532043457
EPOCH unfeeze : 0
Updated Parameters at Epoch 30 Trainable Parameters : 85648900
Epoch 30 Train Acc 100.0 Val Acc 0.0 Train Loss 0.15726438164710999 Val Loss 2.4824156761169434
EPOCH unfeeze : 1
Epoch 31 Train Acc 100.0 Val Acc 0.0 Train Loss 0.1406283974647522 Val Loss 2.55147123336792
EPOCH unfeeze : 2
Epoch 32 Train Acc 100.0 Val Acc 0.0 Train Loss 0.13270403444766998 Val Loss 2.666032075881958
EPOCH unfeeze : 3
Epoch 33 Train Acc 100.0 Val Acc 0.0 Train Loss 0.12102998793125153 Val Loss 2.7807483673095703
EPOCH unfeeze : 4
Epoch 34 Train Acc 100.0 Val Acc 0.0 Train Loss 0.11310376226902008 Val Loss 2.8237075805664062
EPOCH unfeeze : 5
Epoch 35 Train Acc 100.0 Val Acc 0.0 Train Loss 0.10544859617948532 Val Loss 2.9297397136688232
EPOCH unfeeze : 6
Epoch 36 Train Acc 100.0 Val Acc 0.0 Train Loss 0.09820981323719025 Val Loss 2.983612060546875
EPOCH unfeeze : 7
Epoch 37 Train Acc 100.0 Val Acc 0.0 Train Loss 0.09181743860244751 Val Loss 3.0652801990509033
EPOCH unfeeze : 8
Epoch 38 Train Acc 100.0 Val Acc 0.0 Train Loss 0.08624845743179321 Val Loss 3.141613006591797
EPOCH unfeeze : 9
Epoch 39 Train Acc 100.0 Val Acc 0.0 Train Loss 0.08140851557254791 Val Loss 3.184370994567871Configuration saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/config.json
Model weights saved in ../output/umbrella_alldevdata_local/xlsr-ADI17-initialtest/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 8

EPOCH unfeeze : 0
Updated Parameters at Epoch 40 Trainable Parameters : 85648900
Epoch 40 Train Acc 100.0 Val Acc 0.0 Train Loss 0.07702840864658356 Val Loss 3.254685640335083
EPOCH unfeeze : 1
Epoch 41 Train Acc 100.0 Val Acc 0.0 Train Loss 0.07372306287288666 Val Loss 3.3053159713745117
EPOCH unfeeze : 2
Epoch 42 Train Acc 100.0 Val Acc 0.0 Train Loss 0.07052023708820343 Val Loss 3.361082077026367
EPOCH unfeeze : 3
Epoch 43 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06785259395837784 Val Loss 3.4128990173339844
EPOCH unfeeze : 4
Epoch 44 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06453290581703186 Val Loss 3.459599494934082
EPOCH unfeeze : 5
Epoch 45 Train Acc 100.0 Val Acc 0.0 Train Loss 0.06232940033078194 Val Loss 3.5153958797454834
EPOCH unfeeze : 6
Epoch 46 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05925333499908447 Val Loss 3.5613303184509277
EPOCH unfeeze : 7
Epoch 47 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05688846483826637 Val Loss 3.573430299758911
EPOCH unfeeze : 8
Epoch 48 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05473944544792175 Val Loss 3.6309497356414795
EPOCH unfeeze : 9
Epoch 49 Train Acc 100.0 Val Acc 0.0 Train Loss 0.052893735468387604 Val Loss 3.6623146533966064
EPOCH unfeeze : 0
Updated Parameters at Epoch 50 Trainable Parameters : 85648900
Epoch 50 Train Acc 100.0 Val Acc 0.0 Train Loss 0.05102109909057617 Val Loss 3.711622476577759
EPOCH unfeeze : 1
Epoch 51 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0488138310611248 Val Loss 3.7629706859588623
EPOCH unfeeze : 2
Epoch 52 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04722082242369652 Val Loss 3.8069562911987305
EPOCH unfeeze : 3
Epoch 53 Train Acc 100.0 Val Acc 0.0 Train Loss 0.045188743621110916 Val Loss 3.8257317543029785
EPOCH unfeeze : 4
Epoch 54 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04359706491231918 Val Loss 3.850515365600586
EPOCH unfeeze : 5
Epoch 55 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04236917197704315 Val Loss 3.8978981971740723
EPOCH unfeeze : 6
Epoch 56 Train Acc 100.0 Val Acc 0.0 Train Loss 0.04062525928020477 Val Loss 3.9303526878356934
EPOCH unfeeze : 7
Epoch 57 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03918519243597984 Val Loss 3.973137140274048
EPOCH unfeeze : 8
Epoch 58 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03766249492764473 Val Loss 4.007721900939941
EPOCH unfeeze : 9
Epoch 59 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03637092560529709 Val Loss 4.033751010894775
EPOCH unfeeze : 0
Updated Parameters at Epoch 60 Trainable Parameters : 85648900
Epoch 60 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03513946384191513 Val Loss 4.076655387878418
EPOCH unfeeze : 1
Epoch 61 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03406735509634018 Val Loss 4.107021331787109
EPOCH unfeeze : 2
Epoch 62 Train Acc 100.0 Val Acc 0.0 Train Loss 0.032831285148859024 Val Loss 4.144514560699463
EPOCH unfeeze : 3
Epoch 63 Train Acc 100.0 Val Acc 0.0 Train Loss 0.03164590895175934 Val Loss 4.172165393829346
EPOCH unfeeze : 4
Epoch 64 Train Acc 100.0 Val Acc 0.0 Train Loss 0.030359875410795212 Val Loss 4.206811904907227
EPOCH unfeeze : 5
Epoch 65 Train Acc 100.0 Val Acc 0.0 Train Loss 0.029454141855239868 Val Loss 4.2437028884887695
EPOCH unfeeze : 6
Epoch 66 Train Acc 100.0 Val Acc 0.0 Train Loss 0.028634704649448395 Val Loss 4.286510467529297
EPOCH unfeeze : 7
Epoch 67 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02741723135113716 Val Loss 4.315036773681641
EPOCH unfeeze : 8
Epoch 68 Train Acc 100.0 Val Acc 0.0 Train Loss 0.026433706283569336 Val Loss 4.352258682250977
EPOCH unfeeze : 9
Epoch 69 Train Acc 100.0 Val Acc 0.0 Train Loss 0.025584394112229347 Val Loss 4.381479263305664
EPOCH unfeeze : 0
Updated Parameters at Epoch 70 Trainable Parameters : 85648900
Epoch 70 Train Acc 100.0 Val Acc 0.0 Train Loss 0.024721594527363777 Val Loss 4.420122146606445
EPOCH unfeeze : 1
Epoch 71 Train Acc 100.0 Val Acc 0.0 Train Loss 0.023870248347520828 Val Loss 4.452238082885742
EPOCH unfeeze : 2
Epoch 72 Train Acc 100.0 Val Acc 0.0 Train Loss 0.023104149848222733 Val Loss 4.4873504638671875
EPOCH unfeeze : 3
Epoch 73 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02219809591770172 Val Loss 4.5139665603637695
EPOCH unfeeze : 4
Epoch 74 Train Acc 100.0 Val Acc 0.0 Train Loss 0.021546948701143265 Val Loss 4.552799701690674
EPOCH unfeeze : 5
Epoch 75 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02074567601084709 Val Loss 4.582616806030273
EPOCH unfeeze : 6
Epoch 76 Train Acc 100.0 Val Acc 0.0 Train Loss 0.02003476582467556 Val Loss 4.61809778213501
EPOCH unfeeze : 7
Epoch 77 Train Acc 100.0 Val Acc 0.0 Train Loss 0.0194268636405468 Val Loss 4.6423659324646
EPOCH unfeeze : 8
Epoch 78 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01866806671023369 Val Loss 4.6753950119018555
EPOCH unfeeze : 9
Epoch 79 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01809663139283657 Val Loss 4.708605766296387
EPOCH unfeeze : 0
Updated Parameters at Epoch 80 Trainable Parameters : 85648900
Epoch 80 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01748313382267952 Val Loss 4.747751235961914
EPOCH unfeeze : 1
Epoch 81 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01683865860104561 Val Loss 4.789592266082764
EPOCH unfeeze : 2
Epoch 82 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01626216433942318 Val Loss 4.811223983764648
EPOCH unfeeze : 3
Epoch 83 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01576180011034012 Val Loss 4.84399938583374
EPOCH unfeeze : 4
Epoch 84 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01525113731622696 Val Loss 4.880936145782471
EPOCH unfeeze : 5
Epoch 85 Train Acc 100.0 Val Acc 0.0 Train Loss 0.014714863151311874 Val Loss 4.913020610809326
EPOCH unfeeze : 6
Epoch 86 Train Acc 100.0 Val Acc 0.0 Train Loss 0.014223529025912285 Val Loss 4.933305263519287
EPOCH unfeeze : 7
Epoch 87 Train Acc 100.0 Val Acc 0.0 Train Loss 0.013723674230277538 Val Loss 4.9788007736206055
EPOCH unfeeze : 8
Epoch 88 Train Acc 100.0 Val Acc 0.0 Train Loss 0.013285355642437935 Val Loss 5.015230178833008
EPOCH unfeeze : 9
Epoch 89 Train Acc 100.0 Val Acc 0.0 Train Loss 0.012842979282140732 Val Loss 5.048835754394531
EPOCH unfeeze : 0
Updated Parameters at Epoch 90 Trainable Parameters : 85648900
Epoch 90 Train Acc 100.0 Val Acc 0.0 Train Loss 0.012408548966050148 Val Loss 5.073579788208008
EPOCH unfeeze : 1
Epoch 91 Train Acc 100.0 Val Acc 0.0 Train Loss 0.011993024498224258 Val Loss 5.107845783233643
EPOCH unfeeze : 2
Epoch 92 Train Acc 100.0 Val Acc 0.0 Train Loss 0.011590206064283848 Val Loss 5.143310070037842
EPOCH unfeeze : 3
Epoch 93 Train Acc 100.0 Val Acc 0.0 Train Loss 0.011224907822906971 Val Loss 5.181588649749756
EPOCH unfeeze : 4
Epoch 94 Train Acc 100.0 Val Acc 0.0 Train Loss 0.010822078213095665 Val Loss 5.212418079376221
EPOCH unfeeze : 5
Epoch 95 Train Acc 100.0 Val Acc 0.0 Train Loss 0.01048370823264122 Val Loss 5.240835189819336
EPOCH unfeeze : 6
Epoch 96 Train Acc 100.0 Val Acc 0.0 Train Loss 0.010158542543649673 Val Loss 5.272818088531494
EPOCH unfeeze : 7
Epoch 97 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009819362312555313 Val Loss 5.307746410369873
EPOCH unfeeze : 8
Epoch 98 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009531769901514053 Val Loss 5.340606689453125
EPOCH unfeeze : 9
Epoch 99 Train Acc 100.0 Val Acc 0.0 Train Loss 0.009190533310174942 Val Loss 5.37619686126709

------> EVALUATING MODEL... ------------------------------------------ 

LOSS LABELS, tensor([0], device='cuda:0')
  0%|          | 0/4 [00:00<?, ?it/s]/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/apps/python/3.8.3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
LOSS LABELS, tensor([0], device='cuda:0')
CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       4.0
           1       0.00      0.00      0.00       0.0

    accuracy                           0.00       4.0
   macro avg       0.00      0.00      0.00       4.0
weighted avg       0.00      0.00      0.00       4.0


------> SUCCESSFULLY FINISHED ---------------------------------------- 

Finished: 26/09/2022 16:35:28
100%|██████████| 4/4 [00:00<00:00, 18.33it/s]