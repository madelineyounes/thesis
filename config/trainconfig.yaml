evaluation_strategy: epoch
batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 0.00004 
weight_decay: 0.01
adam_beta1: 0.98
adam_beta2: 0.98
adam_epsilon: 0.00000001
train_epochs: 5
max_steps: 35000
lr_scheduler_type: linear
warmup_ratio: 0.1               # Default = 0.0
logging_strategy: steps         # Default = "steps"
logging_steps: 10               # Default = 500
save_strategy: epoch
save_steps: 1000
save_total_limit: 40
fp16: False